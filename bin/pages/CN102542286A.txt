<!DOCTYPE html><html><head><title>专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="学习装置、学习方法、识别装置、识别方法和程序"><meta name="DC.contributor" content="大久保厚志" scheme="inventor"><meta name="DC.contributor" content="横野顺" scheme="inventor"><meta name="DC.contributor" content="索尼公司" scheme="assignee"><meta name="DC.date" content="2011-9-30" scheme="dateSubmitted"><meta name="DC.description" content="本发明提供了一种学习装置、学习方法、识别装置、识别方法和程序，其中，学习装置包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择特征提取滤波器；提取部，其通过使用多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算部，其计算特征之间的相关性；以及学习部，其基于标记信息和相关性，学习同一对象分类器。"><meta name="DC.date" content="2012-7-4"><meta name="DC.relation" content="CN:101030244:A" scheme="references"><meta name="DC.relation" content="CN:101504719:A" scheme="references"><meta name="DC.relation" content="US:20090041312:A1" scheme="references"><meta name="DC.relation" content="US:20090196467:A1" scheme="references"><meta name="DC.relation" content="US:20090238419:A1" scheme="references"><meta name="DC.relation" content="US:20100214442:A1" scheme="references"><meta name="citation_reference" content="MICHAEL J. JONES等: &quot;Face Recognition Using Boosted Local Features&quot;, 《IEEE INTERNATIONAL CONFERENCE ON COMPUTER》, 30 April 2003 (2003-04-30), XP010949458"><meta name="citation_reference" content="RAINER LIENHART等: &quot;Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection&quot;, 《MRL TECHNICAL REPORT》, 30 September 2003 (2003-09-30), pages 2, XP002374203"><meta name="citation_patent_publication_number" content="CN:102542286:A"><meta name="citation_patent_application_number" content="CN:201110305469"><link rel="canonical" href="https://www.google.com/patents/CN102542286A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102542286A?cl=zh"/><meta name="title" content="专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序"/><meta name="description" content="本发明提供了一种学习装置、学习方法、识别装置、识别方法和程序，其中，学习装置包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择特征提取滤波器；提取部，其通过使用多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算部，其计算特征之间的相关性；以及学习部，其基于标记信息和相关性，学习同一对象分类器。"/><meta property="og:title" content="专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102542286A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102542286A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102542286A&amp;usg=AFQjCNGOAIuuA7J4FFlBT_1shEZsg722UQ" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/4e0b169a5c9e6db5957a/CN102542286A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/4e0b169a5c9e6db5957a/CN102542286A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102542286A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102542286A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102542286A?cl=zh" style="display:none"><span itemprop="description">本发明提供了一种学习装置、学习方法、识别装置、识别方法和程序，其中，学习装置包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在每个图像对中的一幅图像和另一幅图像上设置...</span><span itemprop="url">https://www.google.com/patents/CN102542286A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序" title="专利 CN102542286A - 学习装置、学习方法、识别装置、识别方法和程序"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102542286 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110305469</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年7月4日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年9月30日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2010年10月12日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US8811725?hl=zh-CN&amp;cl=zh">US8811725</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20120087574?hl=zh-CN&amp;cl=zh">US20120087574</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110305469.2, </span><span class="patent-bibdata-value">CN 102542286 A, </span><span class="patent-bibdata-value">CN 102542286A, </span><span class="patent-bibdata-value">CN 201110305469, </span><span class="patent-bibdata-value">CN-A-102542286, </span><span class="patent-bibdata-value">CN102542286 A, </span><span class="patent-bibdata-value">CN102542286A, </span><span class="patent-bibdata-value">CN201110305469, </span><span class="patent-bibdata-value">CN201110305469.2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%A4%A7%E4%B9%85%E4%BF%9D%E5%8E%9A%E5%BF%97%22">大久保厚志</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%A8%AA%E9%87%8E%E9%A1%BA%22">横野顺</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E7%B4%A2%E5%B0%BC%E5%85%AC%E5%8F%B8%22">索尼公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102542286A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102542286A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102542286A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (6),</span> <span class="patent-bibdata-value"><a href="#npl-citations">非专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (6),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (2)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110305469&amp;usg=AFQjCNFToe0yJJL2gO6P9YolUMedNaLcSQ"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102542286A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNFf73Wda-4Sa-vfTfzRSnlU1tZ2qg"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT113675272" lang="ZH" load-source="patent-office">学习装置、学习方法、识别装置、识别方法和程序</invention-title>
      </span><br><span class="patent-number">CN 102542286 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA97657763" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明提供了一种学习装置、学习方法、识别装置、识别方法和程序，其中，学习装置包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择特征提取滤波器；提取部，其通过使用多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算部，其计算特征之间的相关性；以及学习部，其基于标记信息和相关性，学习同一对象分类器。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(11)</span></span></div><div class="patent-text"><div mxw-id="PCLM43791526" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种学习装置，包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对； 设置部，其在所述获取部获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；提取部，其通过使用所述选择部选择的多个所述特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算部，其计算通过所述提取部从所述一幅图像中提取的特征与通过所述提取部从所述另一幅图像中提取的特征之间的相关性；以及学习部，其基于标记信息和所述计算部计算出的所述相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的学习装置，其中，所述学习部通过提升学习所述同一对象分类器，所述同一对象分类器是由多个弱分类器组成的强分类器，并且其中，每当所述学习部学习所述弱分类器时，所述选择部随机选择所述指定特征点和所述特征提取滤波器。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求2所述的学习装置，还包括生成部，其中，所述生成部在所述一幅图像和所述另一幅图像中的每幅图像被设置为原始图像时，根据所述原始图像生成第一积分图像，其中，当将所述原始图像的四个角中的指定角设置为基准位置时，所述第一积分图像的每个像素的像素值等于四角形的区域内的像素的亮度值的总和，所述四角形具有将所述基准位置与每个像素的位置连接的对角线，其中，每个特征提取滤波器是矩形滤波器。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求3所述的学习装置，其中，所述生成部在所述一幅图像和所述另一幅图像中的每幅图像被设置为所述原始图像时，进一步根据所述原始图像生成第二积分图像，其中，当将所述原始图像的每个像素的位置设置为顶角为90度的三角形的顶点并且将所述原始图像的四边中的指定边设置为所述三角形的底边时，所述第二积分图像的每个像素的像素值等于所述三角形的区域内的像素的亮度值的总和。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求1所述的学习装置，还包括存储部，所述存储部存储表示所述选择部选择的多个所述特征提取滤波器的信息和所述同一对象分类器的信息。</div>
    </div>
    </div> <div class="claim"> <div num="6" class="claim">
      <div class="claim-text">6.	一种学习方法，包括：获取相同对象出现的多个图像对和不同对象出现的多个图像对； 在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点； 选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；通过使用所选择的多个所述特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
    </div> <div class="claim"> <div num="7" class="claim">
      <div class="claim-text">7.	一种程序，用于使计算机执行包括以下功能的处理：获取相同对象出现的多个图像对和不同对象出现的多个图像对； 在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点； 选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；通过使用所选择的多个所述特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
    </div> <div class="claim"> <div num="8" class="claim">
      <div class="claim-text">8.	一种识别装置，包括：存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，所述学习装置：获取相同对象出现的多个图像对和不同对象出现的多个图像对； 在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点； 选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；通过使用所选择的多个所述特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的所述同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象；获取部，其获取所述图像对；设置部，其在所述获取部获取的每个图像对中的所述一幅图像和所述另一幅图像上设置特征点；提取部，其通过使用由所述存储部中所存储的信息表示的所述多个特征提取滤波器， 在所述获取部获取的图像对中的所述一幅图像和所述另一幅图像中提取所述设置部设置的各个特征点的特征；计算部，其计算通过所述提取部从所述一幅图像中提取的特征与通过所述提取部从所述另一幅图像中提取的特征之间的相关性；以及识别部，其在所述计算部计算出的相关性被提供作为输入时，基于所述同一对象分类器，识别所述获取部获取的图像对中的所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求8所述的识别装置，其中，所述识别部计算基于构成作为强分类器的所述同一对象分类器的多个弱分类器而计算出的得分的总和，并且如果所计算出的总和等于或大于阈值，则将所述获取部获取的图像对中的所述一幅图像和所述另一幅图像中出现的对象识别为同一对象。</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10.	一种识别装置的识别方法，所述识别装置包括存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，所述学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性， 学习用于识别两幅图像中出现的对象是否相同的所述同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象，所述识别方法包括以下步骤：获取图像对；在所获取的每个图像对中的所述一幅图像和所述另一幅图像上设置所述特征点；通过使用由所述存储部中所存储的信息表示的所述多个特征提取滤波器，在所获取的图像对中的所述一幅图像和所述另一幅图像中提取所设置的各个特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及当所计算出的相关性被提供作为输入时，基于所述同一对象分类器，识别所获取的图像对中的所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11.	一种用于使计算机执行识别装置的处理的程序，所述识别装置包括存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，所述学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在所述一幅图像和所述另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取所述指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取所述一幅图像和所述另一幅图像中的每幅图像的所述指定特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的所述同一对象分类器，其中，所述标记信息表示所述一幅图像和所述另一幅图像中出现的对象是否是同一对象，所述处理包括以下功能：获取图像对；在所获取的每个图像对中的所述一幅图像和所述另一幅图像上设置所述特征点；通过使用由所述存储部中所存储的信息表示的所述多个特征提取滤波器，在所获取的图像对中的所述一幅图像和所述另一幅图像中提取所设置的各个特征点的特征；计算从所述一幅图像中提取的特征与从所述另一幅图像中提取的特征之间的相关性；以及当所计算出的相关性被提供作为输入时，基于所述同一对象分类器，识别所获取的图像对中的所述一幅图像和所述另一幅图像中出现的对象是否是同一对象。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES49236345" lang="ZH" load-source="patent-office" class="description">
    <p>学习装置、学习方法、识别装置、识别方法和程序技术领域[0001]	本公开内容涉及一种学习装置、学习方法、识别装置、识别方法和程序。具体地，本公开内容涉及能够更迅速地且更精确地识别两幅图像中出现的对象是否是同一对象的学习装置、学习方法、识别装置、识别方法和程序。背景技术[0002]	相关技术中的面部识别方法大致分为如下两种方法：使用同一人物分类器的识别方法以及使用多类分类器的识别方法。在lace Recognition Using Boosted Local Features"(Michael Jones禾口Paul Viola,Mitsubishi Electric Research Laboratories Technical Report,2003 年 4 月）禾口 "Face Recognition Using Ada-Boosted Gabor Features，，(P. Yang, S. Shan, W. Gao, S. Li, D. Zhang, International Conference on)中公开了使用同一人物分类器的识别方法。使用同一人物分类器的识别例如通过以下操作来执行：计算从两幅面部图像中提取的特征之间的差，并且使用该差作为同一人物分类器的输入。[0003]	"Face Recognition Using Boosted Local Features" ^Ijf^Jf ^&#190;^:&#190;^&#190;!&#190; 对环境变化（诸如，照明的变化）较弱，这是因为执行了两幅图像的对应点之间的特征差的计算。另夕卜，同样地，"I^ace Recognition Using Ada-Boosted Gabor Features”中所公开的技术也被认为应对环境变化较弱，这是因为通过使用与"I^ace Recognition Using Boosted Local Features"中所公开的滤波器不同的滤波器执行相同的计算处理。[0004]	相应地，为了解决应对环境变化较弱的问题，提出了日本未审查专利申请公布第 2008-165731号中所公开的技术。图1是示出日本未审查专利申请公布第2008-165731号中所公开的相关技术中的面部识别的流程的图。[0005]	在日本未审查专利申请公布第2008-165731号中所公开的设备中，在识别时，如箭头#1和#2的箭尖所示，通过使用多个盖伯滤波器从输入图像的各个特征点提取特征，并且针对每个特征点计算特征向量，该特征向量的参数被设置为通过使用每个盖伯滤波器而提取的特征。[0006]	图2是示出盖伯滤波器的图。盖伯滤波器的特性由条纹部分的大小和方向定义。 在日本未审查专利申请公布第2008-165731号中所公开的设备中，根据特征点的位置，在特性由5种不同大小和8种不同朝向定义的40个不同的滤波器当中选择预定数量的滤波器，并且在提取各个特征点处的特征时使用预定数量的滤波器。[0007]	在日本未审查专利申请公布第2008-165731号中所公开的设备中，可以计算根据两幅图像的相同特征点计算出的、特征向量之间的相关性，如空心箭头#11的箭尖所示。另外，如箭头#12的箭尖所示，将参数是表示同一特征点的特征的特征向量的相关系数的相关向量用作分类器的输入，由此确定人物是否相同。[0008]	根据日本未审查专利申请公布第2008-165731号中所公开的技术，在特征提取中没有使用全部40个不同的滤波器，但通过组合滤波器的输出，在特征提取中使用若干个滤6波器。因而，可以改善识别的精确性。 发明内容[0009]	根据日本未审查专利申请公布第2008-165731号中所公开的技术，需要通过针对每个特征点使用多个盖伯滤波器来执行滤波操作，并且因而，提取特征花费了一定时间。[0010]	考虑到上述情形而作出了本公开内容，其中，期望更迅速地且更精确地识别两幅图像中出现的对象是否是同一对象。[0011]	根据本公开内容的第一实施例，提供了一种学习装置，该学习装置包括：获取部， 其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在获取部获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；提取部，其通过使用选择部选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算部，其计算通过提取部从一幅图像中提取的特征与通过提取部从另一幅图像中提取的特征之间的相关性；以及学习部，其基于标记信息和计算部计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0012]	优选地，学习部应该通过提升（boosting)来学习同一对象分类器，该同一对象分类器是由多个弱分类器组成的强分类器。另外，同样优选地，每当学习部学习弱分类器时， 选择部应该随机选择指定特征点和特征提取滤波器。[0013]	优选地，每个特征提取滤波器应该是矩形滤波器。另外，同样优选地，学习装置还应该包括生成部，其中，该生成部在一幅图像和另一幅图像中的每幅图像被设置为原始图像时，根据原始图像生成第一积分图像，其中，当将原始图像的四个角中的指定角设置为基准位置时，该第一积分图像的每个像素的像素值等于四角形的区域内的像素的亮度值的总和，该四角形区域具有将基准位置与每个像素的位置连接的对角线。[0014]	优选地，生成部应该在一幅图像和另一幅图像中的每幅图像被设置为原始图像时，进一步根据原始图像生成第二积分图像，其中，当将原始图像的每个像素的位置设置为顶角为90度的三角形的顶点并且将原始图像的四边中的指定边设置为三角形的底边时， 该第二积分图像的每个像素的像素值等于该三角形的区域内的像素的亮度值的总和。[0015]	优选地，学习装置还应该包括存储部，该存储部存储表示选择部选择的多个特征提取滤波器的信息和同一对象分类器的信息。[0016]	根据本公开内容的第一实施例，提供了一种学习方法，包括以下步骤：获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0017]	根据本公开内容的第一实施例，提供了一种程序，用于使计算机执行包括如下步骤的处理：获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0018]	根据本公开内容的第二实施例，提供了一种识别装置，该识别装置包括：存储部， 其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象；获取部，其获取图像对；设置部，其在获取部获取的每个图像对中的一幅图像和另一幅图像上设置特征点；提取部，其通过使用由存储部中所存储的信息表示的多个特征提取滤波器，在获取部获取的图像对中的一幅图像和另一幅图像中提取设置部设置的各个特征点的特征；计算部，其计算通过提取部从一幅图像中提取的特征与通过提取部从另一幅图像中提取的特征之间的相关性；以及识别部，其在计算部计算出的相关性被提供作为输入时，基于同一对象分类器，识别获取部获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0019]	优选地，识别部应该计算基于构成作为强分类器的同一对象分类器的多个弱分类器计算出的得分的总和，并且如果所计算出的总和等于或大于阈值，则应该将获取部获取的图像对中的一幅图像和另一幅图像中出现的对象识别为相同的对象。[0020]	根据本公开内容的第二实施例，提供了一种识别装置的识别方法，该识别装置包括存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器， 提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。该识别方法包括以下步骤：获取图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；通过使用由存储部中所存储的信息表示的多个特征提取滤波器，在所获取的图像对中的一幅图像和另一幅图像中提取所设置的各个特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及当所计算出的相关性被提供作为输入时，基于同一对象分类器，识别所获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0021]	根据本公开内容的第二实施例，提供了一种用于使计算机执行识别装置的处理的程序，该识别装置包括存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器， 其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。该处理包括以下步骤：获取图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；通过使用由存储部中所存储的信息表示的多个特征提取滤波器，在所获取的图像对中的一幅图像和另一幅图像中提取所设置的各个特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及当所计算出的相关性被提供作为输入时，基于同一对象分类器，识别所获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0022]	在本公开内容的第一实施例中，获取相同对象出现的多个图像对和不同对象出现的多个图像对，在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点，选择设置在一幅图像和另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器。另外，通过使用所选择的多个特征提取滤波器来提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征，计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性，并且基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0023]	在本公开内容的第二实施例中，获取图像对，在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点，并且通过使用由存储部中所存储的信息表示的多个特征提取滤波器来在所获取的图像对中的一幅图像和另一幅图像中提取所设置的各个特征点的特征。另外，计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性，并且当计算部所计算出的相关性被提供作为输入时，基于同一对象分类器，识别所获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0024]	根据本公开内容的实施例，期望更迅速地且更精确地识别两幅图像中出现的对象是否是同一对象。附图说明[0025]	图1是示出相关技术中的面部识别的流程的图；[0026]	图2是示出盖伯滤波器的图；[0027]	图3是示出根据本公开内容的实施例的学习装置的示例性配置的框图；[0028]	图4是示出学习装置执行的学习的图；[0029]	图5是示出设置有特征点的面部图像的示例的图；[0030]	图6A至图6D是示出特征提取滤波器的示例的图；[0031]	图7是示出滤波器组的示例的图；[0032]	图8是示出相关值计算的示例的图；[0033]	图9是示出中间图像生成的示例的图；[0034]	图10是示出使用图9的中间图像的特征提取的示例的图；[0035]	图11是示出计算图9的中间图像的各个像素的像素值的方法的示例的图；[0036]	图12是示出另一中间图像生成的示例的图；[0037]	图13是示出使用图12的中间图像进行特征提取的示例的图；[0038]	图14是示出计算图12的中间图像的各个像素的像素值的方法的示例的图；[0039]	图15是示出学习装置的处理的流程图；[0040]	图16是示出面部的轮廓部分的特征提取的图；[0041]	图17是示出识别装置的示例性配置的框图；[0042]	图18是示出使用识别装置的识别的图；[0043]	图19是示出识别装置的处理的流程图；[0044]	图20是示出信息处理设备的示例性配置的框图；以及[0045]	图21是示出计算机的示例性配置的框图。具体实施方式[0046]	根据本公开内容的实施例，公开了一种学习装置，该学习装置包括：获取部，其获取相同对象出现的多个图像对和不同对象出现的多个图像对；设置部，其在获取部获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择部，其选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；提取部，其通过使用选择部选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算部，其计算通过提取部从一幅图像中提取的特征与通过提取部从另一幅图像中提取的特征之间的相关性； 以及学习部，其基于标记信息和计算部计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0047]	根据本公开内容的另一实施例，公开了一种学习方法，包括以下步骤：获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相10关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。[0048]	根据本公开内容的又一实施例，公开了一种识别装置，该识别装置包括：存储部， 其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器，提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象；获取部，其获取图像对；设置部，其在获取部获取的每个图像对中的一幅图像和另一幅图像上设置特征点；提取部，其通过使用由存储部中所存储的信息表示的多个特征提取滤波器，在获取部获取的图像对中的一幅图像和另一幅图像中提取设置部设置的各个特征点的特征；计算部，其计算通过提取部从一幅图像中提取的特征与通过提取部从另一幅图像中提取的特征之间的相关性；以及识别部，其在计算部计算出的相关性被提供作为输入时，基于同一对象分类器，识别获取部获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0049]	根据本公开内容的再一实施例，公开了一种识别装置的识别方法，该识别装置包括存储部，其存储表示多个特征提取滤波器的信息和通过学习装置的学习而生成的同一对象分类器的信息，其中，学习装置获取相同对象出现的多个图像对和不同对象出现的多个图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；选择设置在该一幅图像和该另一幅图像的相同位置处的多个指定特征点，由此针对每个指定特征点选择用于提取指定特征点的特征的特征提取滤波器；通过使用所选择的多个特征提取滤波器， 提取一幅图像和另一幅图像中的每幅图像的指定特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及基于标记信息和所计算出的相关性，学习用于识别两幅图像中出现的对象是否相同的同一对象分类器，其中，该标记信息表示一幅图像和另一幅图像中出现的对象是否是同一对象。该识别方法包括以下步骤：获取图像对；在所获取的每个图像对中的一幅图像和另一幅图像上设置特征点；通过使用由存储部中所存储的信息表示的多个特征提取滤波器，在所获取的图像对中的一幅图像和另一幅图像中提取所设置的各个特征点的特征；计算从一幅图像中提取的特征与从另一幅图像中提取的特征之间的相关性；以及当所计算出的相关性被提供作为输入时，基于同一对象分类器，识别所获取的图像对中的一幅图像和另一幅图像中出现的对象是否是同一对象。[0050]	关于学习装置[0051]	学习装置的配置[0052]	图3是示出根据本公开内容的实施例的学习装置的示例性配置的框图。[0053]	图3的学习装置1通过统计学习而生成同一人物分类器，该同一人物分类器用于识别两幅图像中出现的面部是否是同一人物的面部。学习装置1生成的同一人物分类器的数据被提供给识别装置，该识别装置实际执行关于两幅图像中出现的面部是否是同一人物的面部的识别。[0054]	在学习装置1中，如图4所示，作为用于学习的图像，输入了同一人物的面部出现的多个图像对和不同人物的面部出现的多个图像对。图像对Pll与P12、图像对P21与P22、 以及图像对Pnl与Pn2中的每一对均是同一人物的面部出现的图像对。另一方面，图像对 P11’与P12’、图像对P21’与P22’、以及图像对Riil’与Rii2’中的每一对均是不同人物的面部出现的图像对。[0055]	学习装置1在面部图像上的多个特征点当中随机地选择预定数量的特征点，其中，每个面部图像均是每幅输入图像中出现的面部部分的图像。另外，学习装置1在预先提供的多个特征提取滤波器当中随机地选择用于提取在所选择的特征点处的面部的特征的特征提取滤波器，并且将该特征提取滤波器设置为弱分类器的候选。[0056]	在图4的示例中，在图像Ρ2Γ和P22’中的每幅图像的特征点当中选择在对应位置（每个位置均是各幅图像上的相同位置）处的四个特征点。另外，选择滤波器组s，该滤波器组由用于提取在各个特征点处的面部的特征的四个特征提取滤波器组成。在学习装置 1中，如图像Ρ2Γ和图像P22’的每幅图像中所示，由白色矩形区域和黑色矩形区域组成的多个不同滤波器被提供作为特征提取滤波器。即，学习装置1通过使用矩形滤波器来提取矩形特征，并且通过被称为提升（诸如，Adaboost)的机器学习来执行分类器的学习。[0057]	学习装置1通过使用所选择的滤波器组s从每幅图像中提取特征，计算参数是从每个图像对的一幅图像中提取的特征的特征向量与参数是从每个图像对的另一幅图像中提取的特征的特征向量之间的相关值。学习装置1基于作为输入（特征量）的、所计算出的相关值而学习分类器，并且通过针对每轮提升重新选择滤波器组来重复地执行学习。[0058]	如上所述，在学习装置1中，随机地选择特征点和特征提取滤波器，并且基于根据每个图像对计算出的特征向量的相关值执行分类器的学习。从而，学习装置1能够通过组合特征来在学习中使用各种位置的特征，并因而能够生成能够进一步改善识别精确度的分类器。稍后将描述学习装置1的一系列处理。[0059]	图3的图像获取部11获取同一人物的面部出现的多个图像对和不同人物的面部出现的多个图像对。学习装置1接收标记信息的输入，该标记信息表示哪对是同一人物的面部出现的图像对以及哪对是不同人物的面部出现的图像对。图像获取部11获取的图像是数码摄相机拍到的且具有各种大小和角度的人物的面部出现的图像。图像获取部11将所获取的图像输出到面部检测部12。[0060]	面部检测部12通过经由对从图像获取部11提供的图像的分析来检测包括在各幅图像中的人物的面部的部分，生成面部图像。面部检测部12将所生成的面部图像输出到特征点设置部13和归一化部14。[0061]	特征点设置部13在从面部检测部12提供的每幅面部图像上设置多个特征点，并且将特征点的信息输出到归一化部14。[0062]	图5是示出设置有特征点的面部图像的示例的图。图5的图像Pll与P12构成一对。在图5的示例中，垂直方向上的9个特征点和水平方向上的6个特征点（S卩，总共M 个特征点）设置在图像Pll和P12中的每个的对应位置处。[0063]	归一化部14通过分析从面部检测部12提供的面部图像来检测面部的朝向，并且执行归一化处理（诸如，对面部图像进行变换的仿射变换），以使得特征点设置部13设置的特征点的位置到达基准位置。从而，即使当从面部检测部12提供的面部图像中出现的面部未朝向正面时，也将面部图像变换成如同朝向正面的面部出现的图像那样的图像。归一化部14将面部图像输出到中间图像生成部20，并且将归一化后的面部图像上的特征点的信息输出到滤波器组选择部15，其中，每幅面部图像被变换使得特征点的位置到达基准位置。[0064]	滤波器组选择部15在每幅面部图像上所设置的多个特征点当中随机地选择预定数量的特征点。另外，滤波器组选择部15在多个特征提取滤波器当中随机地选择用于提取在所选择的各个特征点处的面部的特征的特征提取滤波器。[0065]	图6A至图6D是示出特征提取滤波器的示例的图。[0066]	图6A至图6D中所示的二维滤波器是矩形滤波器。矩形滤波器由白色区域和黑色区域构成，其具有以水平方向上的线、垂直方向上的线、或者相对于某一方向以45度倾斜的线分割的预定数量的矩形。[0067]	使用矩形滤波器提取特征是这样执行的：将矩形滤波器布置在图像上的指定位置处，并且计算包括在白色区域中的像素的亮度值的总和与包括在黑色区域中的像素的亮度值的总和之间的差。例如，图6A的两个矩形滤波器适合于提取边缘的特征，而图6B和图6C 的三个矩形滤波器适合于提取线的特征。[0068]	滤波器组选择部15在针对随机选择的每个特征点的这些特征提取滤波器当中随机选择指定的特征提取滤波器，并且确定滤波器组。[0069]	图7是示出滤波器组的示例的图。[0070]	图7示出滤波器组si至sn这η个滤波器组。对于例如每轮提升选择每个滤波器组。在图7中，附加有数字作为下标的r表示特征提取滤波器（矩形滤波器），并且ρ表示特征点。（rn，pxy)表示如下的特征提取滤波器是特征提取滤波器rn:其被选择为用于提取在特征点Pxy处的面部的特征的滤波器，其中特征点Pxy是在χ列和y行的位置处的特征点。[0071]	滤波器组Sl由在特征点P13处选择的特征提取滤波器A、在特征点p51处选择的特征提取滤波器r5、在特征点处选择的特征提取滤波器r3.....以及在特征点P43处选择的特征提取滤波器ι&#183;8组成。另外，滤波器组s2由在特征点P33处选择的特征提取滤波器r3、在特征点P62处选择的特征提取滤波器r5、在特征点p63处选择的特征提取滤波器η.....以及在特征点P^5处选择的特征提取滤波器r7组成。[0072]	滤波器组选择部15将滤波器组的信息输出到特征提取部16和学习部18，其中，每个滤波器组由如上所述选择的预定数量的滤波器组成。[0073]	特征提取部16通过使用滤波器组选择部15选择的滤波器组，从所有面部图像中提取特征。特征提取部16针对每幅面部图像计算参数是从每幅面部图像中提取的特征的特征向量，并且将所计算出的特征向量的信息输出到相关值计算部17。[0074]	另外，在特征提取部16中，在提取特征时所使用的面部图像不是面部检测部12生成并且经归一化部14归一化的面部图像，而是中间图像生成部20生成的中间图像。如稍后所述，在中间图像生成部20中，基于面部检测部12生成并且经归一化部14归一化的面部图像，生成积分图像作为中间图像。[0075]	相关值计算部17基于从特征提取部16提供的信息，计算根据构成输入到学习装置1的、用于学习的每个图像对的一幅图像（面部图像）计算出的特征向量与根据另一幅面部图像计算出的特征向量之间的相关值。[0076]	图8是示出相关值计算的示例的图。[0077]	将描述选择图7的滤波器组Sl的情况。在这种情况下，从构成同一对的一幅面部图像中，通过分别使用特征提取滤波器巧、r5、r3.....和ι&#183;8提取在特征点p13、p51、P23.....和P43处的特征，从而生成参数是这些特征的特征向量VI。同样地，从构成同一对的另一幅面部图像中，通过分别使用特征提取滤波器I^r5I3.....和ι&#183;8提取在特征点p13’、P51 ’、P23 ’.....和P43 ’处的特征，从而生成参数是这些特征的特征向量VI’。特征点ρ13、ρ51、P23.....和P43中的每个特征点和特征点P13’、Ρ51’、Ρ23’.....和P43’中的每个特征点在每幅图像上的相同位置处。[0078]	相关值计算部17计算特征向量Vl与特征向量VI’的对应参数之间的相关值，并且生成参数是所计算出的相关值的相关向量。相关值计算部17针对用于学习的所有图像对中的每一对生成这样的相关向量，并且将基于每一对生成的相关向量的信息输出到学习部18。[0079]	当将上述滤波器组用作弱分类器的候选时，学习部18基于相关值计算部17生成的相关向量，使用例如Adaboost执行统计学习，从而生成由多个弱分类器构成的同一人物分类器。[0080]	即，当输入用于学习的N个图像对时，学习部18分配1/Ν作为相关值计算部17使用某一滤波器组生成的N个相关向量中的每个相关向量的权重。学习部18基于N个相关向量和标记信息（在同一人物的情况下为+1，否则为-1)执行学习，从而生成弱分类器。[0081]	此后，学习部18增加在前一阶段未被弱分类器分类为学习目标的样本（用于学习的图像对）的权重，并且基于通过使用不同滤波器组生成的N个相关向量而重复地执行加权学习。学习部18使得学习数据存储部19存储初始数据，该初始数据由通过学习获得的滤波器组的信息和各个弱分类器的参数、以及关于同一人物分类器的信息（诸如，可靠性） 组成。每个弱分类器均在接收到相关向量的输入时输出预定得分，其中，该相关向量是通过使用与在弱分类器的学习中所使用的滤波器组相同的滤波器组而从面部图像提取的。[0082]	中间图像生成部20基于从归一化部14提供的面部图像生成积分图像，并且将这些图像作为中间图像输出到特征提取部16。如上所述，在学习装置1中使用的特征提取滤波器是由白色矩形区域和黑色矩形区域组成的矩形滤波器。生成中间图像，以便容易地计算包括在具有指定形状的白色区域或黑色区域中的像素的亮度值的总和。[0083]	图9是示出中间图像生成的示例的图。[0084]	中间图像的大小（像素数量)与原始图像的大小相同。假设将指定位置（诸如， 原始图像的四个角中的左上角）设置为基准位置，则中间图像的每个像素的像素值表示包括在如下四角形中的原始图像的各个像素的亮度值的总和：其具有将基准位置与每个像素的位置连接的对角线。[0085]	在图9的示例中，在左上角处的点Al被设置为基准位置。在中间图像上的点Α4处的像素的像素值表示包括在如下四角形的区域中的原始图像的各个像素的亮度值的总和： 其对角线连接点Al和点Α4且其顶点是点Α1、Α2、Α4和A3。同样地，例如，在中间图像上的右下角处的像素的像素值表示原始图像的全部像素的亮度值的总和。[0086]	图10是示出使用中间图像的特征提取的示例的图。：计算包括在如下四角形的区域中的原始图像的像素的亮度值的总和：其以图10中的阴影线表示并且其顶点是点A13、A14、A4和A15。点All是点 Al与A2之间的点，并且点A12是点Al与点A3之间的点。点A14是点A2与A4之间的点， 并且点A15是点A3与点A4之间的点。点A13是将点A12与点A14连接的水平线与将点 All与点A15连接的垂直线之间的交点。[0088]	在这种情况下，在中间图像上的点A13处的像素的像素值表示包括在顶点是点 A1、A11、A13和A12的四角形的区域中的原始图像的像素的亮度值的总和。另外，在点A14 处的像素的像素值表示包括在顶点是Al、A2、A14和A12的四角形的区域中的原始图像的像素的亮度值的总和。在点A15处的像素的像素值表示包括在顶点是点A1、A11、A15和A3 的四角形的区域中的原始图像的像素的亮度值的总和。[0089]	如上所述，在点A4处的像素的像素值表示包括在顶点是点A1、A2、A4和A3的四角形的区域中的原始图像的像素的亮度值的总和。因此，包括在以阴影线表示的区域中的原始图像的像素的亮度值的总和可以这样获得：从在点A4处的像素的像素值中减去在点A14 处的像素的像素值和在点A15处的像素的像素值，并且与点A13处的像素的像素值相加。[0090]	图11是示出计算中间图像的各个像素的像素值的方法的示例的图。[0091]	将描述计算图11的下侧的以阴影线表示的中间图像的像素P’ 101的亮度值的情况。中间图像的像素P’ 1Q1、P’ 1Q2、P’ 1Q3和P’ 1Q4分别对应于原始图像的像素P1Q1、P1Q2、P1Q3、Pl04o[0092]	在位于中间图像上的指定位置处的2X2四个像素当中在右下侧的单个像素的像素值可以这样获得：从右上像素的像素值与左下像素的像素值的总和中减去左上像素的像素值，并且与原始像素中在对应位置处的单个像素的亮度值相加。即，可以通过从像素P’ 1Q3 的像素值与像素P’ 104的像素值的总和中减去像素P’ 102的像素值、并且与原始图像的像素 P101的亮度值相加，获得中间图像的像素P’ 101的像素值。[0093]	中间图像生成部20通过使用输入到学习装置1的所有图像作为原始图像，生成这样的中间图像。一旦生成这样的中间图像，则当特征提取部16通过使用每个区域以水平线或垂直线分割的多个特征提取滤波器（垂直滤波器和水平滤波器）来执行特征提取时，可以容易地执行计算。[0094]	图12是示出另一中间图像生成的示例的图。[0095]	中间图像生成部20还使用特征提取滤波器（倾斜滤波器）生成用于利于计算的中间图像，其中，该特征提取滤波器的区域以相对于水平线或垂直线以45度倾斜的线来分割。中间图像被生成为使得其每个像素的像素值表示包括在如下三角形的区域中的原始图像的各个像素的亮度值的总和：该三角形的顶点处于每个像素的位置处，底边是指定边 (诸如，原始图像的四个边的上边），并且顶角是90度。[0096]	在图12的示例中，上边是该三角形的底边。在中间图像上的点A33处的像素的像素值表示包括在顶角为90度且顶点是点A31、A32和A33的三角形的区域中的原始图像的各个像素的亮度值的总和。[0097]	图13是示出使用图12的中间图像的特征提取的示例的图。[0098]	将描述计算包括在如下四角形的区域中的原始图像的像素的亮度值的总和的情况：其以图13中的阴影线表示且顶点是点A43、A45、A33和A44。点A44是点A31与A33之15间的点，并且点A45是点A33与点A32之间的点。点A42是与将点A32与点A33连接的线平行并且通过点A44的线与将点A31与点A32连接的线之间的交点。点A41是与将点A31与点A33连接的线平行且通过点A45的线与将点A31与点A32连接的线之间的交点。点A43 是将点A41与点A45连接的线与将点A42与点A44连接的线之间的交点。[0099]	在这种情况下，在点A43处的像素的像素值表示包括在顶点是点A41、A42和A43 的三角形的区域中的原始图像的像素的亮度值的总和。同样地，在点A44处的像素的像素值表示包括在顶点是点A31、A42和A44的三角形的区域中的原始图像的像素的亮度值的总和。在点A45处的像素的像素值表示包括在顶点是A41、A32和A45的三角形的区域中的原始图像的像素的亮度值的总和。[0100]	在中间图像上的点A33处的像素的像素值表示包括在顶点是点A31、A32和A33的三角形的区域中的原始图像的像素的亮度值的总和。相应地，可以通过从在点A33处的像素的像素值中减去在点A44处的像素的像素值和在点A45处的像素的像素值、并且与点A43 处的像素的像素值相加，获得包括在以阴影线表示的区域中的原始图像的像素的亮度值的总禾口。[0101]	图14是示出计算图12的中间图像的各个像素的像素值的方法的示例的图。[0102]	将描述计算图14的下侧的以阴影线表示的中间图像的像素P’ m的亮度值的情况。中间图像的像素P’ m、P’ 112、P’ 113和P’ 114分别对应于原始图像的像素pm、p112、p113、P114O[0103]	位于中间图像上的指定位置处的单个像素的像素值可以这样获得：从左上像素的像素值与右上像素的像素值的总和中减去正上方像素的像素值、并且与原始图像中在对应位置处的单个像素的亮度值相加。即，可以通过从中间图像的像素P’ 112的像素值与像素 P’ Π4的像素值的总和中减去像素P’ Π3的像素值、并且与原始图像的像素P111的亮度值相加，获得中间图像的像素P’ m的像素值。[0104]	中间图像生成部20通过将输入到学习装置1的所有图像作为原始图像，生成这样的中间图像。从而，当特征提取部16通过使用倾斜滤波器来执行特征提取时，可以容易地执行计算。[0105]	学习装置的操作[0106]	接下来，参照图15的流程图，将描述如上所述配置的学习装置1的处理。[0107]	在步骤Sl中，图像获取部11获取同一人物的面部出现的多个图像对以及不同人物的面部出现的多个图像对。[0108]	在步骤S2中，面部检测部12通过检测包括在图像获取部11获取的各幅图像中的人物的面部的部分，生成面部图像。[0109]	在步骤S3中，特征点设置部13在每幅面部图像上设置多个特征点。[0110]	在步骤S4中，归一化部14通过分析面部图像来检测面部的朝向，并且对面部图像进行归一化，以使得特征点的位置到达基准位置。[0111]	在步骤S5中，中间图像生成部20基于归一化后的面部图像生成中间图像。中间图像生成部20将用于通过使用垂直/水平滤波器提取特征的中间图像和用于通过使用倾斜滤波器提取特征的中间图像输出到特征提取部16，其中，根据由图像获取部11获取作为原始图像的各幅图像生成中间图像。[0112]	在步骤S6中，滤波器组选择部15随机地选择在面部图像上所设置的多个特征点。 另外，滤波器组选择部15随机地选择用于提取在所选择的各个特征点处的面部的特征的特征提取滤波器。[0113]	在步骤S7中，特征提取部16通过将滤波器组选择部15选择的滤波器组应用于中间图像，提取所有面部图像的特征。特征提取部16针对每幅面部图像计算参数是所提取的特征的特征向量。[0114]	这里，特征提取部16可通过某一特征提取滤波器提取特征，从而生成特征向量， 并且此后，通过黑色区域和白色区域彼此替换的相同特征提取滤波器提取在相同位置处的特征，从而生成特征向量。使用黑色区域和白色区域被替换的特征提取滤波器的特征提取对应于包括在白色区域中的像素的亮度值的总和与包括在黑色区域中的像素的亮度值的总和之间的差的绝对值的计算。[0115]	从而，具体地，如图16所示，当通过使用白色区域和黑色区域分别布置在面部的轮廓的一部分的左侧和右侧的特征提取滤波器R来提取面部的轮廓的一部分处的特征时， 可以执行不取决于背景的亮度的分类器的学习。[0116]	另外，在通过某一特征提取滤波器提取在某一特征点处的特征后，通过经由同一特征提取滤波器提取在与该特征点相邻的像素点处的特征，可以生成参数是特征的总和或平均值的特征向量。即，通过使位置偏移并且通过同一特征提取滤波器提取特征，特征提取部16计算特征的总和或平均值。[0117]	以这种方式，可以生成不管面部的形状变化如何都能够精确地执行识别的分类器。可任意设置相邻像素点的范围，并且在X轴方向和y轴方向上的偏移范围可彼此不同。[0118]	另外，考虑到特征提取等所消耗的时间，可以略过绝对值的计算和特征提取滤波器的位置偏移的特征提取。[0119]	返回到图15的描述，在步骤S8中，相关值计算部17计算根据构成用于学习的图像对的一幅图像计算出的特征向量与根据构成同一对的另一幅图像计算出的特征向量之间的相关值，从而生成表示相关性的相关向量。[0120]	在步骤S9中，学习部18基于相关值计算部17生成的相关向量执行学习。[0121]	在步骤SlO中，学习部18确定学习是否结束。如果在步骤SlO中确定学习未结束， 则过程返回到步骤S6，重新选择滤波器组，并且重复提升。[0122]	相反，如果在步骤SlO中确定学习结束，则在步骤Sll中，学习部18使学习数据存储部19存储关于作为由多个弱分类器构成的强分类器的同一人物分类器的信息，并且结束处理。[0123]	如上所述，通过组合通过使用多个特征提取滤波器从多个特征点提取的特征来执行学习。以这种方式，可以生成能够进一步改善识别的精确性的分类器。例如，类似于通过同时从两幅图像中的每一幅中提取面部的眼睛和鼻子的特征来执行学习的方式，通过使用在面部上的多个位置处的特征的组合，分类器的精确性可提高。[0124]	另外，生成积分图像作为中间图像，并且通过使用矩形滤波器来执行特征提取。因此，可以使用多个特征提取滤波器与多个特征点的组合，容易地执行更加复杂的计算。当将原始图像直接设置为特征提取的目标而不生成积分图像时，需要亮度值的卷积运算，但在该实施例中，不需要执行该运算。[0125]	关于识别装置[0126]	识别装置的配置[0127]	图17是示出识别装置的示例性配置的框图。[0128]	图17的识别装置2基于学习装置1生成的同一人物分类器，识别两幅图像中出现的面部是否是同一人物的面部。关于同一人物分类器的信息通过网络或通过诸如存储卡的记录介质从学习装置1被提供给识别装置2。[0129]	如图18所示，识别装置2接收人物的面部出现的图像对的输入作为识别目标图像。如空心箭头的箭尖所示，识别装置2基于作为在学习期间选择的特征提取滤波器组的滤波器组sll，提取各幅图像中出现的面部的特征。识别装置2计算参数是从一幅图像中提取的特征的特征向量与参数是从另一幅图像中提取的特征的特征向量之间的相关性，并且通过使用这些向量作为到通过滤波器组sll学习的弱分类器的输入，计算得分。[0130]	同样地，识别装置2基于滤波器组sl2提取各幅图像中出现的面部的特征，并且计算参数是从一幅图像中提取的特征的特征向量与参数是从另一幅图像中提取的特征的特征向量之间的相关性。当所计算出的相关性被提供作为到通过滤波器组sl2学习的弱分类器的输入时，识别装置2计算得分。识别装置2通过各个弱分类器计算这样的得分。例如， 如果得分的总和等于或大于阈值，则确定两幅输入图像中出现的面部是同一人物的面部。[0131]	以这种方式，基于学习装置1生成的同一人物分类器，识别装置2能够精确地识别两幅输入图像中出现的面部是否是同一人物的面部。[0132]	图17的图像获取部31获取人物的面部出现的图像对，并且将图像对输出到面部检测部32。[0133]	类似于学习装置1的面部检测部12，面部检测部32通过经由对从图像获取部31 提供的图像的分析来检测包括在各幅图像中的人物的面部的部分，生成面部图像。面部检测部32将所生成的面部图像输出到特征点设置部33和归一化部34。[0134]	特征点设置部33在从面部检测部32提供的每幅面部图像上设置多个特征点，并且将特征点的信息输出到归一化部；34。[0135]	类似于学习装置1的归一化部14，归一化部34通过分析从面部检测部32提供的面部图像检测面部的朝向，并且对面部图像进行归一化，以使得特征点设置部33设置的特征点的位置到达基准位置。归一化部34将归一化后的面部图像输出到中间图像生成部39， 并且将特征点的信息输出到特征提取部35。[0136]	特征提取部35基于存储在学习数据存储部38中的信息，选择滤波器组。学习数据存储部38存储关于学习装置1生成的同一人物分类器的信息。滤波器组的信息包括表示构成滤波器组的每个特征提取滤波器在特征提取中使用哪个特征点的信息。[0137]	特征提取部35通过使用构成所选择的滤波器组的各个特征提取滤波器，提取在每幅面部图像的指定特征点处的特征。特征提取部35针对每幅面部图像计算参数是从每幅面部图像中提取的特征的特征向量，并且将所计算出的特征向量的信息输出到相关值计算部36。另外，在特征提取部35中在提取特征时使用的面部图像可以是中间图像生成部 39生成的中间图像，并且可以是面部检测部32生成且经归一化部34归一化的面部图像。[0138]	相关值计算部36基于从特征提取部35提供的信息，计算根据作为输入到识别装置2的识别目标的对中的一幅面部图像计算出的特征向量与根据另一幅面部图像计算出的特征向量之间的相关向量。相关值计算部36将所计算出的相关向量的信息输出到识别部37。[0139]	识别部37从学习数据存储部38读出构成同一人物分类器的弱分类器的信息，并且计算当相关值计算部36计算出的相关向量被提供作为到每个弱分类器的输入时的得分。如果通过使用各个弱分类器计算出的得分的总和等于或大于阈值，则识别部37确定两幅输入图像中出现的面部是同一人物的面部。如果总和小于阈值，则识别部37确定两幅输入图像中出现的面部不是同一人物的面部。识别部37的确定结果可显示在例如连接到识别装置2的显示器上，或者可作为声音从连接到识别装置2的扬声器输出。[0140]	类似于学习装置1的中间图像生成部20，中间图像生成部39基于从归一化部34 提供的面部图像，适当地生成积分图像，并且将图像作为中间图像输出到特征提取部35。[0141]	识别装置的操作[0142]	接下来，参照图19的流程图，将描述识别装置2的处理。[0143]	在步骤S21中，图像获取部31获取识别目标的图像对。[0144]	在步骤S22中，面部检测部32通过检测包括在图像获取部31获取的各幅图像中的人物的面部的部分，生成面部图像。[0145]	在步骤S23中，特征点设置部33在每幅面部图像上设置多个特征点。[0146]	在步骤S24中，归一化部34对面部图像进行归一化。[0147]	在步骤S25中，中间图像生成部39基于归一化后的面部图像生成中间图像。类似于学习装置1的中间图像生成部20，中间图像生成部39根据图像获取部31获取作为原始图像的各幅图像，生成用于通过使用垂直/水平滤波器提取特征的中间图像以及用于通过使用倾斜滤波器提取特征的中间图像。[0148]	在步骤S26中，特征提取部35基于存储在学习数据存储部38中的信息，选择滤波器组。[0149]	在步骤S27中，特征提取部35通过使用构成所选滤波器组的各个特征提取滤波器，提取在每幅面部图像的指定特征点处的特征。特征提取部35针对每幅面部图像计算参数是从每幅面部图像中提取的特征的特征向量。[0150]	在步骤S28中，相关值计算部36生成根据作为输入到识别装置2的识别目标的对的一幅面部图像计算出的特征向量与根据另一幅面部图像计算出的特征性之间的相关向量。[0151]	在步骤S29中，识别部37从学习数据存储部38读出构成同一人物分类器的弱分类器的信息，并且计算当相关值计算部36计算出的相关向量被提供作为到每个弱分类器的输入时的得分。[0152]	在步骤S30中，识别部37将新计算出的得分与到目前为止计算出的得分相加。[0153]	在步骤S31中，识别部37确定识别是否结束。如果在步骤S31中确定识别未结束，则过程返回到步骤S26，重新选择滤波器组，并且重复得分计算。例如，在通过使用全部弱分类器计算得分以前，重复使用弱分类器的得分计算。[0154]	相反，如果在步骤S31中确定识别结束，则在步骤S32中，识别部37基于通过使用各个弱分类器计算出的得分的总和，确定两幅图像中出现的面部是否是同一人物的面部。 识别部37输出识别结果，并且结束处理。[0155]	通过上述处理，可以精确地识别两幅图像中出现的面部是否是同一人物的面部。[0156]	变型示例[0157]	在以上描述中，学习装置1和识别装置2是不同的装置。然而，如图20所示，可在单个信息处理设备3中实现学习装置1和识别装置2。由在信息处理设备3中实现的学习装置1获得的学习结果被提供给识别装置2，并且用在关于两幅图像中出现的面部是否是同一人物的面部的识别中。[0158]	另外，描述了在关于面部是否属于同一人物的识别中所使用的分类器的学习以及使用分类器的识别。然而，上述处理可应用于在关于两幅图像中出现的不同对象是否相同的识别中所使用的分类器的学习以及使用分类器的识别。[0159]	计算机的示例性配置[0160]	上述一系列处理可通过硬件执行，以及可通过软件执行。当这一系列处理通过软件执行时，构成软件的程序从程序记录介质被安装在内置于专用硬件中的计算机、通用个人计算机等中。[0161]	图21是示出通过程序执行上述一系列处理的计算机的硬件的示例性配置的框图。[0162]	CPU(中央处理单元）101、R0M(只读存储器）102、和RAM(随机存取存储器）103通过总线104彼此连接。[0163]	总线104还连接到输入/输出接口 105。输入/输出接口 105连接到：由键盘、鼠标等构成的输入部106 ；以及由显示器、扬声器等构成的输出部107。另外，输入/输出接口 105连接到：由硬盘、非易失性存储器等构成的存储部108 ；由网络接口等构成的通信部 109 ；以及驱动可拆卸介质111的驱动器110。[0164]	在如上所述配置的计算机中，例如，CPU 101通过输入/输出接口 105和总线104 将存储在存储部108中的程序加载到RAM 103中并且执行该程序，从而执行上述一系列处理。[0165]	CPU 101执行的程序存储在例如可拆卸介质111中。可替选地，程序通过有线或无线传输介质（诸如，局域网、因特网或数字广播）提供，并且安装在存储部108中。[0166]	另外，计算机执行的程序可以是以本说明书的描述顺序按时间顺序执行处理的程序，并且可以是并行或者在诸如调用定时的必要定时执行处理的程序。[0167]	本公开内容包含与2010年10月12日向日本专利局提交的日本优先权专利申请 JP 2010-229368中所公开的主题内容相关的主题内容，在此通过引用将其全文合并于此。[0168]	本领域的技术人员应当理解，在所附权利要求或其等同方案的范围内，根据设计需要和其它因素，可进行各种修改、组合、子组合以及变更。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101030244A?cl=zh">CN101030244A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2006年3月3日</td><td class="patent-data-table-td patent-date-value">2007年9月5日</td><td class="patent-data-table-td ">中国科学院自动化研究所</td><td class="patent-data-table-td ">基于人体生理图像中排序测度特征的自动身份识别方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101504719A?cl=zh">CN101504719A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年2月5日</td><td class="patent-data-table-td patent-date-value">2009年8月12日</td><td class="patent-data-table-td ">索尼株式会社</td><td class="patent-data-table-td ">图像处理装置、方法和程序</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090041312">US20090041312</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年8月6日</td><td class="patent-data-table-td patent-date-value">2009年2月12日</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Image processing apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090196467">US20090196467</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年1月21日</td><td class="patent-data-table-td patent-date-value">2009年8月6日</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Image processing apparatus and method, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090238419">US20090238419</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年5月7日</td><td class="patent-data-table-td patent-date-value">2009年9月24日</td><td class="patent-data-table-td ">Fotonation Ireland Limited</td><td class="patent-data-table-td ">Face recognition training method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100214442">US20100214442</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年2月22日</td><td class="patent-data-table-td patent-date-value">2010年8月26日</td><td class="patent-data-table-td ">Yuiko Uemura</td><td class="patent-data-table-td ">Image display apparatus and image display method</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">非专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">参考文献</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">MICHAEL J. JONES等: "<a href='http://scholar.google.com/scholar?q="Face+Recognition+Using+Boosted+Local+Features"'>Face Recognition Using Boosted Local Features</a>", 《IEEE INTERNATIONAL CONFERENCE ON COMPUTER》, 30 April 2003 (2003-04-30), XP010949458</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">RAINER LIENHART等: "<a href='http://scholar.google.com/scholar?q="Empirical+Analysis+of+Detection+Cascades+of+Boosted+Classifiers+for+Rapid+Object+Detection"'>Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection</a>", 《MRL TECHNICAL REPORT》, 30 September 2003 (2003-09-30), pages 2, XP002374203</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009620000">G06K9/62</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009460000">G06K9/46</a></span></td></tr><tr><td class="patent-data-table-td "> 合作分类</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/00288">G06K9/00288</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=bShvBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6257">G06K9/6257</a></span></td></tr><tr><td class="patent-data-table-td "> 欧洲专利分类号</td><td class="patent-data-table-td "><span class="nested-value">G06K9/00F3</span>, <span class="nested-value">G06K9/62B7C</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年7月4日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年11月6日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/4e0b169a5c9e6db5957a/CN102542286A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E5%AD%A6%E4%B9%A0%E8%A3%85%E7%BD%AE_%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_%E8%AF%86%E5%88%AB%E8%A3%85%E7%BD%AE.pdf?id=bShvBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U2c8s5V7CgClapjQcxdLTBnQbq_ZA"},"sample_url":"https://www.google.com/patents/reader?id=bShvBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>