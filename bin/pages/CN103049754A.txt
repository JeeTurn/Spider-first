<!DOCTYPE html><html><head><title>专利 CN103049754A - 社交网络的图片推荐方法和装置 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="社交网络的图片推荐方法和装置"><meta name="DC.contributor" content="裴起震" scheme="inventor"><meta name="DC.contributor" content="张骞" scheme="inventor"><meta name="DC.contributor" content="赵立军" scheme="inventor"><meta name="DC.contributor" content="王宁" scheme="inventor"><meta name="DC.contributor" content="东软集团股份有限公司" scheme="assignee"><meta name="DC.date" content="2012-12-7" scheme="dateSubmitted"><meta name="DC.description" content="本发明提供一种社交网络的图片推荐方法和装置，所述方法包括：获取上传图片，并判断是否为人像图片，如果是人像图片，则从社交网络的图片数据库选取人像图片作为对比图片，如果是非人像图片，则从所述图片数据库选取非人像图片作为对比图片；提取上传图片的图片像素的颜色特征值；计算上传图片与从图片数据库中选取的对比图片之间的颜色特征距离；并根据颜色特征距离的大小向用户推荐相关图片。所述方法可以降低用户操作的复杂性，改善用户使用社交网络的体验、并提高社交网络推荐图片的准确性。"><meta name="DC.date" content="2013-4-17"><meta name="DC.relation" content="CN:102254043:A" scheme="references"><meta name="DC.relation" content="CN:102368746:A" scheme="references"><meta name="DC.relation" content="CN:102387094:A" scheme="references"><meta name="DC.relation" content="CN:102521253:A" scheme="references"><meta name="DC.relation" content="US:20070168357:A1" scheme="references"><meta name="citation_patent_publication_number" content="CN:103049754:A"><meta name="citation_patent_application_number" content="CN:201210524615"><link rel="canonical" href="https://www.google.com/patents/CN103049754A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN103049754A?cl=zh"/><meta name="title" content="专利 CN103049754A - 社交网络的图片推荐方法和装置"/><meta name="description" content="本发明提供一种社交网络的图片推荐方法和装置，所述方法包括：获取上传图片，并判断是否为人像图片，如果是人像图片，则从社交网络的图片数据库选取人像图片作为对比图片，如果是非人像图片，则从所述图片数据库选取非人像图片作为对比图片；提取上传图片的图片像素的颜色特征值；计算上传图片与从图片数据库中选取的对比图片之间的颜色特征距离；并根据颜色特征距离的大小向用户推荐相关图片。所述方法可以降低用户操作的复杂性，改善用户使用社交网络的体验、并提高社交网络推荐图片的准确性。"/><meta property="og:title" content="专利 CN103049754A - 社交网络的图片推荐方法和装置"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN103049754A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN103049754A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=RlnwBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN103049754A&amp;usg=AFQjCNHMpi0YwQyBLMHtRDk4Lq8n8J0WlA" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/941319260bf1a3324d6d/CN103049754A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/941319260bf1a3324d6d/CN103049754A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN103049754A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN103049754A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN103049754A?cl=zh" style="display:none"><span itemprop="description">本发明提供一种社交网络的图片推荐方法和装置，所述方法包括：获取上传图片，并判断是否为人像图片，如果是人像图片，则从社交网络的图片数据库选取人像图片作为对比图片，如果是非人像图片，则从所述图片数据库选取...</span><span itemprop="url">https://www.google.com/patents/CN103049754A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN103049754A - 社交网络的图片推荐方法和装置</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN103049754A - 社交网络的图片推荐方法和装置" title="专利 CN103049754A - 社交网络的图片推荐方法和装置"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN103049754 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210524615</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年4月17日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年12月7日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年12月7日</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210524615.5, </span><span class="patent-bibdata-value">CN 103049754 A, </span><span class="patent-bibdata-value">CN 103049754A, </span><span class="patent-bibdata-value">CN 201210524615, </span><span class="patent-bibdata-value">CN-A-103049754, </span><span class="patent-bibdata-value">CN103049754 A, </span><span class="patent-bibdata-value">CN103049754A, </span><span class="patent-bibdata-value">CN201210524615, </span><span class="patent-bibdata-value">CN201210524615.5</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%A3%B4%E8%B5%B7%E9%9C%87%22">裴起震</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%BC%A0%E9%AA%9E%22">张骞</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%B5%B5%E7%AB%8B%E5%86%9B%22">赵立军</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E5%AE%81%22">王宁</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E4%B8%9C%E8%BD%AF%E9%9B%86%E5%9B%A2%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">东软集团股份有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN103049754A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103049754A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103049754A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (2)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=RlnwBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210524615&amp;usg=AFQjCNE_nCC2CzHcx472BBxKIb3HlG9iQA"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=RlnwBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D103049754A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNH2BFI2K_AXyw3rv-1LjPXrwem6Iw"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT123276457" lang="ZH" load-source="patent-office">社交网络的图片推荐方法和装置</invention-title>
      </span><br><span class="patent-number">CN 103049754 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA110159496" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明提供一种社交网络的图片推荐方法和装置，所述方法包括：获取上传图片，并判断是否为人像图片，如果是人像图片，则从社交网络的图片数据库选取人像图片作为对比图片，如果是非人像图片，则从所述图片数据库选取非人像图片作为对比图片；提取上传图片的图片像素的颜色特征值；计算上传图片与从图片数据库中选取的对比图片之间的颜色特征距离；并根据颜色特征距离的大小向用户推荐相关图片。所述方法可以降低用户操作的复杂性，改善用户使用社交网络的体验、并提高社交网络推荐图片的准确性。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(15)</span></span></div><div class="patent-text"><div mxw-id="PCLM52602575" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种社交网络的图片推荐方法，包括：  a)获取上传图片，并判断该上传图片是否为人像图片，如果判断为是人像图片，则从所述社交网络的图片数据库中选取人像图片作为对比图片，如果判断为不是人像图片，则从所述社交网络的图片数据库中选取非人像图片作为对比图片；  b)获得所述上传图片和所述对比图片的颜色特征距离d	;以及  c)将与所述上传图片的颜色特征距离d较小的一个或多个对比图片推荐给所述上传图片的用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.如权利要求1所述的社交网络的图片推荐方法，其中，在步骤b)中，获得所述上传图片和所述对比图片的颜色特征距离d包括：  bl)从所述上传图片中提取各个图片像素的颜色特征值X，其中，X e [X(l，Xl]，X(l与X1为预定值，从所述上传图片中提取图片像素的颜色特征值的方法与从所述对比图片中提取或预先提取图片像素的颜色特征值的方法相同；  b2)与所述对比图片相对应，对所述上传图片做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Au (X)、以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bij(X)，其中，N、1、Mp j均为自然数，且I彡i彡N，Mi与i对应，I彡j彡Mi ;  b3)对于对所述上传图片做出的第i种预定划分，根据所述Aij(X)和Bij(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I &lt; i &lt; N，I彡j彡Mi ;以及  b4)根据所述屯获得所述上传图片和所述对比图片的颜色特征距离d，其中，I &lt;i&lt;N。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.如权利要求1或2所述的社交网络的图片推荐方法，其中，在步骤a)中，采用基于Haar特征和adaboost算法的人脸检测方法来判断所述上传图片是否为人像图片。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.如权利要求3所述的社交网络的图片推荐方法，其中，采用所述基于Haar特征和adaboost算法的人脸检测方法将所述图片数据库的图片分为人像图片和非人像图片。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.如权利要求2所述的社交网络的图片推荐方法，其中，在步骤bl)中，从所述上传图片或所述对比图片中提取图片像素的颜色特征值的步骤包括：  ①获取图片像素在色相-饱和度-亮度（HSV)色彩模型下的色相（H)、饱和度（S)和亮度（V)的值；  ②对所获取的所述图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00021.png"> <img id="icf0001" file="CN103049754AC00021.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00021.png" class="patent-full-image" alt="Figure CN103049754AC00021"> </a> </div>其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、…、[V” H8J为H的一个量化划分，[S。，S1)、…、[Sm SqJ为S的一个量化划分，[V。，V1)、…、[V" Vqv]为V的一个量化划分；③对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值x=hQsQv+sQs+v其中，Qs为S的量化级数，Qv为V的量化级数，并且有X e [x0, X1]，X0=O, X1= (Qh-1) QsQv+ (Qs-1) Qs+ (Qv-1)。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.如权利要求5所述的社交网络的图片推荐方法，其中，在归一化的HSV色彩模型下， Qh8，	Qs=	--4,	Qv=3 ；	<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00031.png"> <img id="icf0002" file="CN103049754AC00031.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00031.png" class="patent-full-image" alt="Figure CN103049754AC00031"> </a> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.如权利要求2所述的社交网络的图片推荐方法，其中，在步骤b3)中，由下述公式获得所述上传图片和所述对比图片在第i种预定划分下的颜色特征距离D1 ：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00032.png"> <img id="icf0003" file="CN103049754AC00032.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00032.png" class="patent-full-image" alt="Figure CN103049754AC00032"> </a> </div>其中，du为从所述对比图片的第i种预定划分中选取的第j个预定区域与从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域之间的颜色特征距离，Wij为 Clij的权重，Wij &gt; 0，且<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00033.png"> <img id="icf0004" file="CN103049754AC00033.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00033.png" class="patent-full-image" alt="Figure CN103049754AC00033"> </a> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.如权利要求2所述的社交网络的图片推荐方法，其中，在步骤b4)中，由下述公式获得所述上传图片和所述对比图片的颜色特征距离d ：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00034.png"> <img id="icf0005" file="CN103049754AC00034.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00034.png" class="patent-full-image" alt="Figure CN103049754AC00034"> </a> </div>其中，Wi为Cli的权重,Wi &gt; 0，且<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00035.png"> <img id="icf0006" file="CN103049754AC00035.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00035.png" class="patent-full-image" alt="Figure CN103049754AC00035"> </a> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.如权利要求2或7或8所述的社交网络的图片推荐方法，其中，在步骤b2)中，N=3 ；第一种预定划分将所述上传图片划分为中心区域和周围区域，并只选取该中心区域作为所述预定区域；第二种预定划分将所述上传图片划分为多个较大的区域，并选取该多个较大的区域作为所述预定区域；以及第三预定种划分将所述上传图片划分为多个较小的区域，并选取该多个较小的区域作为所述预定区域。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10.如权利要求9所述的社交网络的图片推荐方法，其中，在所述第一种预定划分中，中心区域的大小为所述上传图片大小的1/9 ;在所述第二种预定划分中，所述多个较大的区域包括4个大小相同的区域；在所述第三种预定划分中， 所述多个较小的区域包括16个大小相同的区域。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11.如权利要求7所述的社交网络的图片推荐方法，其中，N=3 ；第一种预定划分将所述上传图片划分为中心区域和周围区域，该中心区域的大小为所述上传图片大小的1/9，并只选取该中心区域作为所述预定区域；第二种预定划分将所述上传图片划分为4个大小相同的区域，并选取该4个区域作为所述预定区域；第三预定种划分将所述上传图片划分为16个大小相同的区域，并选取该16个区域作为所述预定区域；以及W11=I ；w2J=1/4, I &lt; j &lt; 4 ；w3J=1/16, I ^ j ^ 16。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12.如权利要求8所述的社交网络的图片推荐方法，其中，N=3 ；第一种预定划分将所述上传图片划分为中心区域和周围区域，该中心区域的大小为所述上传图片大小的1/9，并只选取该中心区域作为所述预定区域；第二种预定划分将所述上传图片划分为4个大小相同的区域，并选取该4个区域作为所述预定区域；第三预定种划分将所述上传图片划分为16个大小相同的区域，并选取该16个区域作为所述预定区域；以及w^l/2 ；w2=1/4 ；、w3=1/4。</div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13.	一种社交网络的图片推荐装置，包括：人像判断与选取单元，用于获取上传图片，并判断该上传图片是否为人像图片，如果判断为是人像图片，则从所述社交网络的图片数据库中选取人像图片作为对比图片，如果判断为不是人像图片，则从所述社交网络的图片数据库中选取非人像图片作为对比图片； 颜色特征距离获得单元，用于获得所述上传图片和所述对比图片的颜色特征距离d ；以及图片推荐单元，将与所述上传图片的颜色特征距离d较小的一个或多个对比图片推荐给所述上传图片的用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14.如权利要求13所述的社交网络的图片推荐装置，其中，所述颜色特征距离获得单元包括：图片像素颜色特征值提取部件，用于从所述对比图片或所述上传图片中提取各个图片像素的颜色特征值X，其中，X e [X(l, xj，Xci与X1为预定值；图片划分部件，用于对所述对比图片和所述上传图片做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Au (X)、以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bu(X)，其中，N、1、Mi, j均为自然数，且I≤i≤N，Mi与i对应，I≤j≤Mi ;  第一颜色特征距离获得部件，对于对所述上传图片做出的第i种预定划分，根据所述Aij(X)和Bu(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I ≤ i ≤ N, I ≤ j ≤ Mi ;以及  第二颜色特征距离获得部件，根据所述Cli获得所述上传图片和所述对比图片的颜色特征距离d，其中，I &lt; i SN。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15.如权利要求14所述的社交网络的图片推荐装置，其中，所述图片像素颜色特征值提取部件包括：  HSV值获取元件，获取图片像素在色相-饱和度-亮度（HSV)色彩模型下的色相（H)、饱和度（S)和亮度（V)的值；  HSV值量化元件，对所获取的所述图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值： <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00051.png"> <img id="icf0007" file="CN103049754AC00051.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AC00051.png" class="patent-full-image" alt="Figure CN103049754AC00051"> </a> </div>  其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、…、[V” H8J为H的一个量化划分，[S。，S1)、…、[Sm SqJ为S的一个量化划分，[V。，V1)、…、[V" Vqv]为V的一个量化划分；  图片像素颜色特征值计算元件，对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值X ：    x=hQsQv+sQs+v  其中，Qs为S的量化级数，Qv为V的量化级数，并且有     X e [x0, X1]，X0=O, X1= (Qh-1) QsQv+ (Qs-1) Qs+ (Qv-1)。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES59662315" lang="ZH" load-source="patent-office" class="description">
    <p>社交网络的图片推荐方法和装置</p>
    <p>技术领域</p>
    <p>[0001]	本发明属于社交网络领域，具体地说，涉及一种社交网络的图片推荐方法和装置。背景技术</p>
    <p>[0002]	目前，在一些社交网络(例如Pinterest、02、花瓣网、堆糖网等）中，用户在网站上分享了自己的图片后，网站即可根据该图片为该用户推荐与该图片相关的或该用户可能感兴趣的其它图片以及这些图片的拥有人，从而把对图片具有相似品味和偏好的用户链接起来，形成网络圈子。</p>
    <p>[0003]	图1示出了社交网络现有的图片推荐方法。如图1所示，社交网络包含有图片数据库10，图片数据库10中保存有例如图片11及图片标注11a、图片12及图片标注12a、图片13及图片标注13a等图片及其标注。所述图片标注是其所对应的图片的说明，它以文本为基础，包括该图片的分类(例如风景、人物、雪山、家居等)、标签(例如蓝天白云、小清新、柔和、平静等)或简短注释等。用户20在网站提交上传图片31以及上传图片标注31a，其中，上传图片标注31a也以文本为基础，可以包括上传图片的分类和/或标签。网站的文本匹配单元40根据上传图片标注31a在图片数据库40中寻找上传图片31的匹配图片，其中，图片标注与上传图片标注31a匹配的图片即为上传图片31的匹配图片，然后对这些匹配图片进行排队，并在所述匹配图片排队中选择若干图片作为相关图片(例如相关图片51、相关图片52等）推荐给用户20。</p>
    <p>[0004]	上述社交网络现有的图片推荐方法的不足之处在于：①用户分享图片时需要人工为该图片做图片标注，这就增加了用户操作的复杂性，并且影响了用户使用社交网络的体验；②由于不同用户对同一图片的观察和理解不同，因此，各用户提供的图片标注难免带有主观色彩，这将造成图片标注的非客观性并导致通过图片标注的匹配而获得的图片不符合用户的预期，从而影响网站推荐的精准性。</p>
    <p>发明内容</p>
    <p>[0005]	本发明是为了解决现有技术中存在的上述技术问题而做出，其目的在于提供一种社交网络的图片推荐方法和系统，以降低用户操作的复杂性，改善用户使用社交网络的体验、以及提高社交网络推荐图片的准确性。</p>
    <p>[0006]	根据本发明的一个方面，提供一种社交网络的图片推荐方法，该方法包括：</p>
    <p>[0007]	a)获取上传图片，并判断该上传图片是否为人像图片，如果判断为是人像图片，则从所述社交网络的图片数据库中选取人像图片作为对比图片，如果判断为不是人像图片，则从所述社交网络的图片数据库中选取非人像图片作为对比图片；</p>
    <p>[0008]	b)获得所述上传图片和所述对比图片的颜色特征距离d ;以及</p>
    <p>[0009]	c)将与所述上传图片的颜色特征距离d较小的一个或多个对比图片推荐给所述上传图片的用户。</p>
    <p>[0010]	优选地，在步骤b)中，获得所述上传图片和所述对比图片的颜色特征距离d可以包括：</p>
    <p>[0011]	bl)从所述上传图片中提取各个图片像素的颜色特征值X，其中，X e [x0, X1], X0^X1为预定值，从所述上传图片中提取图片像素的颜色特征值的方法与从所述对比图片中提取或预先提取图片像素的颜色特征值的方法相同；</p>
    <p>[0012]	b2)与所述对比图片相对应，对所述上传图片做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Aij(X)、以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bu(X)，其中，N、1、Mi, j均为自然数，且I彡i彡N，Mi与i对应，I彡j彡Mi ;</p>
    <p>[0013]	b3)对于对所述上传图片做出的第i种预定划分，根据所述Aij(X)和Bij(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I &lt; i &lt; N，I彡j彡Mi ;以及</p>
    <p>[0014]	b4)根据所述Cli获得所述上传图片和所述对比图片的颜色特征距离d，其中，K i &lt; N。</p>
    <p>[0015]	对于上述社交网络的图片推荐方法，优选地，可以采用基于Haar特征和adaboost算法的人脸检测方法来判断所述上传图片是否为人像图片。进一步优选地，可以采用所述基于Haar特征和adaboost算法的人脸检测方法将所述图片数据库的图片分为人像图片和非人像图片。</p>
    <p>[0016]	在上述步骤bl)中，优选地，从所述上传图片或所述对比图片中提取图片像素的颜色特征值的步骤可以包括：</p>
    <p>[0017]	①获取图片像素在色相-饱和度-亮度（HSV)色彩模型下的色相（H)、饱和度（S)和亮度（V)的值；</p>
    <p>[0018]	②对所获取的所述图片像素的HSV的值，按照下述公式进行量化，以获得与该图</p>
    <p>片像素的HSV值对应的hsv值：		 [0019]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00071.png"> <img id="idf0001" file="CN103049754AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00071.png" class="patent-full-image" alt="Figure CN103049754AD00071"> </a> </div>
    <p>[0020]	其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、…、[V” H8J为H的一个量化划分，[So, S1)、…、[Sm SqJ为S的一个量化划分，[V。，V1)、…、[Vm Vqv]为V的一个量化划分；</p>
    <p>[0021]	③对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值X :</p>
    <p>[0022]	x=hQsQv+sQs+v</p>
    <p>[0023]	其中，Qs为S的量化级数，Qv为V的量化级数，并且有</p>
    <p>[0024]	X e [X0, X1]，X0=O, X1= (Qh-1) QsQv+ (Qs-1) Qs+ (Qv-1)。</p>
    <p>[0025]	进一步优选地，在归一化的HSV色彩模型下，</p>
    <p>[0026]	Qh=8, Qs=4, Qv=3 ；</p>
    <p>[0027]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00081.png"> <img id="idf0002" file="CN103049754AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00081.png" class="patent-full-image" alt="Figure CN103049754AD00081"> </a> </div>
    <p>[0028]	x=12h+4s+v,并且有 χ e [x0, X1]，					X0=ο, x1=98.					</p>
    <p>[0029]	在上述步骤b3)中，优选地，可以由下述公式获得所述上传图片和所述对比图片在第i种预定划分下的颜色特征距离Cli ：</p>
    <p>[0030]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00082.png"> <img id="idf0003" file="CN103049754AD00082.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00082.png" class="patent-full-image" alt="Figure CN103049754AD00082"> </a> </div>
    <p>[0031]	其中，du为从所述对比图片的第i种预定划分中选取的第j个预定区域与从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域之间的颜色特征距离，</p>
    <p>Wij为(Iij的权重，Wij &gt; O,且</p>
    <p>[0032]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00083.png"> <img id="idf0004" file="CN103049754AD00083.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00083.png" class="patent-full-image" alt="Figure CN103049754AD00083"> </a> </div>
    <p>[0033]	在上述步骤b4)中，优选地，可以由下述公式获得所述上传图片和所述对比图片的颜色特征距离d ：</p>
    <p>[0034]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00084.png"> <img id="idf0005" file="CN103049754AD00084.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00084.png" class="patent-full-image" alt="Figure CN103049754AD00084"> </a> </div>
    <p>[0035]	其中，Wi为Cli的权重，Wi &gt; 0，且</p>
    <p>[0036]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00085.png"> <img id="idf0006" file="CN103049754AD00085.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00085.png" class="patent-full-image" alt="Figure CN103049754AD00085"> </a> </div>
    <p>[0037]	对于上述的社交网络的图片推荐方法，在步骤b2)中，优选地，</p>
    <p>[0038]	N=3 ；</p>
    <p>[0039]	第一种预定划分可以将所述上传图片划分为中心区域和周围区域，并可以只选取该中心区域作为所述预定区域；</p>
    <p>[0040]	第二种预定划分可以将所述上传图片划分为多个较大的区域，并可以选取该多个较大的区域作为所述预定区域；以及</p>
    <p>[0041]	第三预定种划分可以将所述上传图片划分为多个较小的区域，并可以选取该多个较小的区域作为所述预定区域。</p>
    <p>[0042]	进一步优选地，在所述第一种预定划分中，中心区域的大小可以为所述上传图片大小的1/9 ;在所述第二种预定划分中，所述多个较大的区域可以包括4个大小相同的区域；在所述第三种预定划分中，所述多个较小的区域可以包括16个大小相同的区域。</p>
    <p>[0043]	在由上述公式获得所述上传图片和所述对比图片在第i种预定划分下的颜色特征距离Cli时，优选地，N=3 ;第一种预定划分可以将所述上传图片划分为中心区域和周围区域，该中心区域的大小可以为所述上传图片大小的1/9，并可以只选取该中心区域作为所述预定区域；第二种预定划分可以将所述上传图片划分为4个大小相同的区域，并可以选取该4个区域作为所述预定区域；第三预定种划分可以将所述上传图片划分为16个大小相同的区域，并可以选取该16个区域作为所述预定区域；以及权重值可以分别为：wn=l ；W2j=I/4,1 &lt; j &lt; 4 ；w3J=1/16, I &lt; j &lt; 16。</p>
    <p>[0044]	在由上述公式获得所述上传图片和所述对比图片的颜色特征距离d时，优选地，N=3;第一种预定划分可以将所述上传图片划分为中心区域和周围区域，该中心区域的大小可以为所述上传图片大小的1/9，并可以只选取该中心区域作为所述预定区域；第二种预定划分可以将所述上传图片划分为4个大小相同的区域，并可以选取该4个区域作为所述预定区域；第三预定种划分可以将所述上传图片划分为16个大小相同的区域，并可以选取该16个区域作为所述预定区域；以及权重值可以分别为：Wl=l/2 ；w2=1/4 ；、w3=1/4。</p>
    <p>[0045]	根据本发明的另一方面，提供一种社交网络的图片推荐装置，其包括：</p>
    <p>[0046]	人像判断与选取单元，用于获取上传图片，并判断该上传图片是否为人像图片，如果判断为是人像图片，则从所述社交网络的图片数据库中选取人像图片作为对比图片，如果判断为不是人像图片，则从所述社交网络的图片数据库中选取非人像图片作为对比图片;</p>
    <p>[0047]	颜色特征距离获得单元，用于获得所述上传图片和所述对比图片的颜色特征距离d ;以及</p>
    <p> [0048]	图片推荐单元，将与所述上传图片的颜色特征距离d较小的一个或多个对比图片推荐给所述上传图片的用户。</p>
    <p>[0049]	优选地，所述颜色特征距离获得单元可以包括：</p>
    <p>[0050]	图片像素颜色特征值提取部件，用于从所述对比图片或所述上传图片中提取各个图片像素的颜色特征值X，其中，X e [X(l，X1], Xtl与X1为预定值；</p>
    <p>[0051]	图片划分部件，用于对所述对比图片和所述上传图片做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Aij(X)、以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bu(X)，其中，N、1、Mi, j均为自然数，且I彡i彡N，Mi与i对应，I彡j彡Mi ;</p>
    <p>[0052]	第一颜色特征距离获得部件，对于对所述上传图片做出的第i种预定划分，根据所述Au(X)和Bu(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I彡i彡N，I彡j彡Mi ;以及</p>
    <p>[0053]	第二颜色特征距离获得部件，根据所述Cli获得所述上传图片和所述对比图片的颜色特征距离d，其中，I彡i彡N。</p>
    <p>[0054]	进一步优选地，所述图片像素颜色特征值提取部件可以包括：</p>
    <p>[0055]	HSV值获取元件，获取图片像素在色相-饱和度-亮度（HSV)色彩模型下的色相(H)、饱和度（S)和亮度（V)的值；</p>
    <p>[0056]	HSV值量化元件，对所获取的所述图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值：</p>
    <p>[0057]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00101.png"> <img id="idf0007" file="CN103049754AD00101.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00101.png" class="patent-full-image" alt="Figure CN103049754AD00101"> </a> </div>
    <p>[0058]	其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、&#183;&#183;</p>
    <p>个量化划分，[So, S1)、…、[Sn，SqJ为S的一个量化划分，[V0, V1)、</p>
    <p>一个量化划分；</p>
    <p>[0059]	图片像素颜色特征值计算元件，对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值X :</p>
    <p>[0060]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00102.png"> <img id="idf0008" file="CN103049754AD00102.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00102.png" class="patent-full-image" alt="Figure CN103049754AD00102"> </a> </div>
    <p>[0061]	其中，Qs为S的量化级数，Qv为V的量化级数，并且有</p>
    <p>[0062]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00103.png"> <img id="idf0009" file="CN103049754AD00103.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00103.png" class="patent-full-image" alt="Figure CN103049754AD00103"> </a> </div>
    <p>[0063]	由上面的描述可知，本发明所述的社交网络的图片推荐方法基于上传图片的人像特征和颜色特征向用户推荐与该上传图片相似度或关联度较大的图片。在这个过程中，用户只需要上传图片即可，不需要人工为该图片做标注，从而减小了用户操作的复杂性，并且改善了用户使用社交网络的体验。另外，由于社交网络基于图片内容特征对上传图片和图片数据库中的图片进行匹配，因此，可以避免引入用户的主观色彩，从而可以提高网站推荐的精准性，所推荐的图片容易符合用户的预期。经用户测试，通过本发明所述的方法推荐的关联图片中，符合用户预期的关联图片占全部推荐的关联图片的比例平均为67%，最高可达81%，该推荐准确率可以满足互联网产品的娱乐应用要求。</p>
    <p>附图说明</p>
    <p>[0064]	通过参考以下结合附图的说明及权利要求书的内容，本发明的其它目的及结果将更加明白及易于理解。在附图中：</p>
    <p>[0065]	图1是示意图，示出了社交网络现有的图片推荐方法；</p>
    <p>[0066]	图2是流程图，示出了本发明的一个实施例所述的社交网络的图片推荐方法；</p>
    <p>[0067]	图3是流程图，示出了本发明的一个实施例所述的颜色特征距离的获得方法；</p>
    <p>[0068]	图4是示意图，示出了 RGB色彩模型；</p>
    <p>[0069]	图5是示意图，示出了 HSV色彩模型；以及[0070]	图6a_6c是示意图，分别示出了本发明的一个实施例所述的对比图片和上传图片的三种预定划分。</p>
    <p>具体实施方式</p>
    <p>[0071]	在下面的描述中，出于说明的目的，为了提供对一个或多个实施例的全面理解，阐述了许多具体细节。然而，很明显，也可以在没有这些具体细节的情况下实现这些实施例。在其它例子中，为了便于描述一个或多个实施例，公知的结构和设备以方框图的形式示出。</p>
    <p>[0072]	在本发明所涉及的社交网络中，图片数据库保存有图片以及与其对应的图片特征。图片特征可以分为三个层次，即以像素为基础的像素层次、以颜色、纹理和形状为基础的视觉层次、以及以图片所表达的情景和含义为基础的语义层次。本发明所涉及的社交网络的图片数据库中保存的图片特征为图片本身所包含的视觉层次的特征。具体说，在本发明所涉及的图片数据库中，图片特征由人像特征和颜色特征来描述。</p>
    <p>[0073]	图片的人像特征反映的是图片以人像为主还是以风景为主。在社交网站中，被推荐的相关图片主要包含人像还是风景，对用户的感受而言差别还是很大的，因此，在社交网站中，考察图片时应该首先关注该图片的人像特征。图片的人像特征可以通过人工来确定，也可以通过计算机人脸检测方法来确定。所谓人脸检测是指对于任意一幅给定的图像，采用一定的策略对其进行搜索以确定其中是否含有人脸。典型的人脸检测方法有模板法、基于器官特征的方法、神经网络方法、基于AdaBoost算法的方法等。考虑到各种人脸检测方法的准确性、计算复杂度和速度，在本发明所涉及的社交网络中，可以采用基于Haar特征和Adaboost算法的人脸检测方法。基于Haar特征和Adaboost算法的人脸检测方法是本领域中的技术人员所公知的。这里只对其作简单的介绍。</p>
    <p>[0074]	Haar特征也即具有特定灰度分布的矩形的特征，可以通过5元组（x，y，W，h，angle)来表示，其中，(x, y)为矩形的左上角位置，（w，h)为矩形的宽和高，angle为矩形的旋转角度。</p>
    <p>[0075]	在人脸检测中，使用Haar特征作为特征模板对人脸图像灰度分布的特点进行描述，使用“积分图”实现特征数值的快速计算。其次，使用Adaboost算法挑选出一些最能代表人脸的矩形特征（即弱分类器)，再按照加权投票的方式将弱分类器构造为强分类器。最后，将得到的若干强分类器串联组成一个级联结构的层叠分类器作为人脸检测器。</p>
    <p>[0076]	使用所述人脸检测器可以判断上传图片是人脸还是非人脸。通过该过程对上传图片进行分类，然后在图片数据库中的同类图片中选择关联推荐图片，可以使推荐结果更加符合用户的预期。</p>
    <p>[0077]	图片的颜色特征反映了图片的像素颜色的统计分布信息。图片的颜色特征可以采用各种统计和估计方法来描述，在图片颜色特征的每种描述中，可以用多种方式定义颜色特征的距离，以衡量两种图片颜色特征之间的相对远近，即，相对关联性或相似度。在本发明中，采用一种特殊的方法来描述图片的颜色特征并度量颜色特征之间的相对远近。具体说，在本发明所涉及的社交网站的图片数据库中，图片的颜色特征可以包含图片像素的颜色特征值，还可以包括图片的各种划分区域内的图片像素的颜色特征值的分布（即，该区域内，具有每种颜色特征值的图片像素的数目占图片像素总数的比例)。对于上传图片，也可以先获取其颜色特征，然后对上传图片和图片数据库中的图片的颜色特征通过特殊定义的颜色特征距离进行比较，如果两者的颜色特征比较接近，则表明两者的关联性或相似度较大。网站可以将与上传图片的关联性或相似度较大的图片推荐给用户。</p>
    <p>[0078]	下面将参照附图来对根据本发明的各个实施例进行详细描述。</p>
    <p>[0079]	图2是流程图，示出了本发明的一个实施例所述的社交网络的图片推荐方法。</p>
    <p>[0080]	如图2所示，首先，在步骤SllO中，获取上传图片。获取上传图片的过程还可以包括上传图片的标准化步骤。也就是说，根据社交网络图片数据库中所保存的图片的格式和尺寸对上传图片进行处理，使上传图片的格式和尺寸与图片数据库中的图片的格式和尺寸对应，以便于比较。</p>
    <p>[0081]	接着在步骤S120中，采用人工方法或采用基于Haar特征和adaboost算法的人脸识别方法来判断所述上传图片是否为人像图片。如果在步骤S120中判断为是人像图片，则在步骤S131中从社交网络的图片数据库选取人像图片作为对比图片；如果在步骤S120中判断为是非人像图片(例如是风景图片)，则在步骤S132中从社交网络的图片数据库选取非人像图片作为对比图片。</p>
    <p>[0082]	应该注意，可以采用所述基于Haar特征和adaboost算法的人脸检测方法将所述图片数据库的图片预先分为人像图片和非人像图片。</p>
    <p>[0083]	然后，在步骤S140中，获得所述上传图片和所述对比图片的颜色特征距离d。后面将结合附图详细描述本发明的一个实施例所述的颜色特征距离d的获得方法。</p>
    <p>[0084]	在步骤S140中获得了上传图片与从图片数据库中选取的对比图片的颜色特征距离d之后，前进到步骤S150，向上传图片的用户（即上传者)推荐相关图片。具体说，可以将与上传图片的颜色特征距离d较小的一个或多个对比图片推荐给上传图片的用户。这里，较小的颜色特征距离是一个相对概念，可以指上传图片与各对比图片的颜色特征距离中最小的若干颜色特征距离，也可指上传图片与各对比图片的颜色特征距离中小于预定阈值的那些颜色特征距离。上传图 片和对比图片的颜色特征距离d越小，则两图片在视觉上相差越小，它们的相似度或关联性越高。</p>
    <p>[0085]	图3是流程图，示出了本发明的一个实施例所述的颜色特征距离的获得方法。</p>
    <p>[0086]	如图3所示，在本发明的一个实施例中，为了获得上传图片和对比图片的颜色特征距离d，首先在步骤S142中，从上传图片中提取各个图片像素的颜色特征值X，其中，χ的取值范围为X ^ [X(|, xj，Xci与X1为预定值。</p>
    <p>[0087]	下面结合本发明的一个实施例详细描述图片像素的颜色特征、颜色特征值及其提取方法。</p>
    <p>[0088]	一幅图片包含排成二维阵列的多个像素，每个像素都包含红（R)绿（G)蓝（B)三原色。图4示出了 RGB色彩模型。如图4所示，该模型基于笛卡尔坐标系，红绿蓝三原色分别在三根坐标轴上，为方便起见，三原色的颜色值都作了归一化，即R、G、B分量都在[0，I]范围内取值，从而得到一个呈立方体的颜色空间。通过三原色分量的不同比例，可以合成任何颜色，其中黑色在原点，白色位于离原点最远角上。</p>
    <p>[0089]	虽然三原色表示颜色很直接，适合面向硬件，但却不符合人眼对颜色的感知规律。在众多的色彩模型(例如，RGB、CMY、YUV、YIQ, HIS、HSV等色彩空间）中符合人眼对颜色的感知规律的一种色彩模型为HSV色彩模型。在HSV色彩模型中，采用色相（Hue，H)、饱和度(Saturation, S)和亮度（Value，V)来表征颜色，其中，色相（H)是色彩的基本属性，是指颜色名称,如红色、黄色等；饱和度（S)是指色彩的纯度，可用来区别颜色的深浅。完全饱和的颜色是指没有渗入白光的颜色，例如单一波长的纯色就是完全饱和的颜色，饱和度越高，色彩越纯，饱和度低则颜色变浅。亮度（V)用来反映光强的大小或明暗的程度。亮度的一个极端是黑色(光强为零)，另一个极端是白色，在这两个极端之间是灰色。</p>
    <p>[0090]	图5示出了 HSV色彩模型。如图5所示，该模型基于柱坐标系，其颜色空间是一个倒立的圆锥体，其中，色相（H)用绕圆锥中心轴的角度来表示(例如，0°表示红色，120°表示绿色，240°表示蓝色）；饱和度（S)用到圆锥中心轴的距离来表示；亮度（V)用到圆锥顶点的距离在圆锥中心轴的投影来表示。进行归一化后，饱和度（S)和亮度（V)都在[0，1]范围内取值。具体说，在该圆锥的顶点处，V=0，H和S无定义，代表黑色；在该圆锥的顶面中心处，S=O, V=l，H无定义，代表白色；在这两点之间的点则表示不同的灰度。在圆锥顶面的圆周上，V=I, S=l，代表各种纯色。HSV色彩模型适合于计算机图像应用。</p>
    <p>[0091]	将图片像素的RGB值转换成HSV值的公式是本领域中的技术人员所公知的，这里就不再写出。转换的结果是，将上述立方体形的RGB空间中的一点映射到上述倒圆锥体形的HSV空间中的一点。</p>
    <p>[0092]	获得图片像素的HSV值后，还需要对其进行量化，以便进行数字化处理。人眼对色彩的色相（H)、饱和度（S)和亮度（V)的感知与其值不呈线性关系，因此，在对每个像素的HSV值进行量化时，不能采用平均划分的方法来量化。否则，对于用户视觉偏差不大的图片，计算出的颜色特征差异可能非常大。由于社交网站中的图片关联推荐只关注用户对图片色彩的直观感觉，因此本发明采用非平均的方法来对色相（H)、饱和度（S)、亮度（V)进行分段量化。</p>
    <p>[0093]	具体说，在本发明的一个实施例中，对所获取的上传图片的每个图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值：</p>
    <p>[0094]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00131.png"> <img id="idf0010" file="CN103049754AD00131.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00131.png" class="patent-full-image" alt="Figure CN103049754AD00131"> </a> </div>
    <p>[0095]</p>
    <p>特征值</p>
    <p>[0096]</p>
    <p>[0097]</p>
    <p>对于所获得的每个图片像素的hsv值，按照下述公式获得该图片像素的像素颜色</p>
    <p>x=hQsQv+sQs+v</p>
    <p>其中，Qs为饱和度的量化级数，Qv为亮度的量化级数，在本实施例中，QS=4，QV=3，从而像素颜色特征值χ的表达式变为：X=12h+4s+v，其中，从上述公式很容易得到χ的取值范围为：x e [χ0, X1]，X0=O, Χ!=98ο</p>
    <p>[0098]	本发明不限于上述量化处理方法。可以进一步对其进行推广。具体说，在本发明的其它实施例中，可以对所获取的上传图片的每个图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00141.png"> <img id="idf0011" file="CN103049754AD00141.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00141.png" class="patent-full-image" alt="Figure CN103049754AD00141"> </a> </div>
    <p>[0100]	其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、…、[V” H8J为H的一个量化划分，[So, S1)、…、[Sm SqJ为S的一个量化划分，[V。，V1)、…、[Vm Vqv]为V的一个量化划分；</p>
    <p>[0101]	然后，对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值χ ：</p>
    <p>[0102]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00142.png"> <img id="idf0012" file="CN103049754AD00142.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00142.png" class="patent-full-image" alt="Figure CN103049754AD00142"> </a> </div>
    <p>[0103]	其中，Qs SS的量化级数，QvS V的量化级数，并且χ的取值范围为:x e [x0, X1],xQ=0，Xi= (Qh-1) QsQv+ (Qs_l) Qs+ (Qv-1)。</p>
    <p>[0104]	应该注意，上述图片像素的颜色特征值χ还可以有其它的定义方法。每个图片像素的颜色特征值应该能够度量该图片像素的颜色特征，以便于与其它图片像素的颜色特征相比较。此外，所述图片数据库中的对比图片的图片像素的颜色特征值采用与上述方法同样的方法提取或预先提取出来并保存起来。</p>
    <p>[0105]	再参看图3，在步骤S142中，从上传图片中提取出各个图片像素的颜色特征值X，之后就可以计算上传图片与从图片数据库中选取的对比图片的颜色特征距离，也就是颜色特征的相似度或关联度。</p>
    <p>[0106]	具体说，根据本发明的一个实施例，首先，在步骤S144中，与所述对比图片相对应，对所述上传图片做出多种预定划分，并从每种预定划分中选取多个预定区域，并获取每个所选的预定区域内的图片像素颜色特征值的分布，即，该预定区域内，具有每种颜色特征值的图片像素的数目占图片像素总数的比例。</p>
    <p>[0107]	图6a_6c是示意图，分别示出了本发明的一个实施例所述的对比图片和上传图片的三种预定划分。</p>
    <p>[0108]	与对比图片11相对应，对上传图片31做出3种划分。如图6a所示，第一种预定划分将上传图片31划分为中心区域和周围区域，并只选取该中心区域作为所述预定区域。更具体地，中心区域的大小可以为上传图片31大小的1/9。计算出的该预定区域的图片像素颜色特征值χ的分布为A11(X),其中，χ的取值范围为χ e [x0, X1], Xtl与X1为预定值（图5a中将A11 (χ)简记为A11,图6b、6c中类似)。A11 (χ)所对应的对比图片11的第一种预定划分中的中心区域的图片像素颜色特征值的分布为B11(X)。</p>
    <p>[0109]	如图6b所示，第二种预定划分将上传图片31划分为多个较大的区域，并选取该多个较大的区域作为所述预定区域。更具体地，该多个较大区域为4个大小相同的区域。计算出的该4个区域的图片像素颜色特征值的分布分别为A21(X)、A22(X)、A23(X)、A24(x)，其中，χ的取值范围为χ e [x0, xj，Xtl与X1为预定值。它们所对应的对比图片11的第二种划分中的4个大小相同的区域的图片像素颜色特征值的分布分别为B21(X) ,B22(X) ,B23(X) ,B24(X)。</p>
    <p>[0110]	如图6c所示，第三预定种划分将上传图片31划分为多个较小的区域，并选取该多个较小的区域作为所述预定区域。更具体地，该多个较小区域为16个大小相同的区域。计算出的该16个区域的图片像素颜色特征值的分布分别为A31(X)、A32(X)、A33(X)、…、A316(X),其中，χ的取值范围为χ e [X(l，Xl]，Xtl与X1为预定值。它们所对应的对比图片11的第三种划分中的16个大小相同的区域的图片像素颜色特征值的分布分别为B31 (x) ,B32(X)、Β33 (χ)、...、B316 (χ)。</p>
    <p>[0111]	应该注意，所述图片数据库中的对比图片的各种预定划分区域中的图片像素颜色特征值的分布可以预先计算并保存起来。</p>
    <p>[0112]	在对所述上传图片做出了各种划分之后，在步骤S146中，计算所述上传图片和所述对比图片在各种预定划分下的颜色特征距离。</p>
    <p>[0113]	具体说，对于上述三种预定&#183;划分，分别使用例如下述公式来计算在第一、第二和第三种预定划分下上传图片31和对比图片11的颜色特征距离屯、d2和d3 ：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00151.png"> <img id="idf0013" file="CN103049754AD00151.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00151.png" class="patent-full-image" alt="Figure CN103049754AD00151"> </a> </div>
    <p>[0117]	其中，通过对dn进行加权平均获得Cl1,由于只选取了一个区域，因此dn的权重W11=I ;通过对d2j进行加权平均获得d2,其中，d2j的权重w2j=l/4,1彡j彡4 ;通过对d3j进行加权平均获得d3，其中，d3j的权重w3j = 1/16，I ^ j ^ 16。</p>
    <p>[0118]	在上述实施例中，通过距离空间中的欧几里得距离公式来定义对应区域的颜色特征值分布之间的距离（dn、d2j、d3j)，通过各对应区域的颜色特征值分布之间的距离的简单平均来获得整个图片在某种划分下的颜色特征距离（C^dyd3)t5显然，可以用其它类型的距离公式以及平均(或综合）方法来计算上传图片和对比图片在某种划分下的颜色特征距离。</p>
    <p>[0119]	最后，在步骤S148中，由例如下述公式计算上传图片31和对比图片11的颜色特征距离d ：</p>
    <p>[0120]	d=w1d1+w2d2+w3d3</p>
    <p>[0121]	其中，W^w2和W3分别屯、d2和d3的权重，W^w2和W3大于O,且</p>
    <p>[0122]	W^W2+W3=I</p>
    <p>[0123]	根据实验结果，权重经验值取为Wl=l/2，w2=l/4，w3=l/4获得的效果较好。显然，可以用其它类型的平均(或综合）方法来根据Cli计算上传图片和对比图片的颜色特征距离。</p>
    <p>[0124]	本发明不限于上述颜色特征距离的计算方法。可以进一步对其进行推广。具体说，在本发明的其它实施例中，在获得了上传图片31和对比图片11的图片像素的颜色特征</p>
    <p>[0116] d值X (其中，Xe [X(l，Xl]，Xtl与X1为预定值）之后，计算上传图片31与对比图片11的颜色特征距离的方法可以包括：首先，与对比图片11相对应，对上传图片31做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Aij(X)'以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bu (χ)，其中，N、1、Mi, j均为自然数，且I≤i≤N，Mi与i对应，I ^ j ^ Mp</p>
    <p>[0125]	接着，对于对上传图片31做出的第i种预定划分，根据所述Aij(X)和Bij(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I &lt; i &lt; N，Mi,特别地，可以由下述公式计算上传图片31和对比图片11在该第i种预定划分下的颜色特征距离Cli ：[0126]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00161.png"> <img id="idf0014" file="CN103049754AD00161.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00161.png" class="patent-full-image" alt="Figure CN103049754AD00161"> </a> </div>
    <p>[0127]	其中，du为从所述对比图片的第i种预定划分中选取的第j个预定区域与从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域之间的颜色特征距离，Wij为(Iij的权重，Wij &gt; O,且</p>
    <p>[0128]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00162.png"> <img id="idf0015" file="CN103049754AD00162.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00162.png" class="patent-full-image" alt="Figure CN103049754AD00162"> </a> </div>
    <p>[0129]	最后，根据所述Cli获得所述上传图片和所述对比图片的颜色特征距离d，其中，I ^ i 特别地，可以由下述公式计算上传图片31和对比图片11的颜色特征距离d ：</p>
    <p>[0130]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00163.png"> <img id="idf0016" file="CN103049754AD00163.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00163.png" class="patent-full-image" alt="Figure CN103049754AD00163"> </a> </div>
    <p>[0131]	其中，Wi为Cli的权重，Wi &gt; 0，且</p>
    <p>[0132]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00164.png"> <img id="idf0017" file="CN103049754AD00164.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00164.png" class="patent-full-image" alt="Figure CN103049754AD00164"> </a> </div>
    <p>[0133]	如上参照图2-图6c描述了本发明所述的社交网络的图片推荐方法。本发明所述的社交网络的图片推荐方法，可以采用软件实现，也可以采用硬件实现，或采用软件和硬件组合的方式实现。</p>
    <p>[0134]	根据本发明的另一实施例，提供一种社交网络的图片推荐装置，其包括：</p>
    <p>[0135]	人像判断与选取单元，用于获取上传图片，并判断该上传图片是否为人像图片，如果判断为是人像图片，则从所述社交网络的图片数据库中选取人像图片作为对比图片，如果判断为不是人像图片，则从所述社交网络的图片数据库中选取非人像图片作为对比图片;</p>
    <p>[0136]	颜色特征距离获得单元，用于获得所述上传图片和所述对比图片的颜色特征距离d ;以及</p>
    <p>[0137]	图片推荐单元，将与所述上传图片的颜色特征距离d较小的一个或多个对比图片推荐给所述上传图片的用户。</p>
    <p>[0138]	其中，优选地，所述颜色特征距离获得单元可以包括：</p>
    <p>[0139]	图片像素颜色特征值提取部件，用于从所述对比图片或所述上传图片中提取各个图片像素的颜色特征值X，其中，X e [X(l，X1], Xtl与X1为预定值；[0140]	图片划分部件，用于对所述对比图片和所述上传图片做出N种预定划分，从第i种预定划分中选取Mi个预定区域，并获取从所述对比图片的第i种预定划分中选取的第j个预定区域的图片像素颜色特征值分布Aij(X)、以及从所述上传图片的第i种对应的预定划分中选取的第j个对应的预定区域的图片像素颜色特征值分布Bu(X)，其中，N、1、Mi, j均为自然数，且I≤i≤N，Mi与i对应，I≤j≤Mi ;</p>
    <p>[0141]	第一颜色特征距离获得部件，对于对所述上传图片做出的第i种预定划分，根据所述Au(X)和Bu(X)获得所述上传图片和所述对比图片在该第i种预定划分下的颜色特征距离屯，其中，I≤i≤N，I≤j≤Mi ;以及</p>
    <p>[0142]	第二颜色特征距离获得部件，根据所述Cli获得所述上传图片和所述对比图片的颜色特征距离d，其中，I≤i≤N。</p>
    <p>[0143]	进一步优选地，所述图片像素颜色特征值提取部件可以包括：</p>
    <p>[0144]	HSV值获取元件，获取图片像素在色相-饱和度-亮度（HSV)色彩模型下的色相(H)、饱和度（S)和亮度（V)的值；</p>
    <p>[0145]	HSV值量化元件，对所获取的所述图片像素的HSV的值，按照下述公式进行量化，以获得与该图片像素的HSV值对应的hsv值：</p>
    <p>[0146]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00171.png"> <img id="idf0018" file="CN103049754AD00171.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00171.png" class="patent-full-image" alt="Figure CN103049754AD00171"> </a> </div>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00172.png"> <img id="idf0019" file="CN103049754AD00172.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103049754A/CN103049754AD00172.png" class="patent-full-image" alt="Figure CN103049754AD00172"> </a> </div>
    <p>[0147]	其中，Qh、Qs和Qv分别为H、S和V的量化级数，[H0, H1)、…、[V” H8J为H的一个量化划分，[So, S1)、…、[Sm SqJ为S的一个量化划分，[V。，V1)、…、[Vm Vqv]为V的一个量化划分；</p>
    <p>[0148]	图片像素颜色特征值计算元件，对于所获得的所述图片像素的hsv值，按照下述公式计算该图片像素的颜色特征值X :</p>
    <p>[0149]	x=hQsQv+sQs+v</p>
    <p>[0150]	其中，Qs为S的量化级数，Qv为V的量化级数,并且有</p>
    <p>[0151]	χ e [x0, xj , X0=O, X1=(Qh-1)QsQ^(Qs-1)QJ(Qv-1)。</p>
    <p>[0152]	由上面的描述可知，本发明所述的社交网络的图片推荐方法基于上传图片的人像特征和颜色特征向用户推荐与该上传图片相似度或关联度较大的图片。在这个过程中，用户只需要上传图片即可，不需要人工为该图片做标注，或者只需要做最简短明确的标注(人像、非人像等)，从而减小了用户操作的复杂性，并且改善了用户使用社交网络的体验。另外，由于社交网络基于图片内容特征对上传图片和图片数据库中的图片进行匹配，因此，可以避免引入用户的主观色彩，从而可以提高网站推荐的精准性，所推荐的图片容易符合用户的预期。经用户测试，通过本发明所述的方法推荐的关联图片中，符合用户预期的关联图片占全部推荐的关联图片的比例平均为67%，最高可达81%，该推荐准确率可以满足互联网产品的娱乐应用要求。</p>
    <p>[0153]	尽管已经结合详细示出并描述的优选实施例公开了本发明，但是本领域技术人员应当理解，对于上述本发明所提出的社交网络的图片推荐方法和装置，还可以在不脱离本发明内容的基础上做出各种改进。因此，本发明的保护范围应当由所附的权利要求书的内容确定。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102254043A?cl=zh">CN102254043A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年8月17日</td><td class="patent-data-table-td patent-date-value">2011年11月23日</td><td class="patent-data-table-td ">电子科技大学</td><td class="patent-data-table-td ">一种基于语义映射的服装图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102368746A?cl=zh">CN102368746A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年9月8日</td><td class="patent-data-table-td patent-date-value">2012年3月7日</td><td class="patent-data-table-td ">宇龙计算机通信科技(深圳)有限公司</td><td class="patent-data-table-td ">图片信息推送方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102387094A?cl=zh">CN102387094A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年10月24日</td><td class="patent-data-table-td patent-date-value">2012年3月21日</td><td class="patent-data-table-td ">Tcl集团股份有限公司</td><td class="patent-data-table-td ">一种网络社交的建立方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521253A?cl=zh">CN102521253A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月17日</td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td ">西安交通大学</td><td class="patent-data-table-td ">一种可视化的网络用户多媒体管理方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070168357">US20070168357</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2006年12月29日</td><td class="patent-data-table-td patent-date-value">2007年7月19日</td><td class="patent-data-table-td ">G &amp; G Commerce Ltd.</td><td class="patent-data-table-td ">Merchandise recommending system and method thereof</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=RlnwBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009460000">G06K9/46</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=RlnwBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年4月17日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年5月15日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/941319260bf1a3324d6d/CN103049754A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E7%89%87%E6%8E%A8%E8%8D%90%E6%96%B9%E6%B3%95%E5%92%8C%E8%A3%85.pdf?id=RlnwBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U3FUIVzN5oSI4HEaFA7hsEdh9sO7Q"},"sample_url":"https://www.google.com/patents/reader?id=RlnwBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>