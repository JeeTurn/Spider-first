<!DOCTYPE html><html><head><title>专利 CN102508848A - 一种人机智能交互方法及系统 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种人机智能交互方法及系统"><meta name="DC.contributor" content="李戈晶" scheme="inventor"><meta name="DC.contributor" content="靳鑫" scheme="inventor"><meta name="DC.contributor" content="李戈晶" scheme="assignee"><meta name="DC.contributor" content="靳鑫" scheme="assignee"><meta name="DC.date" content="2011-9-30" scheme="dateSubmitted"><meta name="DC.description" content="本发明涉及一种人机智能交互方法及系统，先制作在客户端与人交互的事件的展现素材并上传于服务内容数据库中；服务内容数据库存储上传的信息，该信息以三维坐标形式存储；用户注册来获取初级权限；注册成功后，实时记载用户访问的情况，根据用户访问情况来计算积分；由积分来判断应得的权限等级，当客户确认兑换，命令内容发布单元检索服务内容数据库中对应等级的事件；检索得到的事件调用并存储在内容发布单元中；客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；当用户打开触摸屏访问，交互单元依据客户端发送过来的信息和时间点从内容发布单元匹配出相应事件，并把事件数据处理后发送至显示单元来与人互动交流。"><meta name="DC.date" content="2012-6-20"><meta name="DC.relation" content="CN:101127621:A" scheme="references"><meta name="DC.relation" content="CN:101201738:A" scheme="references"><meta name="DC.relation" content="TW:200901020" scheme="references"><meta name="citation_patent_publication_number" content="CN:102508848:A"><meta name="citation_patent_application_number" content="CN:201110291628"><link rel="canonical" href="https://www.google.com/patents/CN102508848A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102508848A?cl=zh"/><meta name="title" content="专利 CN102508848A - 一种人机智能交互方法及系统"/><meta name="description" content="本发明涉及一种人机智能交互方法及系统，先制作在客户端与人交互的事件的展现素材并上传于服务内容数据库中；服务内容数据库存储上传的信息，该信息以三维坐标形式存储；用户注册来获取初级权限；注册成功后，实时记载用户访问的情况，根据用户访问情况来计算积分；由积分来判断应得的权限等级，当客户确认兑换，命令内容发布单元检索服务内容数据库中对应等级的事件；检索得到的事件调用并存储在内容发布单元中；客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；当用户打开触摸屏访问，交互单元依据客户端发送过来的信息和时间点从内容发布单元匹配出相应事件，并把事件数据处理后发送至显示单元来与人互动交流。"/><meta property="og:title" content="专利 CN102508848A - 一种人机智能交互方法及系统"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102508848A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102508848A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102508848A&amp;usg=AFQjCNHoLAxZK8KahdUNj2Q13WMKtogWbQ" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/2c1a5403d9689ab5263d/CN102508848A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/2c1a5403d9689ab5263d/CN102508848A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102508848A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102508848A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102508848A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102508848B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102508848A?cl=zh" style="display:none"><span itemprop="description">本发明涉及一种人机智能交互方法及系统，先制作在客户端与人交互的事件的展现素材并上传于服务内容数据库中；服务内容数据库存储上传的信息，该信息以三维坐标形式存储；用户注册来获取初级权限；注册成功后，实时记...</span><span itemprop="url">https://www.google.com/patents/CN102508848A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102508848A - 一种人机智能交互方法及系统</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102508848A - 一种人机智能交互方法及系统" title="专利 CN102508848A - 一种人机智能交互方法及系统"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102508848 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110291628</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年6月20日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年9月30日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2011年9月30日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102508848B?hl=zh-CN&amp;cl=zh">CN102508848B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110291628.8, </span><span class="patent-bibdata-value">CN 102508848 A, </span><span class="patent-bibdata-value">CN 102508848A, </span><span class="patent-bibdata-value">CN 201110291628, </span><span class="patent-bibdata-value">CN-A-102508848, </span><span class="patent-bibdata-value">CN102508848 A, </span><span class="patent-bibdata-value">CN102508848A, </span><span class="patent-bibdata-value">CN201110291628, </span><span class="patent-bibdata-value">CN201110291628.8</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9D%8E%E6%88%88%E6%99%B6%22">李戈晶</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%9D%B3%E9%91%AB%22">靳鑫</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E6%9D%8E%E6%88%88%E6%99%B6%22">李戈晶</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E9%9D%B3%E9%91%AB%22">靳鑫</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102508848A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102508848A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102508848A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110291628&amp;usg=AFQjCNFDgB_8Z9YD8Z7r2SS0qNJOSW_rKA"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102508848A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNF2mJDXQN0OQ-6FbsmNsZakPxphcQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT113434213" lang="ZH" load-source="patent-office">一种人机智能交互方法及系统</invention-title>
      </span><br><span class="patent-number">CN 102508848 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA97385917" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明涉及一种人机智能交互方法及系统，先制作在客户端与人交互的事件的展现素材并上传于服务内容数据库中；服务内容数据库存储上传的信息，该信息以三维坐标形式存储；用户注册来获取初级权限；注册成功后，实时记载用户访问的情况，根据用户访问情况来计算积分；由积分来判断应得的权限等级，当客户确认兑换，命令内容发布单元检索服务内容数据库中对应等级的事件；检索得到的事件调用并存储在内容发布单元中；客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；当用户打开触摸屏访问，交互单元依据客户端发送过来的信息和时间点从内容发布单元匹配出相应事件，并把事件数据处理后发送至显示单元来与人互动交流。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(10)</span></span></div><div class="patent-text"><div mxw-id="PCLM43023002" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种人机智能交互方法，其特征在于，该方法的具体工作步骤包括：步骤1)：制作在客户端与人交互的事件的图片序列、视频和声音的展现素材并上传于所述服务内容数据库中；步骤幻：服务端中的服务内容数据库存储了所述步骤1)上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级； 步骤；3)：用户注册来获取人机交互的初级权限；步骤4)：注册成功后，实时记载用户访问的情况，根据用户访问的情况来计算积分； 步骤幻：根据所述步骤4)得到的积分来判断应获得的权限等级，当客户确认兑换，命令服务端中的内容发布单元检索所述步骤幻的服务内容数据库中对应等级的数据信息； 步骤6)：所述步骤幻检索得到的数据信息调用并存储在内容发布单元中； 步骤7)：客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；步骤8)：当用户触摸触摸屏访问，服务端中的交互单元依据所述步骤7)中客户端发送过来的信息数据和时间点从内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给客户端的显示单元来与人情感互动交流；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的一种人机智能交互方法，其特征在于，所述步骤1)还包括在制作与人交互的事件的图片序列、视频和声音的展现素材时广告的植入、内容管理、监测、 统计和更新。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1或2所述的一种人机智能交互方法，其特征在于，所述步骤1)中的客户端为能联网的终端设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求1或2所述的一种人机智能交互方法，其特征在于，所述步骤1)中的客户端通过wifi、3G或GPRS与所述服务端通信。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求1或2所述的一种人机智能交互方法，其特征在于，所述步骤1)采用真实拍摄或动画制作的方式获取原视频流，经过处理后制作出人机交互的视频或图片序列，根据交互等级和时间点分类并上传所述服务内容数据库中。</div>
    </div>
    </div> <div class="claim"> <div num="6" class="claim">
      <div class="claim-text">6.	一种人机智能交互系统，其特征在于，该系统包括客户端、服务端和事件制作端；所述客户端包括传感单元、显示单元、积分计算单元和用户注册单元；所述服务端包括内容发布单元、积分兑换单元、服务内容数据库和交互单元；所述事件制作端，用于制作在客户端与人交互的事件的图片序列、视频和声音的展现素材，上传于所述服务内容数据库中；所述服务内容数据库，用于存储所述事件制作模块上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级；所述用户注册单元，用于用户注册来获取人机交互的初级权限；所述积分计算单元，用于实时记载用户访问的情况，根据用户访问的情况来计算积分；所述积分兑换单元，用于判断所述客户端积分计算单元得到的积分应获得的权限等级，客户确认兑换后，命令所述内容发布单元检索所述服务内容数据库中对应等级的数据 fn息；所述内容发布单元，用于检索得到相应权限等级的数据信息，并调用存储该数据信息；所述传感单元，用于接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；所述交互单元，用于当用户打开触摸屏访问时，依据所述传感单元发送过来的信息数据和时间点从所述内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给所述显示单元其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件；所述显示单元，用于显示该时间点相匹配的时间并与人情感互动交流。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.根据权利要求6所述的一种人机智能交互系统，其特征在于，所述事件制作端还包括广告管理单元，用于在制作与人交互的事件的图片序列、视频和声音的展现素材时广告的植入、内容管理、监测、统计和更新。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.根据权利要求6或7所述的一种人机智能交互系统，其特征在于，所述客户端为能联网的终端设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求6或7所述的一种人机智能交互系统，其特征在于，所述客户端通过 wifi、3G或GPRS与所述服务端通信。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10.根据权利要求6或7所述的一种人机智能交互系统，其特征在于，所述事件制作端采用真实拍摄或动画制作的方式获取原视频流，经过处理后制作出人机交互的视频或图片序列，根据交互等级和时间点分类并上传所述服务内容数据库中。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES47583671" lang="ZH" load-source="patent-office" class="description">
    <p>一种人机智能交互方法及系统</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及智能领域，特别涉及一种人机智能交互方法及系统。 背景技术</p>
    <p>[0002]	在计算机、移动终端（包括手机、平板电脑等）或者其他应用终端上（包括电冰箱人机交互系统、电子售卖机人机交互系统等)，只是存在着单一的事件交互，只有事件的体现。此体系体现在如：确认、取消、出发事件、购买等交互。</p>
    <p>[0003]	1965年为起点的的命令行式交互方式（dos操作系统人机交互方式），是通过输入专业的程序语言达到与计算机的沟通和命令。此方式需要用户学习底层的交互语言，以英文字母、公式、命令行等方式向计算机发送指令。如：此人机交互方式专业性极强，一般非专业人士无法操作；人机交互效率不高，一个简单的交互需要若干行专业命令组成；无法产生直观的乐趣，在抽象的命令符中，普通用户没有直观的娱乐感受。次方式在现在部分领域还在应用，多为专业领域。命令行形式的人机交互方式只存在单独的、原始的事件轴概念。</p>
    <p>[0004]	1984年为起点沿用至今的图形界面的人机交互方式，是通过人与计算机间的图形进行确认和沟通，完成交互过程。此方式是把人机交互命令形象成图形（如按钮、图片等），人更能直观的了解到操作的目的和遇见操作结果，进一步增加了人机交互的效率和乐趣。图形界面操作方式基于windows等操作系统，此方式需要一定的电脑操作基础，需要一定的专业知识。人机交互上是点击操作要求，计算机完成操作要求，此方式给人更多是“工具”、“电脑”、“机器”的交互感受；在以“人性为本”的科技发展方向下，图形界面交互方式的设计思路是让人适应机器的为初衷的，人的操作和交互受制于操作系统的操作思路。图形界面交互方式存在单独的事件概念。</p>
    <p>[0005]	总之，以上人机交互方式仅是单纯的机械互动，没有太多的人情味，因此越来越不能满足现代人们的需要。</p>
    <p>发明内容</p>
    <p>[0006]	本发明的目的在于，为解决上述问题，本发明提出一种人机智能交互方法及系统， 以人真实时间体系的时间点，该时间点对于每个人来说应该发生的多个事件中的一件及随着人机交互的发展获得的一个时间点越来越多的事件或功能，利用真人或虚拟表现元素为人机交互介质，通互联网服务器端的内容发布系统和客户端的展示系统，实现发布服务系统和客户端对时间、事件、交互进展的判断、分析、调用，给用户以虚拟真实的人机交互效</p>
    <p>：^ ο</p>
    <p>[0007]	为实现上述发明目的，本发明提出一种人机智能交互方法，该方法的具体工作步骤包括：</p>
    <p>[0008]	步骤1):制作在客户端与人交互的事件的图片序列、视频和声音的展现素材并上传于所述服务内容数据库中；</p>
    <p>[0009]	步骤幻：服务端中的服务内容数据库存储了所述步骤1)上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级；</p>
    <p>[0010]	步骤；3)：用户注册来获取人机交互的初级权限；</p>
    <p>[0011]	步骤4)：注册成功后，实时记载用户访问的情况，根据用户访问的情况来计算积分；</p>
    <p>[0012]	步骤5)：根据所述步骤4)得到的积分来判断应获得的权限等级，当客户确认兑换，命令服务端中的内容发布单元检索所述步骤幻的服务内容数据库中对应等级的数据 fn息；</p>
    <p>[0013]	步骤6)：所述步骤幻检索得到的数据信息调用并存储在内容发布单元中；</p>
    <p>[0014]	步骤7)：客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；</p>
    <p>[0015]	步骤8)：当用户触摸触摸屏访问，服务端中的交互单元依据所述步骤7)中客户端发送过来的信息数据和时间点从内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给客户端的显示单元来与人情感互动交流；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件。</p>
    <p>[0016]	更优选地，所述步骤1)还包括在制作与人交互的事件的图片序列、视频和声音的展现素材时广告的植入、内容管理、监测、统计和更新。</p>
    <p>[0017]	更优选地，所述步骤1)中的客户端为能联网的终端设备。</p>
    <p>[0018]	更优选地，所述步骤1)中的客户端通过wifi、3G或GPRS与所述服务端通信。</p>
    <p>[0019]	更优选地，所述步骤1)采用真实拍摄或动画制作的方式获取原视频流，经过处理后制作出人机交互的视频或图片序列，根据交互等级和时间点分类并上传所述服务内容数据库中。</p>
    <p>[0020]	为实现上述发明目的，本发明还提出一种人机智能交互系统，该系统包括客户端、 服务端和事件制作端；所述客户端包括传感单元、显示单元、积分计算单元和用户注册单元；所述服务端包括内容发布单元、积分兑换单元、服务内容数据库和交互单元；</p>
    <p>[0021]	所述事件制作端，用于制作在客户端与人交互的事件的图片序列、视频和声音的展现素材，上传于所述服务内容数据库中；</p>
    <p>[0022]	所述服务内容数据库，用于存储所述事件制作模块上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级；</p>
    <p>[0023]	所述用户注册单元，用于用户注册来获取人机交互的初级权限；</p>
    <p>[0024]	所述积分计算单元，用于实时记载用户访问的情况，根据用户访问的情况来计算积分；</p>
    <p>[0025]	所述积分兑换单元，用于判断所述客户端积分计算单元得到的积分应获得的权限等级，客户确认兑换后，命令所述内容发布单元检索所述服务内容数据库中对应等级的数据信息；</p>
    <p>[0026]	所述内容发布单元，用于检索得到相应权限等级的数据信息，并调用存储该数据 fn息；</p>
    <p>[0027]	所述传感单元，用于接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；</p>
    <p>[0028]	所述交互单元，用于当用户打开触摸屏访问时，依据所述传感单元发送过来的信息数据和时间点从所述内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给所述显示单元其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件；</p>
    <p>[0029]	所述显示单元，用于显示该时间点相匹配的时间并与人情感互动交流。</p>
    <p>[0030]	更优选地，所述事件制作端还包括广告管理单元，用于在制作与人交互的事件的图片序列、视频和声音的展现素材时广告的植入、内容管理、监测、统计和更新。</p>
    <p>[0031]	更优选地，所述客户端为能联网的终端设备。</p>
    <p>[0032]	更优选地，所述客户端通过wifi、3G或GPRS与所述服务端通信。</p>
    <p>[0033]	更优选地，所述事件制作端采用真实拍摄或动画制作的方式获取原视频流，经过处理后制作出人机交互的视频或图片序列，根据交互等级和时间点分类并上传所述服务内容数据库中。</p>
    <p>[0034]	本发明具备如下几个优点：本发明的技术方案可以达到虚拟真实的人机互动，并且在相对应的时间段达到对应现实的交互方式，在每个时间段内的交互方式多种多样，用户体验不会感到重复无趣。根据用户的不断使用，可以产生更为丰富多彩的人机交流素材或功能。本发明的人机交互界面简单易用，有更好的用户体验，用户可以在相应等级可以获取相应的功能使用体验。随着用户使用该系统的时间、频率等数值的增长，用户将能逐步使用更多的功能及与更多的真人视频产生互动。另外，在人机交互的视频素材中进行广告的植入和管理，具有巨大的经济效益。</p>
    <p>附图说明</p>
    <p>[0035]	图1为本发明提出的一种人机智能交互方法中事件的制作流程图之一；</p>
    <p>[0036]	图2为本发明提出的一种人机智能交互方法中事件的制作流程图之二 ；</p>
    <p>[0037]	图3为本发明提出的人机智能交互方法及系统的基本工作原理示意图；</p>
    <p>[0038]	图4为本发明提出的一种人机智能交互方法的流程图；</p>
    <p>[0039]	图5为本发明提出的一种人机智能交互系统的功能框图。</p>
    <p>具体实施方式</p>
    <p>[0040]	下面结合附图和具体实施方式，对本发明的技术方案进行进一步详细的说明。</p>
    <p>[0041]	以人真实时间体系的时间点，该时间点对于每个人来说应该发生的多个事件中的一件及随着人机交互的发展获得的一个时间点越来越多的事件或功能，利用真人或虚拟表现元素为人机交互介质，通互联网服务器端的内容发布系统和客户端的展示系统，实现发布服务系统和客户端对时间、事件、交互进展的判断、分析、调用，给用户以虚拟真实的人机交互效果。其中，“事件” 一词的含义是时间和空间所指定的时空中的一件事情。</p>
    <p>[0042]	比如：早上8:00这个时间点或者7:55&#12316;8:30这一时间段内对应的事件可能为开始离开家去上班、吃早餐和在睡懒觉。这些事件是由图片序列、视频、声音是模拟真实操作的展现素材，作为虚拟真实的人机交互元素，包括前期的策划、拍摄、导演、角色团队，中期的拍摄设备、灯光设备、道具、拍摄场地等硬件环境、后期的视频处理、配音、合成、压缩等后期处理。</p>
    <p>[0043]	如图1所示，图1为本发明提出的一种人机智能交互方法中事件的制作流程图之</p>
    <p>一。用真人拍摄的方式，设定相应故事和环境，通过真实拍摄、动画制作等方式获取原视频流，经过后期处理后，制作出人机交互的视频或图片序列。</p>
    <p>[0044]	如图2所示，图2为本发明提出的一种人机智能交互方法中事件的制作流程图之</p>
    <p>二。通过客户端的拍摄或服务器端提供现有的内容，利用终端3d实时渲染的方式获得虚拟的人机交互形象，在通过服务器端更新或终端内置的动作、场景、声音等合成，达到虚拟真实的人机交互体验。</p>
    <p>[0045]	本发明提出的一种人机智能交互方法及系统。其基本核心原理用三维的坐标系来表示。如图3所示，图3为本发明提出的人机智能交互方法及系统的基本工作原理示意图。 基本工作原理用三维坐标系来表示，第一坐标轴为时间轴，第二坐标轴为事件轴，第三坐标轴为发展轴。</p>
    <p>[0046]	其中，时间轴为整个体系中的基础元素，也是区别与现有人机交互系统的，让用户产生真实体验的基础。时间轴是整体系统中的框架，事件轴和发展轴都按照时间轴的框架来展现的。</p>
    <p>[0047]	时间轴的控制是由服务端后台程序实现，通过后台系统对时间轴的时间区段进行管理，包括增加区段、删除区段、合并区段等。</p>
    <p>[0048]	时间轴的展现是通过客户端来实现，客户端在链接互联网的时候，服务端判断客户端用户的时间轴状态和用户信息状态，即使同步时间轴信息，达到服务器和客户端的统</p>
    <p>ο</p>
    <p>[0049]	时间轴按照真实人类的时间运行，并由“人”，也就是运营团队控制和规范。事件轴为在真实世界中，人与人、人与事物的发展都伴随着具体的事件，本系统中的事件轴是虚拟真实世界中的事件。事件轴是在同一时间轴的基础上，不同的表现内容，这些内容在按照服务端控制这些内容的排序和展现。</p>
    <p>[0050]	事件轴的控制是由服务端后台程序实现，通过后台系统对同一时间轴进行管理， 包括内容（视频、图片序列等）的增加、删减。内容以单个文件的形式，通过后台发布系统指定到时间轴某个点上。事件轴是在客户端来展现的，用户在访问客户端的时候，直接展现为对应时间轴上的具体事件内容。</p>
    <p>[0051]	事件轴还包括通过与客户端的交互，事件会与用户产生交互。客户端如果是一个手机，当用户触摸手机屏幕时，手机具有传感器的功能，感应周围的环境因素，或手机接收到移动服务器发送的天气预报等其他外界信息后，手机把上述信息发送给服务端，服务端智能匹配出相关已经在该时间点设置的多个事件中的相应事件来。</p>
    <p>[0052]	比如，早上8:05分，用户的手机已经收到移动服务中心发布的天气预报信息，预报今天有小到中雨。用户触摸了手机触摸屏，进入了人机交互系统，该系统根据当时用户真实的时间来调出该时间段的相关事件来，该事件是提前制作的，并且通过服务端智能匹配出该事件，在手机屏幕上展现该事件图像视频。例如用户开始上班，那么展现的视频图像为一个美女打扮的漂亮，并带着雨伞准备出门上班。甚至视频上的美女还温柔的说“路上注意安全，准时上班”。如果用户的手机收到移动服务中心发布的天气预报信息，预报今天晴天， 那么展现的视频图像为一个打扮漂亮的美女，头戴遮阳帽准备出门上班。并且用户还可以在手机触摸屏上不同操作热区与视频中的事件产生互动，例如：登陆该系统后，展现在手机视频上的是一个美女在端着一杯咖啡，用户点击咖啡杯，视频中的美女会把咖啡举起来示意请您喝。用户点击美女的手机区域，视频中的美女在视频中搅拌咖啡。</p>
    <p>[0053]	事件轴按照运营团队控制和规范，通过事件制作端制作相关事件内容，根据客户端实时感应的周围信息和通过外界接收的信息来匹配出相关时间点或时间段对应的事件来。</p>
    <p>[0054]	事件轴中的商业拓展是基于事件轴对应具体的交互内容展现和虚拟主角的表演为基础。在客户端下载的同时，也形成的可观的媒体效应，道具、服装、故事脚本等一些列的商业内容植入，为平台实际的运行带来了可持续发展的基础。</p>
    <p>[0055]	在真实世界里，人与人之间的关系随着时间的推进和平实交流等事件的发展，人与人的关系得以发展，从而在以后的时间里会有更深事件发生，本发明中的发展轴就是虚拟了真实世界中的事物发展状态。</p>
    <p>[0056]	发展轴的控制是由服务端后台程序实现。通过后台系统，设定用户在某一条件下， 对应发展轴的内容（视频、图片序列等）以单个文件形式，发布到指定到时间轴某个点上。</p>
    <p>[0057]	发展轴是在客户端来展现的，客户在访问客户端并在链接互联网的状态下，通过对比客户端的相应指数和服务器端对发展轴中内容进行对比，达到要求后通过服务端和客户端的更新系统，更新到客户端并展现给用户。此方式虚拟了人与人与人事物间的发展，通过指数的方式“翻译”给服务器端系统。</p>
    <p>[0058]	发展轴通过数值来作为计算单位，客户在操作客户端的时候，会积累相应的数值以加奖励，达到一定数值后，经服务器端判断可以领取相应的功能或内容作为奖励，在用户确认后，系统下载相关内容，系统扣取新功能相应的数值。</p>
    <p>[0059]	例如1 ：本发明的技术方案的工作原理中发展轴通过积分指数累计体系：</p>
    <p>[0060]	A-访问用户端的次数：每一次计提1个积分数值；</p>
    <p>[0061]	B-访问用户端累计时间：每10分钟计提1个积分数值；</p>
    <p>[0062]	C-某个应用功能中领取相应的数值：计提为C ；</p>
    <p>[0063]	用户累计积分数值公式为：A+B+C = X(累计值）；</p>
    <p>[0064]	例如2 ：本发明的技术方案的工作原理中发展轴积分兑换方式之一：</p>
    <p>[0065]	X-用户累计数值；</p>
    <p>[0066]	Y-发展轴中发展内容兑换要求数值；</p>
    <p>[0067]	Xl-用户剩余数值；</p>
    <p>[0068]	用户兑换公式为：X-Y = Xl (累计值）；</p>
    <p>[0069]	发展轴中兑换机制：本发明的技术方案的工作原理中发展轴对应的新内容和更高级的功能，用户可以以积分累计方式兑换，也可以通过现实中购买的方式购得。</p>
    <p>[0070]	发展轴的规划：发展轴的规划及内容由运营团队通过服务端设置时规范。</p>
    <p>[0071]	综上所述，本发明技术方案的工作原理的实际运用的体现为：先根据真实环境的全天作息时间设定时间段，在每个小时至每个分钟都设定视频随机播放。视频内容与真实时间的设定相匹配，在用户打开触摸屏时，服务器根据当时用户所处的真实时间和环境因素从众多时间中调取合适的视频展现。在进入下一个时间节点时系统自动调取匹配此真实时间的视频展现。除了在一个M小时（一天）周期的视频内容匹配外，用户在不同真实日期所能了解到的视频内容会随着用户的使用时间及频率进行改变。系统在用户使用时即会调取用户使用数据（时间、喜好及频率等）并进行分析，通过数据分析得到的结果自动为用户匹配新的视频内容。系统会通过的视频在工作日、非工作日、节假日的系统自动匹配相应的真实日期的视频内容。在同一个时间点上，除了与时间相关的视频内容，系统会根据用户触摸真人的部位匹配相应的交互视频。通过每日时间节点的设定、每个时间节点上视频匹配内容的设定、用户数据提取分析的设定、每天视频内容与真实时间的匹配设定架构起一个增长的生态系统。那么，本发明的一种人机智能交互方法如图4所示。该方法的具体工作步骤包括：</p>
    <p>[0072]	步骤1):制作在客户端与人交互的事件的图片序列、视频和声音的展现素材并上传于所述服务内容数据库中；</p>
    <p>[0073]	步骤幻：服务端中的服务内容数据库存储了所述步骤1)上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级；</p>
    <p>[0074]	步骤幻：用户注册来获取人机交互的初级权限；</p>
    <p>[0075]	步骤4)：注册成功后，实时记载用户访问的情况，根据用户访问的情况来计算积分；</p>
    <p>[0076]	步骤幻：根据所述步骤4)得到的积分来判断应获得的权限等级，当客户确认兑换，命令服务端中的内容发布单元检索所述步骤幻的服务内容数据库中对应等级的数据 fn息；</p>
    <p>[0077]	步骤6)：所述步骤幻检索得到的数据信息调用并存储在内容发布单元中；</p>
    <p>[0078]	步骤7)：客户端接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；</p>
    <p>[0079]	步骤8)：当用户触摸触摸屏访问，服务端中的交互单元依据所述步骤7)中客户端发送过来的信息数据和时间点从内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给客户端的显示单元来与人情感互动交流；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件。</p>
    <p>[0080]	如图5所示，图5为本发明提出的一种人机智能交互系统的功能框图。该系统包括客户端、服务端和事件制作端。客户端与服务端通过wifi、3G或GPRS等有线或无线技术联网通信相连，服务端与事件制作端相连。所述客户端包括传感单元、显示单元、积分计算单元和用户注册单元；所述服务端包括内容发布单元、积分兑换单元、服务内容数据库和交互单元；</p>
    <p>[0081]	所述事件制作端，用于制作在客户端与人交互的事件的图片序列、视频和声音的展现素材，上传于所述服务内容数据库中；</p>
    <p>[0082]	所述服务内容数据库，用于存储所述事件制作模块上传的信息，该信息以三维坐标形式存储；其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件，第三坐标为客户端人机交互发展等级；</p>
    <p>[0083]	所述用户注册单元，用于用户注册来获取人机交互的初级权限；</p>
    <p>[0084]	所述积分计算单元，用于实时记载用户访问的情况，根据用户访问的情况来计算积分；[0085]	所述积分兑换单元，用于判断所述客户端积分计算单元得到的积分应获得的权限等级，客户确认兑换后，命令所述内容发布单元检索所述服务内容数据库中对应等级的数据信息；</p>
    <p>[0086]	所述内容发布单元，用于检索得到相应权限等级的数据信息，并调用存储该数据 fn息；</p>
    <p>[0087]	所述传感单元，用于接收外界发送的信息，同时实时感应周围信息，并把该信息传输给交互单元；</p>
    <p>[0088]	所述交互单元，用于当用户打开触摸屏访问时，依据所述传感单元发送过来的信息数据和时间点从所述内容发布单元匹配出相应事件，并把事件的三维数据转换为二维数据后发送给所述显示单元其中，第一坐标为按照人类全天作息规律设置的时间点，第二坐标为时间点对应的事件；</p>
    <p>[0089]	所述显示单元，用于显示该时间点相匹配的时间并与人情感互动交流。</p>
    <p>[0090]	客户端：为应用平台终端软件，如手机、平板电脑、家用电器显示屏等。是展现更新内容、完成人机交互、向服务器端反馈用户信息的作用，比喻成整体系统的“骨头框架”。客户端以java、php、asp、或.net为开发基础，通过调用平台操作系统API达到功能的统一。 客户端通过wifi、3G或GPRS等有线或无线技术方式链接服务器，客户端根据用户相关指数，判断和下载相关更新内容，如视频、图像序列、新功能模块等。客户端在通过判断客户端的时间、地理位置等个人信息，展现相关更新内容，从而实现用户以时间、事件、交互发展进度的模拟真实的用户体验。客户端是与人直接交互的设备，如手机、平板电脑、电脑、家电信息界面等。</p>
    <p>[0091]	服务端：互联网服务器端是本系统的核心，起着大脑作用。服务端系统以jsp为服务器端开发基础，构架互联网服务器端更新系统、内容数据库和用户数据库。通过与客户端无线或者有线的方式，完成客户端的内容、程序、服务的更新。服务端主要作用是与终端平台达到内容的更新、同步、控制作用。是让人机交互产生虚拟真实的核心。服务端包括：</p>
    <p>[0092]	轴管理系统：控制时间轴、时间轴中事件轴、以及在时间轴和事件周对应的事态发展轴。</p>
    <p>[0093]	用户管理系统：用户注册、登录、用户数据判断、用户管理等。</p>
    <p>[0094]	内容发布系统：更新元素的指定、分配、发布、同步、管理等。</p>
    <p>[0095]	广告管理系统：广告属性的更新内容管理、监测、统计、更新系统。</p>
    <p>[0096]	事件制作端：制作端生产出来的图片序列、视频、声音是模拟真实操作的展现素材，在整体系统中，起着“肉”的作用。作为虚拟真实的人机交互元素，制作真实的体验是基础。获得虚拟真实的人机交互方式，通过以下两种方式。制作端是内容制作体系，包括前期的策划、拍摄、导演、角色团队，中期的拍摄设备、灯光设备、道具、拍摄场地等硬件环境、后期的视频处理、配音、合成、压缩等后期处理。通过真实拍摄、动画制作等方式获取原视频流，经过后期处理后，制作出人机交互的视频或图片序列。或通过客户端的拍摄或服务器端提供现有的内容，利用终端3d实时渲染的方式获得虚拟的人机交互形象，在通过服务器端更新或终端内置的动作、场景、声音等合成，达到虚拟真实的人机交互体验。</p>
    <p>[0097]	最后所应说明的是，以上实施例仅用以说明本发明的技术方案而非限制。尽管参照实施例对本发明进行了详细说明，本领域的普通技术人员应当理解，对本发明的技术方案进行修改或者等同替换，都不脱离本发明技术方案的精神和范围，其均应涵盖在本发明的权利要求范围当中。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101127621A?cl=zh">CN101127621A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年9月26日</td><td class="patent-data-table-td patent-date-value">2008年2月20日</td><td class="patent-data-table-td ">许立新</td><td class="patent-data-table-td ">虚拟互联网跨媒体系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101201738A?cl=zh">CN101201738A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2006年12月14日</td><td class="patent-data-table-td patent-date-value">2008年6月18日</td><td class="patent-data-table-td ">英业达股份有限公司</td><td class="patent-data-table-td ">通过计算机执行多媒体互动人机界面的系统及其方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DTW%26NR%3D200901020A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGsUP6FT9S8yQin4jgGi3HNj2P7Ww">TW200901020A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">没有名称</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=H04L0029080000">H04L29/08</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=nbxYBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年6月20日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年7月18日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2014年1月22日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年4月8日</td><td class="patent-data-table-td ">ASS</td><td class="patent-data-table-td ">Succession or assignment of patent right</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">WEINI TECHNOLOGY (BEIJING) CO., LTD.</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">FORMER OWNER: JIN XIN</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20150319</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">FORMER OWNER: LI GEJING</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20150319</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年4月8日</td><td class="patent-data-table-td ">C41</td><td class="patent-data-table-td ">Transfer of patent application or patent right or utility model</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年4月8日</td><td class="patent-data-table-td ">COR</td><td class="patent-data-table-td ">Change of bibliographic data</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CORRECT: ADDRESS; FROM: 100055 XICHENG, BEIJING TO: 100044 XICHENG, BEIJING</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/2c1a5403d9689ab5263d/CN102508848A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E4%BA%BA%E6%9C%BA%E6%99%BA%E8%83%BD%E4%BA%A4%E4%BA%92%E6%96%B9%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9F.pdf?id=nbxYBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U1q_heB_T5Ec6RnQLoTvQLNacOv7Q"},"sample_url":"https://www.google.com/patents/reader?id=nbxYBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>