<!DOCTYPE html><html><head><title>专利 CN102238190A - 身份认证方法及系统 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="身份认证方法及系统"><meta name="DC.contributor" content="何婷婷" scheme="inventor"><meta name="DC.contributor" content="刘庆峰" scheme="inventor"><meta name="DC.contributor" content="潘逸倩" scheme="inventor"><meta name="DC.contributor" content="王智国" scheme="inventor"><meta name="DC.contributor" content="胡国平" scheme="inventor"><meta name="DC.contributor" content="胡郁" scheme="inventor"><meta name="DC.contributor" content="魏思" scheme="inventor"><meta name="DC.contributor" content="安徽科大讯飞信息科技股份有限公司" scheme="assignee"><meta name="DC.date" content="2011-8-1" scheme="dateSubmitted"><meta name="DC.description" content="本发明公开了一种身份认证方法及系统，该方法包括：在用户登录时，接收当前登录用户录入的连续语音信号；提取所述连续语音信号中的声纹特征序列；计算所述声纹特征序列与背景模型的似然度；计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度；所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；如果所述似然比大于设定的阈值，则确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。利用本发明，可以提高基于声纹密码进行身份认证的准确率。"><meta name="DC.date" content="2011-11-9"><meta name="DC.relation" content="CN:101833951:A" scheme="references"><meta name="DC.relation" content="CN:102024455:A" scheme="references"><meta name="DC.relation" content="US:20060111905:A1" scheme="references"><meta name="DC.relation" content="US:20080059156:A1" scheme="references"><meta name="citation_patent_publication_number" content="CN:102238190:A"><meta name="citation_patent_application_number" content="CN:201110218045"><link rel="canonical" href="https://www.google.com/patents/CN102238190A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102238190A?cl=zh"/><meta name="title" content="专利 CN102238190A - 身份认证方法及系统"/><meta name="description" content="本发明公开了一种身份认证方法及系统，该方法包括：在用户登录时，接收当前登录用户录入的连续语音信号；提取所述连续语音信号中的声纹特征序列；计算所述声纹特征序列与背景模型的似然度；计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度；所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；如果所述似然比大于设定的阈值，则确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。利用本发明，可以提高基于声纹密码进行身份认证的准确率。"/><meta property="og:title" content="专利 CN102238190A - 身份认证方法及系统"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102238190A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102238190A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102238190A&amp;usg=AFQjCNFXJ4ci-SlcfWhivOvO_ia5c_oBAw" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/b5011886f1217d14543a/CN102238190A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/b5011886f1217d14543a/CN102238190A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102238190A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102238190A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102238190A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102238190B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102238190A?cl=zh" style="display:none"><span itemprop="description">本发明公开了一种身份认证方法及系统，该方法包括：在用户登录时，接收当前登录用户录入的连续语音信号；提取所述连续语音信号中的声纹特征序列；计算所述声纹特征序列与背景模型的似然度；计算所述声纹特征序列与所...</span><span itemprop="url">https://www.google.com/patents/CN102238190A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102238190A - 身份认证方法及系统</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102238190A - 身份认证方法及系统" title="专利 CN102238190A - 身份认证方法及系统"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102238190 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110218045</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2011年11月9日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年8月1日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2011年8月1日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102238190B?hl=zh-CN&amp;cl=zh">CN102238190B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110218045.2, </span><span class="patent-bibdata-value">CN 102238190 A, </span><span class="patent-bibdata-value">CN 102238190A, </span><span class="patent-bibdata-value">CN 201110218045, </span><span class="patent-bibdata-value">CN-A-102238190, </span><span class="patent-bibdata-value">CN102238190 A, </span><span class="patent-bibdata-value">CN102238190A, </span><span class="patent-bibdata-value">CN201110218045, </span><span class="patent-bibdata-value">CN201110218045.2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E4%BD%95%E5%A9%B7%E5%A9%B7%22">何婷婷</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%88%98%E5%BA%86%E5%B3%B0%22">刘庆峰</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%BD%98%E9%80%B8%E5%80%A9%22">潘逸倩</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E6%99%BA%E5%9B%BD%22">王智国</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%83%A1%E5%9B%BD%E5%B9%B3%22">胡国平</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%83%A1%E9%83%81%22">胡郁</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%AD%8F%E6%80%9D%22">魏思</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%AE%89%E5%BE%BD%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">安徽科大讯飞信息科技股份有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102238190A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102238190A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102238190A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (3),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (4)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110218045&amp;usg=AFQjCNFyE0-4PAdkI2CwQ0udRX7kQUuqxw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102238190A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHGRdA82gEBjEToxedmhiMnVX_PeQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT104677583" lang="ZH" load-source="patent-office">身份认证方法及系统</invention-title>
      </span><br><span class="patent-number">CN 102238190 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA86582394" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明公开了一种身份认证方法及系统，该方法包括：在用户登录时，接收当前登录用户录入的连续语音信号；提取所述连续语音信号中的声纹特征序列；计算所述声纹特征序列与背景模型的似然度；计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度；所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；如果所述似然比大于设定的阈值，则确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。利用本发明，可以提高基于声纹密码进行身份认证的准确率。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(18)</span></span></div><div class="patent-text"><div mxw-id="PCLM37054943" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种身份认证方法，其特征在于，包括：在用户登录时，接收当前登录用户录入的连续语音信号；提取所述连续语音信号中的声纹特征序列，所述声纹特征序列包含一组声纹特征；计算所述声纹特征序列与背景模型的似然度；计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；如果所述似然比大于设定的阈值，则确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.如权利要求1所述的方法，其特征在于，所述计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度包括：分别计算所述声纹特征序列与每个混合高斯模型的似然度；根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.如权利要求2所述的方法，其特征在于，所述分别计算所述声纹特征序列与每个混合高斯模型的似然度包括：分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；选择所述声纹特征序列中一组声纹特征对应一个混合高斯模型计算得到的似然度总和的时间平均值作为所述声纹特征序列与该混合高斯模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.如权利要求2所述的方法，其特征在于，所述根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度包括：选择所述声纹特征序列对应所有混合高斯模型计算得到的似然度的平均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度；或者选择所述声纹特征序列对应所有混合高斯模型计算得到的似然度的最大值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.如权利要求1所述的方法，其特征在于，所述计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度包括：分别计算所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度；根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.如权利要求5所述的方法，其特征在于，所述分别计算所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度包括：分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的似然度中的最大值作为该声纹特征与所述多混合高斯模型的似然度；或者， 选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的所有似然度的平均值作为该声纹特征与所述多混合高斯模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.如权利要求5所述的方法，其特征在于，所述根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度包括：选择所述声纹特征序列中所有声纹特征对应多混合高斯模型计算得到的似然度的时间平均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.如权利要求1至7任一项所述的方法，其特征在于，所述方法还包括： 在用户注册时，接收所述用户录入的注册语音信号；根据所述注册语音信号构建所述用户的说话人模型，该过程包括： 从所述注册语音信号中提取声纹特征；根据所述注册语音信号的重复次数及帧数确定所述用户的说话人模型的所有混合高斯模型；根据从所述注册语音信号中提取的声纹特征估计所述用户的说话人模型的所有混合高斯模型的高斯均值参数；根据从所述注册语音信号中提取的声纹特征估计所述用户的说话人模型的所有混合高斯模型的高斯方差参数。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.如权利要求8所述的方法，其特征在于，所述根据所述注册语音信号的重复次数及帧数确定所述用户的说话人模型的所有混合高斯模型包括：设定所述用户的说话人模型的混合高斯模型数小于或等于所述注册语音信号的重复次数；设定对应于每一个混合高斯模型的高斯数小于或等于所述混合高斯模型对应的注册语音信号的帧数。</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10.	一种身份认证系统，其特征在于，包括：语音信号接收单元，用于在用户登录时，接收当前登录用户录入的连续语音信号； 提取单元，用于提取所述连续语音信号中的声纹特征序列，所述声纹特征序列包含一组声纹特征；第一计算单元，用于计算所述声纹特征序列与背景模型的似然度； 第二计算单元，用于计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；第三计算单元，用于根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；判断单元，用于在所述第三计算单元计算得到的似然比大于设定的阈值时，确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11.如权利要求10所述的系统，其特征在于，所述第二计算单元包括：第一计算子单元，用于分别计算所述声纹特征序列与每个混合高斯模型的似然度； 第一确定子单元，用于根据所述第一计算子单元的计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12.如权利要求11所述的系统，其特征在于，所述第一计算子单元包括：第一计算模块，用于分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；第一选择模块，用于选择所述声纹特征序列中一组声纹特征对应一个混合高斯模型计算得到的似然度总和的时间平均值作为所述声纹特征序列与该混合高斯模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13.如权利要求11所述的系统，其特征在于，所述第一确定子单元，具体用于选择所述声纹特征序列对应所有混合高斯模型计算得到的似然度的平均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度； 或者，选择所述声纹特征序列对应所有混合高斯模型计算得到的似然度的最大值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14.如权利要求10所述的系统，其特征在于，所述第二计算单元包括：第二计算子单元，用于分别计算所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度；第二确定子单元，用于根据所述第二计算子单元的计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15.如权利要求14所述的系统，其特征在于，所述第二计算子单元包括：第二计算模块，用于分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；第二选择模块，用于选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的似然度中的最大值作为该声纹特征与所述多混合高斯模型的似然度；或者选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的所有似然度的平均值作为该声纹特征与所述多混合高斯模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16.如权利要求14所述的系统，其特征在于，所述第二确定子单元，具体用于选择所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度的时间平均值为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17.如权利要求10至16任一项所述的系统，其特征在于，所述语音信号接收单元，还用于在用户注册时，接收所述用户录入的注册语音信号；所述系统还包括：模型构建单元，用于根据所述注册语音信号构建所述用户的说话人模型，所述模型构建单元包括：特征提取子单元，用于从所述注册语音信号中提取声纹特征；拓扑结构确定子单元，用于根据所述注册语音信号的重复次数及帧数确定所述用户的说话人模型的所有混合高斯模型；第一估计子单元，用于利用所述特征提取子单元提取的声纹特征估计所述拓扑结构确定子单元确定的所有混合高斯模型的高斯均值参数；第二估计子单元，用于利用所述特征提取子单元提取的声纹特征估计所述拓扑结构确定子单元确定的所有混合高斯模型的高斯方差参数。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18.如权利要求17所述的系统，其特征在于，所述拓扑结构确定子单元，具体用于设定所述用户的说话人模型的混合高斯模型数小于或等于所述注册语音信号的重复次数；设定对应于每一个混合高斯模型的高斯数小于或等于所述混合高斯模型对应的注册语音信号的帧数。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES42708859" lang="ZH" load-source="patent-office" class="description">
    <p>身份认证方法及系统</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及身份识别技术领域，特别涉及一种身份认证方法及系统。 背景技术</p>
    <p>[0002]	声纹识别（Voiaprint Recognition, VPR)也称为说话人识别，有两类，即说话人辨认和说话人确认。前者用以判断某段语音是若干人中的哪一个所说的，是“多选一”问题； 而后者用以确认某段语音是否是指定的某个人所说的，是“一对一判别”问题。不同的任务和应用会使用不同的声纹识别技术。</p>
    <p>[0003]	声纹认证是指根据采集到的语音信号确认说话人身份，属于“一对一”的判别问题。现今主流的声纹认证系统采用了基于假设检验的框架，通过分别计算声纹信号相对于说话人模型以及背景模型的似然度并比较它们的似然比和预先根据经验设置的阈值大小来确认。显然背景模型和说话人模型的精确度将直接影响到声纹认证效果，在基于数据驱动的统计模型设定下训练数据量越大则模型效果越好。</p>
    <p>[0004]	声纹密码认证是一种文本相关的说话人身份认证方法。该方法要求用户语音输入确定密码文本，并据此确认说话人身份。在该应用中用户注册及身份认证均采用确定密码文本的语音输入，因而其声纹往往较为一致，相应的可取得相比于文本无关的说话人确认更好的认证效果。</p>
    <p>[0005]	现今声纹密码认证系统最为主流的技术路线是GMM-UBM算法，即分别采用了混合高斯（Gaussian Mixture Model,GMM)模型模拟背景模型（Universal Background Model, UBM)及说话人模型。UBM模型用于描述说话人声纹的共性。由于各说话人声纹总有各自特异性，相应的基于多说话人训练数据的UBM模型需要复杂的模型结构以满足分布分散数据的拟合要求。目前UBM模型通常选择10&#190;甚至更大高斯数的GMM模型。</p>
    <p>[0006]	说话人模型由系统在用户注册时根据注册语音在线训练得到。由于注册用语音样本往往有限，直接据此训练复杂模型由于数据稀疏易导致模型不够精确等问题。为此，在现有技术中，通常是以背景模型为初始模型通过各种自适应方法根据少量说话人数据调整模型部分参数，如目前最为常用的基于最大后验概率（Maximum A Posterior, MAP)的自适应算法等，将用户声纹共性自适应为当前说话人个性。</p>
    <p>[0007]	在自适应更新算法下说话人的混合高斯模型和通用背景高斯模型的各高斯之间形成一对一的对应关系，因此，使得说话人模型参数过多，在注册数据量较少的声纹密码认证系统中容易导致以下问题：</p>
    <p>[0008]	1.模型冗余：声纹密码认证系统中说话人模型是由几遍注册语音密码重复的样本数据训练得到的。过少的样本数据导致自适应算法只能更新初始背景模型中部分高斯， 而很多都保留了和背景模型类似的高斯分量。冗余模型参量的存在容易导致存储及运算压力的增大，进而影响解码的效率。</p>
    <p>[0009]	2.模型训练量较大：在自适应算法中，需要计算初始背景模型的10M甚至更大高斯数的每个高斯的样本统计量，并对其参数更新。[0010]	3.在自适应算法中，由于说话人模型的方差重估较为困难，因而往往直接采用背景模型的方差。由于背景模型是基于多说话人训练数据得到的模拟声纹共性的模型，其模型概率分布方差往往较大。而说话人模型的方差模拟的说话人特定声纹的特点，具有特异性。直接用背景模型方差不能很好地体现说话人模型特点，降低了不同说话人模型之间的区分性，从而影响识别准确率。</p>
    <p>发明内容</p>
    <p>[0011]	本发明实施例提供一种身份认证方法和系统，以提高基于声纹密码进行身份认证的准确率。</p>
    <p>[0012]	本发明实施例一方面提供一种身份认证方法，包括：</p>
    <p>[0013]	在用户登录时，接收当前登录用户录入的连续语音信号；</p>
    <p>[0014]	提取所述连续语音信号中的声纹特征序列，所述声纹特征序列包含一组声纹特征；</p>
    <p>[0015]	计算所述声纹特征序列与背景模型的似然度；</p>
    <p>[0016]	计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；</p>
    <p>[0017]	根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；</p>
    <p>[0018]	如果所述似然比大于设定的阈值，则确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。</p>
    <p>[0019]	本发明实施例另一方面提供一种身份认证系统，包括：</p>
    <p>[0020]	语音信号接收单元，用于在用户登录时，接收当前登录用户录入的连续语音信号；</p>
    <p>[0021]	提取单元，用于提取所述连续语音信号中的声纹特征序列，所述声纹特征序列包含一组声纹特征；</p>
    <p>[0022]	第一计算单元，用于计算所述声纹特征序列与背景模型的似然度；</p>
    <p>[0023]	第二计算单元，用于计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；</p>
    <p>[0024]	第三计算单元，用于根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；</p>
    <p>[0025]	判断单元，用于在所述第三计算单元计算得到的似然比大于设定的阈值时，确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。</p>
    <p>[0026]	本发明实施例提供的身份认证方法和系统，根据当前登录用户录入的连续语音信号中的声纹特征序列，分别计算声纹特征序列与当前登录用户的说话人模型及背景模型的似然度，然后计算似然比，根据得到的似然比确定当前登录用户是否为有效认证用户。由于在该方案中，所使用的说话人模型是根据当前登录用户注册时录入的语音信号构建的多混合高斯模型，从而可以模拟所述用户说出同一语音信号（即密码）存在的不同发音变化的特点，提高了基于声纹密码进行身份认证的准确率。 附图说明</p>
    <p>[0027]	为了更清楚地说明本发明实施的技术方案，下面将对实施例中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。</p>
    <p>[0028]	图1是本发明实施例身份认证方法的流程图；</p>
    <p>[0029]	图2是本发明实施例中背景模型参数训练过程的一种流程图；</p>
    <p>[0030]	图3是传统的利用自适应算法构建说话人模型的流程图；</p>
    <p>[0031]	图4是本发明实施例中构建说话人模型的流程图；</p>
    <p>[0032]	图5是本发明实施例身份认证系统的一种结构示意图；</p>
    <p>[0033]	图6是本发明实施例身份认证系统的另一种结构示意图。</p>
    <p>具体实施方式</p>
    <p>[0034]	下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。</p>
    <p>[0035]	如图1所示，是本发明实施例身份认证方法的流程图，包括以下步骤：</p>
    <p>[0036]	步骤101，在用户登录时，接收当前登录用户录入的连续语音信号。</p>
    <p>[0037]	步骤102，提取所述连续语音信号中的声纹特征序列。</p>
    <p>[0038]	该声纹特征序列包含一组声纹特征，可以有效地区分不同的说话人，且对同一说话人的变化保持相对稳定。</p>
    <p>[0039]	所述声纹特征主要有：谱包络参数语音特征，基音轮廓、共振峰频率带宽特征，线性预测系数，倒谱系数等。考虑到上述声纹特征的可量化性、训练样本的数量和系统性能的评价等问题，可以选用MFCC (Mel Frequency Cepstrum Coefficient，Mel频率倒谱系数）特征，对窗长25ms帧移IOms的每帧语音数据做短时分析得到MFCC参数及其一阶二阶差分， 共计39维。这样，每句语音信号可以量化为一个39维声纹特征矢量序列X。</p>
    <p>[0040]	步骤103，计算所述声纹特征序列与背景模型的似然度。</p>
    <p>[0041]	帧数为T的声纹特征矢量序列X相应于背景模型（UBM)的似然度为：</p>
    <p>[0042]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00071.png"> <img id="idf0001" file="CN102238190AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00071.png" class="patent-full-image" alt="Figure CN102238190AD00071"> </a> </div>
    <p>[0043]	其中，Cm是第m个高斯的加权系数，满足Σ Cm = 1。μ m以及Σ m分别是第m个高斯</p>
    <p>m=l</p>
    <p>的均值和方差。其中N(.)满足正态分布，用于计算t时刻的声纹特征矢量Xt在单高斯分量上的似然度：</p>
    <p>W、，	�、	1	Oit^m)</p>
    <p>[0044]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00072.png"> <img id="idf0002" file="CN102238190AD00072.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00072.png" class="patent-full-image" alt="Figure CN102238190AD00072"> </a> </div>
    <p>[0045]	步骤104，计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型。</p>
    <p>[0046]	由于说话人模型是根据所述当前登录用户注册时录入的语音信号构建的多混合高斯模型，因此，在该步骤中，计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度时，需要分别计算所述声纹特征序列中各声纹特征与每个混合高斯模型的似然度； 然后根据计算得到的所有似然度确定所述声纹特征与所述当前登录用户的说话人模型的似然度。具体可以有多种实现方式，比如：</p>
    <p>[0047]	1.先分别计算所述声纹特征序列与每个混合高斯模型的似然度，然后再根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0048]	在这种方式中，可以分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；选择所述声纹特征序列中一组声纹特征对应一个混合高斯模型计算得到的似然度总和的时间平均值作为所述声纹特征序列与该混合高斯模型的似然度。</p>
    <p>[0049]	而在得到所述声纹特征序列与每个混合高斯模型的似然度后，可以选择其中的一个最大值或均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0050]	2.先分别计算所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度，然后再根据计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0051]	在这种方式中，可以分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的似然度中的最大值作为该声纹特征与所述多混合高斯模型的似然度；或者，选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的所有似然度的平均值作为该声纹特征与所述多混合高斯模型的似然度。</p>
    <p>[0052]	而在得到所述声纹特征中每个声纹特征与多混合高斯模型的似然度后，选择声纹特征序列的所有声纹特征似然度的总和时间平均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0053]	当然，还可以有其他选择方式，比如对计算得到的所有似然度进行加权平均等，对此本发明实施例不做限定。</p>
    <p>[0054]	步骤105，根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比。</p>
    <p>[0055]	似然比为：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00081.png"> <img id="idf0003" file="CN102238190AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00081.png" class="patent-full-image" alt="Figure CN102238190AD00081"> </a> </div>
    <p>[0056]	其中，p(X|U)为所述声纹特征与说话人模型的似然度，P(XlUBM)为所述声纹特征与背景模型的似然度。</p>
    <p>[0057]	步骤106，判断所述似然比是否大于设定的阈值，如果是，则执行步骤107 ；否则， 执行步骤108。</p>
    <p>[0058]	上述阈值可以由系统预先设定，一般来说，该阈值越大，则系统的灵敏度越高，要求用户在登录时尽可能按照注册时录入的语音信号（即密码）的发音，反之，则系统的灵敏度较低，允许用户登录时录入的语音信号的发音与注册时的发音存在一定的变化。</p>
    <p>[0059]	步骤107，确定所述当前登录用户为有效认证用户。</p>
    <p>[0060]	步骤108，确定所述当前登录用户为非认证用户。</p>
    <p>[0061]	需要说明的是，为了提高系统的鲁棒性，在上述步骤101和步骤102之前，还可以对所述连续语音信号进行降噪处理，比如，首先通过对语音信号的短时能量和短时过零率分析，将连续的语音信号分割成独立的语音片断和非语音片断。然后通过前端降噪处理减少信道噪音及背景噪音的干扰，提高语音信噪比，为后续系统处理提供干净的信号。</p>
    <p>[0062]	用户声纹特征既有相对稳定性，又有变异性。一方面容易受到身体状况、年龄、情绪等的影响，另一方面容易受到外界环境噪音及语音采集信道的干扰，因此说话人模型需要能较好地区别同一说话人的不同声纹变化。在本发明实施例中，说话人模型是根据所述当前登录用户注册时录入的语音信号构建的多混合高斯模型，混合高斯模型数和每一个混合高斯模型的高斯数与用户注册时录入的语音信号的重复次数和该语音信号的帧数相关， 从而可以利用多个混合高斯模型模拟用户对说出同一密码（即上述语音信号）存在的不同发音变化的特点，提高了基于声纹密码进行身份认证的准确率。</p>
    <p>[0063]	在本发明实施例中，背景模型用于描述说话人声纹的共性，该背景模型需要预先构建，具体可以采用现有技术中的一些方式，比如，采用IOM或者更大高斯数的混合高斯模型模拟背景模型，其模型参数训练过程如图2所示。</p>
    <p>[0064]	步骤201，从多说话人训练语音信号中分别提取声纹特征，每个声纹特征作为一个特征矢量。</p>
    <p>[0065]	步骤202，利用聚类算法对上述特征矢量进行聚类，得到K个高斯的初始化均值，K 是预先设置的混合高斯模型个数。</p>
    <p>[0066]	比如，可以采用传统的LBG(Linde，Buzo, Gray)聚类算法，通过训练矢量集和一定的迭代算法来逼近最优的再生码本。</p>
    <p>[0067]	步骤203，利用EM (Expectation Maximization)算法迭代更新上述均值、方差及各高斯对应的加权系数，得到背景模型。</p>
    <p>[0068]	具体的迭代更新过程与现有技术相同，在此不再详细描述。</p>
    <p>[0069]	当然，还可以采用其他方式构建背景模型，对此本发明实施例不做限定。</p>
    <p>[0070]	在本发明实施例中，需要区分用户是处于登录模式还是注册模式，如果是登录模式，则需要按照图1所示的流程对该用户进行基于声纹密码的身份认证，如果是注册模式， 则需要接收所述用户录入的注册语音信号，并根据所述注册语音信号构建所述用户的说话人模型。</p>
    <p>[0071]	本发明实施例中说话人模型的构建过程与传统的说话人模型的构建过程是完全不同的，为了更好地说明这一点，下面首先对传统的说话人模型的构建过程做简单说明。</p>
    <p>[0072]	传统的说话人模型的构建过程是以背景模型为初始模型，通过自适应方法调整模型部分参数，如目前最为常用的基于最大后验概率的自适应算法等。自适应算法根据少量说话人数据将用户声纹共性自适应为当前说话人个性，其具体训练流程如图3所示，包括以下步骤：</p>
    <p>[0073]	步骤301，从用户录入的注册语音信号中提取声纹特征。[0074]	步骤302，利用所述声纹特征自适应更新背景模型混合高斯的均值μω。</p>
    <p>[0075]	具体地，新高斯均值μ m计算为样本统计量和原始高斯均值的加权平均，即：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00101.png"> <img id="idf0004" file="CN102238190AD00101.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00101.png" class="patent-full-image" alt="Figure CN102238190AD00101"> </a> </div>
    <p>[0077]	其中，xt表示第t帧声纹特征，Ym(Xt)表示第t帧声纹特征落于第m个高斯的概率，τ是遗忘因子，用于平衡历史均值以及样本对新均值的更新力度。一般来说，τ值越大，则新均值主要受原始均值制约。而若τ值较小，则新均值主要由样本统计量决定，更多的体现了新样本分布的特点。</p>
    <p>[0078]	步骤303，复制背景模型方差作为所述用户的说话人模型方差。</p>
    <p>[0079]	步骤304，生成所述用户的说话人模型。</p>
    <p>[0080]	在本发明实施例中，需要在用户注册时，接收所述用户录入的注册语音信号，并根据所述注册语音信号构建所述用户的说话人模型。该说话人模型由多个混合高斯模型构成，以模拟说话人对说出同一密码存在的不同发音变化的特点，而且，说明人模型中每个混合高斯模型单独训练方差，以解决传统方法中直接复制背景模型方差导致方差过大，不符合实际应用的问题。</p>
    <p>[0081]	如图4所示，是本发明实施例中构建说话人模型的流程图，包括以下步骤：</p>
    <p>[0082]	步骤401，将用户录音入的注册语音信号保存为一离散能量序列。</p>
    <p>[0083]	假设用户注册输入同一密码内容Ν(比如N = 2、3等）次，则得到N个独立的离散能量序列。</p>
    <p>[0084]	步骤402，从得到的离散能量序列中提取声纹特征。</p>
    <p>[0085]	具体过程与前面的步骤102类似，在此不再详细描述。</p>
    <p>[0086]	步骤403，根据所述注册语音信号的重复次数及帧数确定所述用户的说话人模型的所有混合高斯模型。</p>
    <p>[0087]	在声纹密码应用中，用户输入统一的文本内容作为密码使用。比如，可以设定所述用户的说话人模型的混合高斯模型数等于所述注册语音信号的重复次数，并设定对应于每一个混合高斯模型的高斯数等于所述混合高斯模型对应的注册语音信号的帧数，具体可表示为：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00102.png"> <img id="idf0005" file="CN102238190AD00102.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00102.png" class="patent-full-image" alt="Figure CN102238190AD00102"> </a> </div>
    <p>[0089]	其中，T(k)是混合高斯模型Mk的高斯数，等同于模型对应的第k个语音样本的帧数。而/^,Σ=分别是混合高斯模型Mk的第m个高斯分量的加权系数、均值及方差。</p>
    <p>[0090]	当然，本发明实施例并不限定上述说话人模型的拓扑方式，其混合高斯模型数及每一个混合高斯模型的高斯数也可以不与所述语音信号的重复次数及帧数完全对应相等， 也可以通过采用聚类算法选取混合高斯模型数小于所述注册语音信号的重复次数，同样， 每一个混合高斯模型的高斯数也可以小于所述注册语音信号的帧数。</p>
    <p>[0091]	步骤404，根据提取的声纹特征估计所有混合高斯模型的高斯均值参数。</p>
    <p>[0092]	在本发明实施例中，根据单一训练样本确定其对应混合高斯模型的高斯均值参量。具体地，可以将混合高斯模型的每个高斯均值矢量设置为样本的特征矢量值，即 μ&#943; = Okm。其中//二表示第k个混合模型的第m个高斯的均值，而O=表示第k个语音信号的第 m帧语音的声纹特征矢量。</p>
    <p>[0093]	步骤405，根据提取的声纹特征估计所有混合高斯模型的高斯方差参数。</p>
    <p>[0094]	可以假设说话人模型中每个混合高斯的多个高斯具有为全局统一的矩阵，以实现较少数据上的方差重估问题。在该假设下，Σ= =Tk (即第k个混合高斯模型的所有高斯分量的协方差矩阵具有相同的矩阵数值）。具体地，对给定的样本声纹特征序列0k，根据 O =丨O1,02,...,OaJ ,即所有剩余样本声纹特征序列的统计信息，重估混合高斯模型Mk的方差，计算如下：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00111.png"> <img id="idf0006" file="CN102238190AD00111.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00111.png" class="patent-full-image" alt="Figure CN102238190AD00111"> </a> </div>
    <p>[0096]	其中，Of表示第η句注册密码（即注册语音信号）的第i个语音帧（即样本），&lt; 表示第k个混合高斯模型的第m个高斯均值，&lt;(。")表示样本落在均值为//二的高斯上的概率。</p>
    <p>[0097]	这样，对说话人模型的每一单独混合高斯Mk，都可以利用非Ok的样本数据得到对应的方差参量。若注册语音信号为N句，则得到N个不同的方差矩阵。</p>
    <p>[0098]	特别地，可以假设该方差矩阵为对角阵以进一步减少数据稀疏问题，即 K =^=diag(^)。另外还可以进一步考虑说话人模型的多个混合高斯模型的多个高斯的方差具有全局统一的对角阵，以更好地解决数据稀疏情况下的模型方差重估问题。在该假设下，Σ二 = Σ = diag{Y) &#8222;</p>
    <p>[0099]	步骤406，估计所有混合高斯模型的高斯加权系数参数。</p>
    <p>[0100]	考虑到本实施例中混合高斯模型的高斯均值由样本矢量直接确定，因而在样本上每个高斯都是以1的概率存在的，即出现概率相同。为此本实施例中可以设置混合模型中每个高斯的加权系数均等，即：</p>
    <p>[0101]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00112.png"> <img id="idf0007" file="CN102238190AD00112.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102238190A/CN102238190AD00112.png" class="patent-full-image" alt="Figure CN102238190AD00112"> </a> </div>
    <p>(7)</p>
    <p>[0102]	利用上述图4所示流程，可以根据注册语音的句数及句长设置说话人模型中的混合高斯模型的个数并确定模型的拓扑结构，通过对所有混合高斯模型的高斯均值、方差以及加权系数的合理设定，有效地解决了传统基于声纹密码认证的系统中存在的数据稀疏的训练问题，提高了混合高斯模型之间的区分性，进而可以提高身份认证的准确率。而且，使用的混合高斯模型更小更有效，相对于现有技术，大大改善了运算速率及储存数据所需的内存压力。</p>
    <p>[0103]	相应地，本发明实施例还提供一种身份认证系统，如图5所示，是本发明实施例身份认证系统的一种结构示意图。</p>
    <p>[0104]	在该实施例中，所述系统包括：[0105]	语音信号接收单元501，用于在用户登录时，接收当前登录用户录入的连续语音信号；</p>
    <p>[0106]	提取单元502，用于提取所述连续语音信号中的声纹特征序列；</p>
    <p>[0107]	第一计算单元503，用于计算所述声纹特征序列与背景模型的似然度；</p>
    <p>[0108]	第二计算单元504，用于计算所述声纹特征序列与所述当前登录用户的说话人模型的似然度，所述说话人模型是根据所述当前登录用户注册时录入的注册语音信号的重复次数及帧数构建的多混合高斯模型；</p>
    <p>[0109]	第三计算单元505，用于根据所述声纹特征序列与说话人模型的似然度、以及所述声纹特征序列与背景模型的似然度，计算似然比；</p>
    <p>[0110]	判断单元506，用于在所述第三计算单元505计算得到的似然比大于设定的阈值时，确定所述当前登录用户为有效认证用户，否则确定所述当前登录用户为非认证用户。</p>
    <p>[0111]	上述该声纹特征序列包含一组声纹特征，可以有效地区分不同的说话人，且对同一说话人的变化保持相对稳定。</p>
    <p>[0112]	比如，提取单元502可以提取的声纹特征主要有：谱包络参数语音特征，基音轮廓、共振峰频率带宽特征，线性预测系数，倒谱系数等。考虑到上述声纹特征的可量化性、训练样本的数量和系统性能的评价等问题，可以选用MFCC(Mel Frequency Cepstrum Coefficient,Mel频率倒谱系数）特征，对窗长25ms帧移IOms的每帧语音数据做短时分析得到MFCC参数及其一阶二阶差分，共计39维。这样，每句语音信号可以量化为一个39维声纹特征序列X。</p>
    <p>[0113]	上述背景模型可以是系统预先构建并在初始化时载入的，背景模型的具体构建过程本发明实施例不做限定。</p>
    <p>[0114]	上述说话人模型是根据所述当前登录用户注册时录入的语音信号构建的多混合高斯模型，相应地，在本发明实施例中，上述第二计算单元504可以有多种实现方式，比如：</p>
    <p>[0115]	在一种实现方式中，所述第二计算单元504包括：第一计算子单元和第一确定子单元。其中：</p>
    <p>[0116]	所述第一计算子单元，用于分别计算所述声纹特征序列与每个混合高斯模型的似然度；</p>
    <p>[0117]	所述第一确定子单元，用于根据所述第一计算子单元的计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0118]	上述第一计算子单元可以包括：第一计算模块和第一选择模块，其中：</p>
    <p>[0119]	所述第一计算模块，用于分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；</p>
    <p>[0120]	所述第一选择模块，用于选择所述声纹特征序列中一组声纹特征对应一个混合高斯模型计算得到的似然度总和的时间平均值作为所述声纹特征序列与该混合高斯模型的似然度。</p>
    <p>[0121]	相应地，上述第一确定子单元也可以有多种实现方式，比如，在第一计算子单元得到所述声纹特征序列与每个混合高斯模型的似然度后，第一确定子单元可以选择其中的一个最大值或均值作为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0122]	在另一种实现方式中，所述第二计算单元504包括：第二计算子单元和第二确定子单元。其中：</p>
    <p>[0123]	所述第二计算子单元，用于分别计算所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度；</p>
    <p>[0124]	所述第二选择子单元，用于根据所述第二计算子单元的计算结果确定所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0125]	上述第二计算子单元可以包括：第二计算模块和第二选择模块，其中：</p>
    <p>[0126]	所述第二计算模块，用于分别计算所述声纹特征序列中每个声纹特征与所述多混合高斯模型中每个混合高斯模型的似然度；</p>
    <p>[0127]	所述第二选择模块，用于选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的似然度中的最大值作为该声纹特征与所述多混合高斯模型的似然度；或者选择所述声纹特征序列中一个声纹特征对应所述多混合高斯模型中每个混合高斯模型计算得到的所有似然度的平均值作为该声纹特征与所述多混合高斯模型的似然度。</p>
    <p>[0128]	相应地，上述第二确定子单元也可以有多种实现方式，比如，在第二计算子单元得到所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度后，第二确定子单元可以选择所述声纹特征序列中每个声纹特征相对于所述多混合高斯模型的似然度的时间平均值为所述声纹特征序列与所述当前登录用户的说话人模型的似然度。</p>
    <p>[0129]	当然，第二计算单元504还可以采用其他方式实现，对此本发明实施例不做限定。</p>
    <p>[0130]	上述第一计算单元503、第二计算单元504和第三计算单元505的具体计算过程可参照前面本发明实施例身份认证方法中的描述，在此不再赘述。</p>
    <p>[0131]	在本发明实施例中，说话人模型是根据所述当前登录用户注册时录入的语音信号构建的多混合高斯模型，混合高斯模型数和每一个混合高斯模型的高斯数与用户注册时录入的语音信号的重复次数和该语音信号的帧数相关，从而可以利用多个混合高斯模型模拟用户对说出同一密码（即上述语音信号）存在的不同发音变化的特点，提高了基于声纹密码进行身份认证的准确率。</p>
    <p>[0132]	如图6所示，是本发明实施例身份认证系统的另一种结构示意图。</p>
    <p>[0133]	与图5所示实施例不同的是，在该实施例中，所述语音信号接收单元501还用于在用户注册时，接收所述用户录入的注册语音信号。</p>
    <p>[0134]	另外，在该系统中还进一步包括：模型构建单元601，用于根据所述注册语音信号构建所述用户的说话人模型，该模型构建单元601包括：</p>
    <p>[0135]	特征提取子单元611，用于从所述注册语音信号中提取声纹特征；</p>
    <p>[0136]	拓扑结构确定子单元612，用于根据所述注册语音信号的重复次数及帧数确定所述用户的说话人模型的所有混合高斯模型；</p>
    <p>[0137]	比如，可以设定所述用户的说话人模型的混合高斯模型数小于或等于所述注册语音信号的重复次数；设定对应于每一个混合高斯模型的高斯数小于或等于所述注册语音信号的帧数；</p>
    <p>[0138]	第一估计子单元613，用于利用特征提取子单元611提取的声纹特征估计所述拓扑结构确定子单元612确定的所有混合高斯模型的高斯均值参数；</p>
    <p>[0139]	第二估计子单元614，用于利用特征提取子单元611提取的声纹特征估计所述拓扑结构确定子单元612确定的所有混合高斯模型的高斯方差参数。</p>
    <p>[0140]	上述各估计子单元对混合高斯模型中的相应参数的估计方法可参照前面的描述， 在此不再赘述。</p>
    <p>[0141]	本发明实施例的身份认证系统，可以根据注册语音的句数及句长设置说话人模型中的混合高斯模型的个数并确定模型的拓扑结构，通过对所有混合高斯模型的高斯均值、 方差以及加权系数的合理设定，有效地解决了传统基于声纹密码认证的系统中存在的数据稀疏的训练问题，提高了混合高斯模型之间的区分性，进而可以提高身份认证的准确率。而且，使用的混合高斯模型更小更有效，相对于现有技术，大大改善了运算速率及储存数据所需的内存压力。</p>
    <p>[0142]	本说明书中的各个实施例均采用递进的方式描述，各个实施例之间相同相似的部分互相参见即可，每个实施例重点说明的都是与其他实施例的不同之处。尤其，对于系统实施例而言，由于其基本相似于方法实施例，所以描述得比较简单，相关之处参见方法实施例的部分说明即可。以上所描述的系统实施例仅仅是示意性的，其中所述作为分离部件说明的单元及模块可以是或者也可以不是物理上分开的。另外，还可以根据实际的需要选择其中的部分或者全部单元和模块来实现本实施例方案的目的。本领域普通技术人员在不付出创造性劳动的情况下，即可以理解并实施。</p>
    <p>[0143]	以上公开的仅为本发明的优选实施方式，但本发明并非局限于此，任何本领域的技术人员能思之的没有创造性的变化，以及在不脱离本发明原理前提下所作的若干改进和润饰，都应落在本发明的保护范围内。</p>
    <p>14</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101833951A?cl=zh">CN101833951A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年3月4日</td><td class="patent-data-table-td patent-date-value">2010年9月15日</td><td class="patent-data-table-td ">清华大学</td><td class="patent-data-table-td ">用于说话人识别的多背景模型建立方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102024455A?cl=zh">CN102024455A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年9月10日</td><td class="patent-data-table-td patent-date-value">2011年4月20日</td><td class="patent-data-table-td ">索尼株式会社</td><td class="patent-data-table-td ">说话人识别系统及其方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060111905">US20060111905</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2004年11月22日</td><td class="patent-data-table-td patent-date-value">2006年5月25日</td><td class="patent-data-table-td ">Jiri Navratil</td><td class="patent-data-table-td ">Method and apparatus for training a text independent speaker recognition system using speech data with text labels</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080059156">US20080059156</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年7月20日</td><td class="patent-data-table-td patent-date-value">2008年3月6日</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for processing speech data</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102510426A?cl=zh">CN102510426A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月29日</td><td class="patent-data-table-td patent-date-value">2012年6月20日</td><td class="patent-data-table-td ">安徽科大讯飞信息科技股份有限公司</td><td class="patent-data-table-td ">个人助理应用访问方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102710602A?cl=zh">CN102710602A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月28日</td><td class="patent-data-table-td patent-date-value">2012年10月3日</td><td class="patent-data-table-td ">深圳创维－Rgb电子有限公司</td><td class="patent-data-table-td ">一种电子设备语音登录方法、系统及电视机</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102968990A?cl=zh">CN102968990A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月15日</td><td class="patent-data-table-td patent-date-value">2013年3月13日</td><td class="patent-data-table-td ">江苏嘉利德电子科技有限公司</td><td class="patent-data-table-td ">说话人识别方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102968990B?cl=zh">CN102968990B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月15日</td><td class="patent-data-table-td patent-date-value">2015年4月15日</td><td class="patent-data-table-td ">朱东来</td><td class="patent-data-table-td ">说话人识别方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103226951B?cl=zh">CN103226951B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年4月19日</td><td class="patent-data-table-td patent-date-value">2015年5月6日</td><td class="patent-data-table-td ">清华大学</td><td class="patent-data-table-td ">基于模型顺序自适应技术的说话人确认系统创建方法</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G10L0017000000">G10L17/00</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=H04L0009320000">H04L9/32</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=z8GaBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=H04L0029060000">H04L29/06</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2011年11月9日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2011年12月21日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年12月11日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年4月1日</td><td class="patent-data-table-td ">C56</td><td class="patent-data-table-td ">Change in the name or address of the patentee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">IFLYTEK CO., LTD.</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">FORMER NAME: ANHUI USTC IFLYTEK CO., LTD.</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/b5011886f1217d14543a/CN102238190A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81%E6%96%B9%E6%B3%95%E5%8F%8A%E7%B3%BB%E7%BB%9F.pdf?id=z8GaBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U3JjPszgR4YBjLAWo172FT-_Oh2Aw"},"sample_url":"https://www.google.com/patents/reader?id=z8GaBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>