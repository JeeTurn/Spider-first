<!DOCTYPE html><html><head><title>专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于AdaBoost的三维模型自动分类方法"><meta name="DC.contributor" content="刘贞报" scheme="inventor"><meta name="DC.contributor" content="张凤" scheme="inventor"><meta name="DC.contributor" content="布树辉" scheme="inventor"><meta name="DC.contributor" content="西北工业大学" scheme="assignee"><meta name="DC.date" content="2012-9-24" scheme="dateSubmitted"><meta name="DC.description" content="本发明提供了一种基于AdaBoost的三维模型自动分类方法，计算三维模型中任意两顶点的近似测地距离，根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，采用近似的方法模拟包含所有顶点关系的仿射矩阵，用Jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，取第2个至第21个共20个特征值作为三维模型的描述符，使用AdaBoost方法对三维模型进行分类。本发明可以实现三维模型的自动特征提取并利用特征进行三维模型的自动分类，和现有分类方法相比较具有分类精度高，适用范围广的特点。"><meta name="DC.date" content="2013-2-6"><meta name="DC.relation" content="CN:101398886:A" scheme="references"><meta name="DC.relation" content="CN:102622609:A" scheme="references"><meta name="DC.relation" content="US:20090319454:A1" scheme="references"><meta name="citation_reference" content="刘小明等: &quot;基于适应加权非对称AdaBoost HMM的三维模型分类算法&quot;, 《浙江大学学报（工学版）》, no. 408, 31 August 2006 (2006-08-31)"><meta name="citation_reference" content="孙晓鹏: &quot;三维模型的分割及应用研究&quot;, 《中国博士学位论文全文数据库》信息科技辑（月刊）》, no. 12, 31 December 2005 (2005-12-31)"><meta name="citation_patent_publication_number" content="CN:102915448:A"><meta name="citation_patent_application_number" content="CN:201210358777"><link rel="canonical" href="https://www.google.com/patents/CN102915448A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102915448A?cl=zh"/><meta name="title" content="专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法"/><meta name="description" content="本发明提供了一种基于AdaBoost的三维模型自动分类方法，计算三维模型中任意两顶点的近似测地距离，根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，采用近似的方法模拟包含所有顶点关系的仿射矩阵，用Jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，取第2个至第21个共20个特征值作为三维模型的描述符，使用AdaBoost方法对三维模型进行分类。本发明可以实现三维模型的自动特征提取并利用特征进行三维模型的自动分类，和现有分类方法相比较具有分类精度高，适用范围广的特点。"/><meta property="og:title" content="专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102915448A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102915448A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=e3bBBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102915448A&amp;usg=AFQjCNHnBxICMBEiRC3POX5iH0uYwIQxGQ" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/bc7c704f818f9b9c96a2/CN102915448A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/bc7c704f818f9b9c96a2/CN102915448A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102915448A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102915448A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102915448A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102915448B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102915448A?cl=zh" style="display:none"><span itemprop="description">本发明提供了一种基于AdaBoost的三维模型自动分类方法，计算三维模型中任意两顶点的近似测地距离，根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，采用近似的方法模拟包含所有顶点关系的 ...</span><span itemprop="url">https://www.google.com/patents/CN102915448A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法" title="专利 CN102915448A - 一种基于AdaBoost的三维模型自动分类方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102915448 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210358777</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年2月6日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年9月24日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年9月24日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102915448B?hl=zh-CN&amp;cl=zh">CN102915448B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210358777.6, </span><span class="patent-bibdata-value">CN 102915448 A, </span><span class="patent-bibdata-value">CN 102915448A, </span><span class="patent-bibdata-value">CN 201210358777, </span><span class="patent-bibdata-value">CN-A-102915448, </span><span class="patent-bibdata-value">CN102915448 A, </span><span class="patent-bibdata-value">CN102915448A, </span><span class="patent-bibdata-value">CN201210358777, </span><span class="patent-bibdata-value">CN201210358777.6</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%88%98%E8%B4%9E%E6%8A%A5%22">刘贞报</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%BC%A0%E5%87%A4%22">张凤</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%B8%83%E6%A0%91%E8%BE%89%22">布树辉</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E8%A5%BF%E5%8C%97%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%22">西北工业大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102915448A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102915448A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102915448A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#npl-citations">非专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (1),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=e3bBBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210358777&amp;usg=AFQjCNFJNzkegdPvLLZNlwX76Z_ibRXlSQ"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=e3bBBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102915448A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHQCBPF7OtSQ_jpcvk9x50UhLrlPA"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT120603559" lang="ZH" load-source="patent-office">一种基于AdaBoost的三维模型自动分类方法</invention-title>
      </span><br><span class="patent-number">CN 102915448 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA106347698" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明提供了一种基于AdaBoost的三维模型自动分类方法，计算三维模型中任意两顶点的近似测地距离，根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，采用近似的方法模拟包含所有顶点关系的仿射矩阵，用Jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，取第2个至第21个共20个特征值作为三维模型的描述符，使用AdaBoost方法对三维模型进行分类。本发明可以实现三维模型的自动特征提取并利用特征进行三维模型的自动分类，和现有分类方法相比较具有分类精度高，适用范围广的特点。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(1)</span></span></div><div class="patent-text"><div mxw-id="PCLM50955576" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种基于AdaBoost的三维模型自动分类方法，其特征在于包括下述步骤：(1)计算三维模型中任意两顶点的近似测地距离，其中，任意相邻两顶点采用欧氏距离作为它们的近似测地距离，任意不相邻两顶点的近似测地距离采用Dikstra算法计算；(2)根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，仿射矩阵的行数和列数都是三维模型的顶点数，仿射矩阵中任一元素指的是以该元素所在的行和列为顶点索引号的两个顶点的近似测地距离进行高斯化后的值；（3)采用Nysti^m近似的方法模拟包含所有顶点关系的仿射矩阵，用jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，取第2个至第21个共20个特征值作为三维模型的描述符；(4)根据步骤（3)中得到的三维模型的描述符，使用AdaBoost方法对三维模型进行分类。2</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES58010148" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于AdaBoost的三维模型自动分类方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及一种三维模型的自动分类方法。</p>
    <p>背景技术</p>
    <p>[0002]	作为继声音、图像和视频之后的第四代多媒体数据类型，三维模型是最直观、最具表现力的多媒体信息。随着激光扫描技术及网络技术的快速发展，三维模型的创建和应用越来越广泛，三维模型资源越来越丰富。企业产品类型及品种的增多、产品数据规模的膨胀，使得产品设计中三维模型的分类研究具有重要的理论与工程意义。而基于形状的三维模型分类作为计算机图形学领域的一个新兴研究热点，在工业产品的模型设计、虚拟现实、模拟仿真、3D游戏、计算机视觉、分子生物学和三维地理信息等各个领域获得了广泛的应用。</p>
    <p>[0003]在目前国内外公开的文献中，在	Z. Barutcuoglu and C. Decoro, “Hierarchicalshape classification using Bayesian aggregation，，，IEEE International Conferenceon Shape Modeling and Applications, 2006.中提出了基于 Bayesian aggregation 的分类方法，对语义层次结构中的三维模型进行分类。在层次结构模型中，使用相对独立的分类器对每一类进行分类，产生的分类结果将会与层次结构中“父-子”关系发生分歧。为了保持一致，一个举例图形必须不能被分为一类，除非这个图形已经在层次结构中被分为“父”类。在给定的用于一个任意形状描述符的一些独立的分类器的情况下，把它们每一个的很可能不一致的分类结果结合起来，然后在贝叶斯的框架下获得一组最具有一致性的分类结果。这样的错误改正可以通过利用分层结构来提高整个分类结果的精度° 在 Z. Liu, J. Mitani, Y. Fukui and S. Nishihara, “A 3D shape classifier withneural network supervision，，，International Journal of Computer Applications inTechnology, Vol. 38, No. 1-3, 2010.中提出了基于监督型神经网络的三维模型分类方法，该方法该提供了一种基于监督点空间的密度分布的三维图形分类器。首先通过特征化点空间的密度分布提取出低阶的特征样本，然后训练一个前馈控制的神经网络来学习这些特征，从而获得一个有效的分类器。此分类器分为两个阶段，分别为用于训练数据的训练阶段和评估分类效果的测试阶段。而需要注意的是分类器的精度不仅和每个样本的权重相关，而且和神经网络中的隐藏阶层的隐藏单元个数息息相关。隐藏单元个数不同，分类精度也会有很大差别。因此在训练分类器时，选择最恰当的隐藏单元个数是非常有必要的。</p>
    <p>[0004]	但上述两种三维模型分类方法有几点不足：</p>
    <p>[0005]	(I)基于Bayesian aggregation的三维模型分类方法主要是针对属于层次结构中的三维模型进行分类，具有一定的局限性，适用范围较小；</p>
    <p>[0006]	(2)基于神经网络的三维模型分类方法分类精度较低，分类的正确率较低。</p>
    <p>发明内容</p>
    <p>[0007]	为了克服现有技术分类范围局限和精度较低的不足，本发明提供的一种三维模型</p>
    <p>3自动分类方法，可以对Halfedge结构的三维模型或CAD模型进行自动特征提取与分类，可以对同一模型的不同姿态或大小进行提取出基本相同的特征，然后将模型的特征进行训练和测试，得到了高精度的分类结果。</p>
    <p>[0008]	本发明解决其技术问题所采用的技术方案包括以下步骤：</p>
    <p>[0009]	(I)计算三维模型中任意两顶点的近似测地距离，其中，任意相邻两顶点采用欧氏距离作为它们的近似测地距离，任意不相邻两顶点的近似测地距离采用Dikstra算法计算；</p>
    <p>[0010]	(2)根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，仿射矩阵的行数和列数都是三维模型的顶点数，仿射矩阵中任一元素指的是以该元素所在的行和列为顶点索引号的两个顶点的近似测地距离进行高斯化后的值；</p>
    <p>[0011]	(3)采用NystWm近似的方法模拟包含所有顶点关系的仿射矩阵，用Jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，取第2个至第21个共20个特征值作为三维模型的描述符；</p>
    <p>[0012]	(4)根据步骤（3)中得到的三维模型的描述符，使用AdaBoost方法对三维模型进行分类。</p>
    <p>[0013]	本发明的有益效果是：</p>
    <p>[0014]	本发明实现了一种三维模型的分类方式，该方法可以实现三维模型的自动特征提取并利用特征进行三维模型的自动分类，和现有分类方法相比较具有分类精度高，适用范围广的特点。</p>
    <p>[0015]	本发明之所以具有上述的有益效果其原因在于：针对任意三维模型采用谱嵌入的方法得到一种三维模型的描述符。该谱嵌入是基于以近似测地线距离为基础构造出仿射矩阵的特征值，这一过程称为姿态无关变换，即可获得一种对三维物体的刚性变换、均匀缩放、姿态变换(如弯曲等）保持不变的标准化谱嵌入。谱嵌入得到的特征值即为三维模型的描述符，将通过上述谱嵌入方法获得的三维形状描述符作为分类特征，利用Adaboost算法对三维模型进行分类。AdaBoost算法针对不同的训练集训练同一个基本分类器(弱分类器)，然后把这些在不同训练集上得到的弱分类器聚合，构成一个更强的最终的分类器(强分类器)。因此该强分类器具有最小的误差率，使用该分类器进行三维模型分类可以获得很高的分类精度。</p>
    <p>附图说明</p>
    <p>[0016]	图I为该发明实现的总流程图；</p>
    <p>[0017]	图2为三维模型特征提取的流程图；</p>
    <p>[0018]	图3为AdaBoost方法训练分类器的流程图；</p>
    <p>[0019]	图4为AdaBoost方法测试分类器的流程图；</p>
    <p>[0020]	图5为实例的分类精度的统计直方图。</p>
    <p>具体实施方式</p>
    <p>[0021]	本发明包括以下步骤：</p>
    <p>[0022]	(I)计算三维模型中任意两顶点的近似测地距离。三维模型由许多顶点构成，任意相邻两顶点采用欧氏距离作为它们的近似测地距离，任意不相邻两顶点的近似测地距离采用Dikstra算法计算。</p>
    <p>[0023]	(2)根据计算出的三维模型任意两个顶点的近似测地距离组成一个三维模型的仿射矩阵，仿射矩阵的行数和列数都是三维模型的顶点数，仿射矩阵中任一元素指的是以该元素所在的行和列为顶点索引号的两个顶点的近似测地距离进行高斯化后的值。</p>
    <p>[0024]	(3)采用NysWtoi近似的方法来有效地模拟包含所有顶点关系的仿射矩阵的特征值和特征向量。通过该方法可以提高计算效率，节省时间。NysWim近似是一种基于子采样的技术。在三维模型的所有顶点中选用最远点采样的方式进行采样，即每次采样时都选取到之前的采样点的近似测地距离最大的点。这样做可以尽量使采样点分布在模型尖端的尖点上，而且使最终的近似最大限度地接近原来的值，从而达到模拟的效果。利用采样点之间的近似测定距离建立一个包含所有采样点之间关系的仿射矩阵，最后用Jacobi方法特征分解该仿射矩阵，将得到的特征值按从大到小的顺序排列，由于通过采样仿射矩阵特征分解求得的特征值具有一个特性就是衰减得很快，且第一个特征值偏差较大，故取第2个至第21个共20个特征值作为三维模型的描述符。</p>
    <p>[0025]	(4)使用AdaBoost方法对三维模型进行分类。将步骤（3)中得到的三维模型的特征值作为样本进行分类。将每一类模型的特征值等分为两部分，一部分作为训练集样本，另一部分作为测试集样本。训练集是一个包含所有需要分类的模型的特征值和类别标签的集合，通过在训练集上运行一段机器学习程序，即可得到一个分类器，该分类器即为运行过后的该段机器学习程序。而测试集也是一个包含不同类模型的特征值和类别标签的集合，它是用来测试分类器的分类效果。在AdaBoost方法中，每一个训练样本都被赋予一个权重，表明它被某个分量分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就降低；相反，如果某个样本点没有被正确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能够聚焦于那些分类较困难的样本上。在具体实现上，最初令每个样本的权重都相等。对于第t次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器ht。然后就根据这个分类器，来提高被它错分的那些样本点的权重，并降低可以被正确分类的样本权。然后，权重更新过的样本集被用来训练下一个分类器ht+1。整个训练过程如此进行下去。本发明训练和测试时，都是把某一类形状的特征值和剩余其它类形状的特征值分别贴上两类不同的标签进行二分类，然后循环多次直至所有类别分类完成。训练时，对训练集进行连续多次迭代分类直至分类误差为最小，就获得了一个最终的强分类器。测试时，使用该强分类器对测试集进行测试分类，分类结果即为形状的标签，如果标签与被测试形状的标签一致即为正确分类，反之则为错误分类。分析分类结果，即统计每一类中正确分类的模型数和错误分类的模型数，然后计算每一类正确的模型数占总模型数的百分比，该百分比就是分类精度。</p>
    <p>[0026]	下面结合附图和实施例对本发明进一步说明。</p>
    <p>[0027]	首先附图I展示了本发明实现三维模型自动特征提取与分类的总流程，该总流程图包含了实现最终分类所需的各个主要步骤。本发明的目的是通过提取三维模型的特征进行自动分类，分类器通过程序来实现，对于读入提供的三维模型的文件，提取相应的三维模型测地线特征，然后利用Adaboost的学习方法进行训练，获取一个自动分类器。</p>
    <p>[0028]	下面是具体的实现步骤。</p>
    <p>5[0029]	一、三维模型任意两个顶点的测地距离的计算</p>
    <p>[0030]	三维模型由网格进行表达，网格由有顶点、边、多边形构成。其中，本发明通过特征化顶点距离来实现的，因此任意相邻两顶点之间的距离采用近似测地距离来表达，公式的 形式如下</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00061.png"> <img id="idf0001" file="CN102915448AD00061.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00061.png" class="patent-full-image" alt="Figure CN102915448AD00061"> </a> </div>
    <p>[0032]	任意不相邻两顶点的近似测地距离采用Dkstra算法来近似计算。</p>
    <p>[0033]	Dijkstra算法的基本思路是：假设每个点都有一对标号（d」，Pj)，其中(Ij是从起源点s到点j的最短路径的长度（从顶点到其本身的最短路径是零路（没有弧的路），其长度等于零）％则是从S到j的最短路径中j点的前一点。求解从起源点S到点j的最短路径算法的基本过程如下：</p>
    <p>[0034]	I)初始化。起源点设置为：①ds=0, Ps为空；②所有其他点：屯=Pi=?;③标记起源点s,记k=s,其他所有点设为未标记的。</p>
    <p>[0035]	2)检验从所有已标记的点k到其直接连接的未标记的点j的距离，并设置：</p>
    <p>[0036]	dj=min [dj, dk+lkJ]	(1-2)</p>
    <p>[0037]	式中，Ikj是从点k到j的直接连接距离。</p>
    <p>[0038]	3)选取下一个点。从所有未标记的结点中，选取4中最小的一个i :</p>
    <p>[0039]	d^min [d」，所有未标记的点j] (1-3)</p>
    <p>[0040]	点i就被选为最短路径中的一点，并设为已标记的。</p>
    <p>[0041]	4)找到点i的前一点。从已标记的点中找到直接连接到点i的点j%作为前一点，</p>
    <p>设置：</p>
    <p>[0042]	i =	(1-4)</p>
    <p>[0043]	5)标记点i。如果所有点已标记，则算法完全推出，否则，记k=i，转到2)再继续。</p>
    <p>[0044]	通过上述算法我们可以求得三维模型中任意两顶点之间的近似测地距离。</p>
    <p>[0045]	二、三维模型仿射矩阵的建立</p>
    <p>[0046]	三维模型仿射矩阵中任一元素指的是以该元素所在的行和列为顶点索引号的两个顶点近似测地距离高斯化后的值。定义矩阵A的仿射关系为高斯仿射：</p>
    <p>[0047]	Aij = exp (_屯//2 σ 2) (2&#8212;1)</p>
    <p>[0048]	其中Clij为三维网格上顶点i和顶点j之间的近似测地距离，σ是高斯宽度。可以从定义中看到，两个点之间的仿射关系是和它们之间的近似测地距离方向相关的。而使用高斯仿射，可以有效地减少近似测地距离很远的顶点对当前顶点的影响。这里，我们将σ定义为：</p>
    <p>[0049]	ο = max (i, j) ((IijI	(2-2)</p>
    <p>[0050]	即所有顶点之间的近似测地距离的最大值。将σ定义为这种数据相关的形式可以使最后的嵌入结果做到对均匀缩放不变，因为实际上可以将这一定义看作是一个标准化的过程。通过对大量的实验结果的观察可以得知，只要σ足够大，最后的嵌入关于σ是相对稳定的。</p>
    <p>[0051 ] 三、Nystr0m算法近似三维模型的特征值</p>
    <p>[0052]	对一个具有η个顶点的三维模型，如果要将其仿射矩阵A完全构造出来，其时间复杂度是0(n2logn)。而且，对一个nXn的矩阵进行特征分解的时间复杂度是0(n3)。为了提高效率使其满足在线处理的要求，我们使用Nystrttai近似的方法来有效地模拟矩阵A的特征值和特征向量。Nystrdm近似是一种基于子采样技术，它可以将本文中用到的仿射矩阵的构造及其特征分解的时间复杂度降低到0(nlogn+l3)，其中I是选取的样本数，通常I&lt;&lt; n，实际的时间复杂度可以认为是O(nlogn)。如图2所示为Nystr0m近似应用的例子。</p>
    <p>[0053]	NystrSm近似的基本思想是，将子矩阵特征分解得到的特征向量和特征值用到整个矩阵的特征向量近似中。考虑一个有η个点的集合Z = X U Y。其中X和Y的大小分别是I和m，并且= 0 ,这样X和Y就形成了 Z的一个划分。将对称的仿射矩阵W e Rnxn写成分块的形式：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00071.png"> <img id="idf0002" file="CN102915448AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00071.png" class="patent-full-image" alt="Figure CN102915448AD00071"> </a> </div>
    <p>[0055]	其中，A e Rlxl和C e Rmxm分别是集合X和Y所对应的仿射矩阵，B e Rlxm是X中的点和Y中的点的仿射关系。令X为采样点的集合，则其所对应的仿射矩阵A可以特征分解为A = UAUt，那么W的特征向量就可以使用Nystrdm近似的方法近似为：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00072.png"> <img id="idf0003" file="CN102915448AD00072.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00072.png" class="patent-full-image" alt="Figure CN102915448AD00072"> </a> </div>
    <p>[0057]	即我们只需构造一个子块[Α B]就可以近似出整个矩阵W的特征向量了。</p>
    <p>[0058]	本发明中我们使用的是仿射矩阵的特征值作为样本进行分类，因此只需在原有的顶点中进行采样，本发明选取了 50个采样点获得一个近似的仿射矩阵。该近似仿射矩阵采用Jacobi方法特征分解，将求得的特征值按从大到小的顺序排列。</p>
    <p>[0059]	通过上述三个步骤我们就可以得到三维模型的特征值，即该三维模型的描述符，具体流程图如附图2所示。</p>
    <p>[0060]	四、AdaBoost方法的三维模型特征值分类</p>
    <p>[0061]	AdaBoost 是 “adaptive boosting” (自适应增强）的缩写。“boosting 法”（增强法）的目标是提高任何给定的学习学习算法的分类准确率。在boosting法中，我们首先根据已有的训练样本集设计一个分类器，要求这个分类器的准确率比平均性能要好。然后依次顺序地加入多个分量分类器系统，最后形成一个总体分类器，它对训练样本的准确率能够任意的提高。在这种情况下，我们说，分类准确率被增强了。概括的说，本方法依次训练一组分量分类器，其中每个分量分类器的训练集都选择自己有的其它各个分类器所给出的“最富信息”（most informative)的样本点组成。而最终的判决结果则是根据这些分量分类器的结果共同决定。boosting方法也可以被递归的使用，即对分量分类器本身也进行boosting。用这种方式，可以获得非常小的分类误差率。甚至，在类别之间可分的情况下可以达到零误差。</p>
    <p>[0062]	AdaBoost方法是基本boosting方法的一个变形,它允许设计者不断地加入新的“弱分类器”，直到达到某个预定的足够小的误差率，对于许多实际的应用，AdaBoost方法确实被证明是非常有效的。本发明主要是利用AdaBoost算法对提取的三维图像的特征值进行分类。</p>
    <p>[0063]	如附图3所示，AdaBoost算法针对不同的训练集训练同一个基本分类器(弱分类</p>
    <p>7CN 102915448 A	说明书	6/7 页</p>
    <p>器)，然后把这些在不同训练集上得到的分类器集合起来，构成一个更强的最终的分类器(强分类器)。理论证明，只要每个弱分类器分类能力比随机猜测要好，当其个数趋向于无穷个数时，强分类器的错误率将趋向于零。AdaBoost算法中不同的训练集是通过调整每个样本对应的权重实现的。最开始的时候，每个样本对应的权重是相同的，在此样本分布下训练出一个基本分类器Ii1 (X)。对于Ii1 (X)错分的样本，则增加其对应样本的权重；而对于正确分类的样本，则降低其权重。这样可以使得错分的样本突出出来，并得到一个新的样本分布。同时，根据错分的情况赋Th1(X) &#8212;个权重，表示该基本分类器的重要程度，错分得越少权重越大。在新的样本分布下，再次对基本分类器进行训练，得到基本分类器h2(x)及其权重。依次类推，经过T次这样的循环，就得到了 T个基本分类器，以及T个对应的权重。最后把这T个基本分类器按一定权重累加起来，就得到了最终所期望的强分类器。</p>
    <p>[0064]	AdaBoost算法的具体描述如下：</p>
    <p>[0065]	假定X表示样本空间，Y表示样本类别标识集合，假设是二值分类问题，这里限定Y= {-1，+1} O 令 S=Kxi, Yi) I i=l, 2, ..., m}为样本训练集，其中 Xi e X, yi e Y。</p>
    <p>[0066]	①初始化m个样本的权值，假设初始样本分布Dt为均匀分布=D1 (i) =l/m, Dt (i)表示在第t轮迭代中赋给样本（Xi，Yi)的权值。</p>
    <p>[0067]	②令T表示迭代的次数。</p>
    <p>[0068]③	For t=lto T do</p>
    <p>[0069]	根据样本分布Dt，通过对训练集S进行抽样(有回放）产生训练集St。在训练集St上训练分类器ht。用分类器ht对原训练集S中的所有样本分类。得到本轮的分类器ht:X &#8212;Y，并且有误差 Et=Pri-DiQit(Xi) ^ Yi] O 令 Ctt= (1/2)1 η[(1- ε t)/ ε J，更新每个样本的权值</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00081.png"> <img id="idf0004" file="CN102915448AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00081.png" class="patent-full-image" alt="Figure CN102915448AD00081"> </a> </div>
    <p>[0071]，其中，Zt是一个正规因子，用来确保ΣΑ+Ια)=1。</p>
    <p>[0072]	End for</p>
    <p>[0073]	④最终的预测输出为：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00082.png"> <img id="idf0005" file="CN102915448AD00082.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00082.png" class="patent-full-image" alt="Figure CN102915448AD00082"> </a> </div>
    <p>[0075]	利用最终的强分类对测试集中的三维模型进行测试的流程图如图4所示，可以看出测试的流程和训练的流程基本相似，只不过在测试之前我们已经知道了每一个弱分类器和它的权重，以及迭代次数T，所以可以直接得到测试的结果，及分类的结果。</p>
    <p>[0076]	对于多分类问题，我们可以把它看作是二分类问题的延伸，即把每一类与其它多类看作二分类问题进行分类，直至全部类型循环完毕，就可以得到多分类的结果了。</p>
    <p>[0077]	本发明效果可以通过以下仿真实验进一步说明。仿真实验的数据集是加拿大McGill University Shape Benchmark三维模型数据集。该数据集共包含10类形状（ants,crabs, hands, humans, octopuses, pliers, snakes, spectacles, spiders, teddies),本发明将该数据集分为训练集和测试集。训练集包含10类，每类10个模型，共100个模型。测试集包含10类，每类10个模型，共100个模型。在训练集上训练分类器，在测试集上评估分类器的分类结果。在训练和测试时，都是把一类形状的特征值和其它类形状的特征值分</p>
    <p>8别贴上不同的标签进行分类，这样连续进行10次就可以得到所有分类结果。对分类结果进行统计，统计结果如表I所示，其中误差率和正确率分别是指分类错误和分类正确的三维模型的数目占分类模型总数目的百分比。</p>
    <p>[0078]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00091.png"> <img id="idf0006" file="CN102915448AD00091.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102915448A/CN102915448AD00091.png" class="patent-full-image" alt="Figure CN102915448AD00091"> </a> </div>
    <p>[0079]表	I</p>
    <p>[0080]	每类三维模型的分类精度如附图5所示，从分类结果中可以看出，本发明采用的三维图形自动特征提取和分类的方法可以将分类的平均精度达到95. 9%，这在现有的图形分类方法中具有一定的优越性，具有良好的分类的性能。</p>
    <p>9</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101398886A?cl=zh">CN101398886A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年3月17日</td><td class="patent-data-table-td patent-date-value">2009年4月1日</td><td class="patent-data-table-td ">杭州大清智能技术开发有限公司</td><td class="patent-data-table-td ">一种基于双目被动立体视觉的快速三维人脸识别方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102622609A?cl=zh">CN102622609A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年3月1日</td><td class="patent-data-table-td patent-date-value">2012年8月1日</td><td class="patent-data-table-td ">西北工业大学</td><td class="patent-data-table-td ">一种基于支持向量机的三维模型自动分类方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090319454">US20090319454</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value">2009年12月24日</td><td class="patent-data-table-td ">Drexel University</td><td class="patent-data-table-td ">Automated learning of model classifications</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">非专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">参考文献</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">刘小明等: "<a href='http://scholar.google.com/scholar?q="%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E5%8A%A0%E6%9D%83%E9%9D%9E%E5%AF%B9%E7%A7%B0AdaBoost+HMM%E7%9A%84%E4%B8%89%E7%BB%B4%E6%A8%A1%E5%9E%8B%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95"'>基于适应加权非对称AdaBoost HMM的三维模型分类算法</a>", 《浙江大学学报（工学版）》, no. 408, 31 August 2006 (2006-08-31)</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">孙晓鹏: "<a href='http://scholar.google.com/scholar?q="%E4%B8%89%E7%BB%B4%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%86%E5%89%B2%E5%8F%8A%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6"'>三维模型的分割及应用研究</a>", 《中国博士学位论文全文数据库》信息科技辑（月刊）》, no. 12, 31 December 2005 (2005-12-31)</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=e3bBBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009620000">G06K9/62</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年2月6日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年3月20日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年10月14日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/bc7c704f818f9b9c96a2/CN102915448A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8EAdaBoost%E7%9A%84%E4%B8%89%E7%BB%B4%E6%A8%A1%E5%9E%8B%E8%87%AA.pdf?id=e3bBBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U0-S-Fop1B_yAn1xaT59wpbvkVBhQ"},"sample_url":"https://www.google.com/patents/reader?id=e3bBBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>