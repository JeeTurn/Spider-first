<!DOCTYPE html><html><head><title>专利 CN102254043A - 一种基于语义映射的服装图像检索方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于语义映射的服装图像检索方法"><meta name="DC.contributor" content="丁剑" scheme="inventor"><meta name="DC.contributor" content="叶茂" scheme="inventor"><meta name="DC.contributor" content="周景磊" scheme="inventor"><meta name="DC.contributor" content="电子科技大学" scheme="assignee"><meta name="DC.date" content="2011-8-17" scheme="dateSubmitted"><meta name="DC.description" content="本发明属于计算机多媒体技术领域，公开了一种基于语义映射的服装图像检索方法。具体包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤。本发明的方法依靠已有的服装图像和相应的文本描述信息，通过多类特征聚类的方式构建服装领域知识库，通过Graph Cut模型融合知识库中的服装图像信息、知识库中的服装图像与描述文本的共生信息、知识库中的服装描述文本信息，对提交的待查询图像进行服装语义的获取，并根据获取的服装语义对提交的服装图像进行相似服装图像的检索。"><meta name="DC.date" content="2011-11-23"><meta name="DC.relation" content="CN:101692224:A" scheme="references"><meta name="DC.relation" content="CN:101853299:A" scheme="references"><meta name="DC.relation" content="CN:1936892:A" scheme="references"><meta name="citation_patent_publication_number" content="CN:102254043:A"><meta name="citation_patent_application_number" content="CN:201110236889"><link rel="canonical" href="https://www.google.com/patents/CN102254043A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102254043A?cl=zh"/><meta name="title" content="专利 CN102254043A - 一种基于语义映射的服装图像检索方法"/><meta name="description" content="本发明属于计算机多媒体技术领域，公开了一种基于语义映射的服装图像检索方法。具体包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤。本发明的方法依靠已有的服装图像和相应的文本描述信息，通过多类特征聚类的方式构建服装领域知识库，通过Graph Cut模型融合知识库中的服装图像信息、知识库中的服装图像与描述文本的共生信息、知识库中的服装描述文本信息，对提交的待查询图像进行服装语义的获取，并根据获取的服装语义对提交的服装图像进行相似服装图像的检索。"/><meta property="og:title" content="专利 CN102254043A - 一种基于语义映射的服装图像检索方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102254043A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102254043A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=GvmaBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102254043A&amp;usg=AFQjCNEnE2Cil_CRLb1QbXrzLnjiArIcEw" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/96e10e9e648fee1ee4f2/CN102254043A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/96e10e9e648fee1ee4f2/CN102254043A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102254043A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102254043A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102254043A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102254043B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102254043A?cl=zh" style="display:none"><span itemprop="description">本发明属于计算机多媒体技术领域，公开了一种基于语义映射的服装图像检索方法。具体包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤。本发明的方法依靠已有的服装图像和相应的文本描述信息，通过多类特征聚类的方式构建服装领域知识库，通过Graph...</span><span itemprop="url">https://www.google.com/patents/CN102254043A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102254043A - 一种基于语义映射的服装图像检索方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102254043A - 一种基于语义映射的服装图像检索方法" title="专利 CN102254043A - 一种基于语义映射的服装图像检索方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102254043 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110236889</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2011年11月23日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年8月17日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2011年8月17日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102254043B?hl=zh-CN&amp;cl=zh">CN102254043B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110236889.X, </span><span class="patent-bibdata-value">CN 102254043 A, </span><span class="patent-bibdata-value">CN 102254043A, </span><span class="patent-bibdata-value">CN 201110236889, </span><span class="patent-bibdata-value">CN-A-102254043, </span><span class="patent-bibdata-value">CN102254043 A, </span><span class="patent-bibdata-value">CN102254043A, </span><span class="patent-bibdata-value">CN201110236889, </span><span class="patent-bibdata-value">CN201110236889.X</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E4%B8%81%E5%89%91%22">丁剑</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%8F%B6%E8%8C%82%22">叶茂</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%91%A8%E6%99%AF%E7%A3%8A%22">周景磊</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%22">电子科技大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102254043A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102254043A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102254043A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (7),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (1),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=GvmaBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110236889&amp;usg=AFQjCNG714LgMYpT7neFMeOUHNGjpvQKoQ"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=GvmaBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102254043A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHVHALua1aW71Gwbl1p9DK21nZpXQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT104735050" lang="ZH" load-source="patent-office">一种基于语义映射的服装图像检索方法</invention-title>
      </span><br><span class="patent-number">CN 102254043 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA86635875" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明属于计算机多媒体技术领域，公开了一种基于语义映射的服装图像检索方法。具体包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤。本发明的方法依靠已有的服装图像和相应的文本描述信息，通过多类特征聚类的方式构建服装领域知识库，通过Graph&#160;Cut模型融合知识库中的服装图像信息、知识库中的服装图像与描述文本的共生信息、知识库中的服装描述文本信息，对提交的待查询图像进行服装语义的获取，并根据获取的服装语义对提交的服装图像进行相似服装图像的检索。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(6)</span></span></div><div class="patent-text"><div mxw-id="PCLM37094673" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种基于语义映射的服装图像检索方法，包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤，其中，构建服装领域知识库步骤具体过程如下：51.提取服装图像的底层特征：对服装图像数据库中的服装图像进行预处理，在预处理的基础之上，提取服装图像的底层特征；52.构建服装领域知识库：依靠服装图像的底层特征构建服装领域知识库，依次对服装图像的底层特征进行特征聚类，从归属于某一类别的服装图像所对应的服装网页中，提取高频词，作为描述该类别服装特征的领域知识，在得到该类别的服装领域知识之后，从该类别服装图像中剔除掉对该类别领域知识贡献度小于设定阈值的服装图像；获取服装图像的语义步骤具体过程如下：53.依靠服装领域知识库，对待获取语义信息的服装图像进行语义获取，首先提取待获取语义信息的服装图像的底层特征，将得到的底层特征分配到距离最近的服装图像类别集中，将该类别集所对应的高频词库作为初始的语义信息，通过计算与所属类别集中所有服装图像的特征距离与语义距离之积的总和，来度量待获取语义信息的服装图像与已有服装图像的图像与图像，图像与语义的关联程度，并通过计算初始语义信息的语义距离来度量待获取语义信息的服装图像与已有服装图像的语义与语义的关联程度，最后从初始的语义信息中抽取用于描述待获取语义信息的服装图像的语义信息；检索待查询服装图像步骤具体过程如下：54.根据步骤S3，获取待查询服装图像的语义信息，然后依据获取到的语义信息对服装图像进行检索，依照检索的结果从服装图像数据库中提取相应的服装图像作为查询结果返回。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的服装图像检索方法，其特征在于，步骤Sl中所述的底层特征包括颜色特征、纹理特征、梯度特征、形状特征以及局部点特征。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1或2所述的服装图像检索方法，其特征在于，步骤S3具体采用Graph Cut模型从初始的语义信息中抽取用于描述服装图像的语义信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求3所述的服装图像检索方法，其特征在于，步骤Sl具体包括如下分步骤：511.对服装图像进行中值滤波；512.对滤波的服装图像进行灰度值拉伸；513.提取服装图像颜色特征描述符，采用RGB颜色空间的非线性混合运算作为特征描述子；514.提对服装图像纹理特征描述符，采用局部二值模式作为特征描述子；515.提取服装图像梯度特征描述符，采用梯度直方图作为特征描述子；516.提取服装图像形状特征描述符，采用形状上下文作为特征描述子；517.提取服装图像局部特征点描述符，采用尺度不变特征点作为特征描述子。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求3或4所述的服装图像检索方法，其特征在于，步骤S2具体包括如下分步骤：S21.对服装图像的颜色特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类颜色特征的服装颜色知识库；s22.对服装图像的纹理特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类纹理特征的服装纹理知识库；s23.对服装图像的梯度特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类梯度特征的服装梯度知识库；s24.对服装图像的形状特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类形状特征的服装形状知识库；s25.对服装图像的局部特征点进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类局部特征点的服装局部特征点知识库；s26.对服装颜色领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除；s27.对服装纹理领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除；s28.对服装梯度领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除；s29.对服装形状领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除；S210.对服装局部特征点领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.根据权利要求5所述的服装图像检索方法，其特征在于，步骤S3具体包括如下分步骤：s31.对待获取服装语义信息的服装图像进行Sll&#12316;S17步骤的操作；s32.依据待获取服装语义信息的服装图像的颜色特征，确定其在服装颜色知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集；s33.依据待获取服装语义信息的服装图像的纹理特征，确定其在服装纹理知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集；s34.依据待获取服装语义信息的服装图像的梯度特征，确定其在服装梯度知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集；s35.依据待获取服装语义信息的服装图像的形状特征，确定其在服装形状知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集；s36.依据待获取服装语义信息的服装图像的局部特征点，确定其在服装局部特征点知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集；s37.计算待处理服装图像与初始服装语义信息集中包含的服装颜色领域知识所属类别的样本图像集中每一张服装图像的颜色特征距离；s38.计算待处理服装图像与初始服装语义信息集中包含的服装纹理领域知识所属类别的样本图像集中每一张服装图像的纹理特征距离；s39.计算待处理服装图像与初始服装语义信息集中包含的服装梯度领域知识所属类别的样本图像集中每一张服装图像的梯度特征距离；S310.计算待处理服装图像与初始服装语义信息集中包含的服装形状领域知识所属类别的样本图像集中每一张服装图像的形状特征距离；&#183;5311.计算待处理服装图像与初始服装语义信息集中包含的服装局部特征点领域知识所属类别的样本图像集中每一张服装图像的局部特征点距离；&#183;5312.计算初始语义集中每个一语义是否出现在所属类别的样本图像的对应网页中， 若出现则考虑该图像对于该语义的影响程度，否则不考虑该图像对于该语义的影响程度， 最终对于每个语义加总，计算存在影响程度的样本图像与待处理服装图像的标准化距离。&#183;5313.计算初始语义集中任何一个语义对的语义距离以及在服装网页中的共生概率， 该语义距离与共生概率的乘积为语义相关程度的度量；&#183;5314.将初始语义集中的语义词作为节点，利用S312&#12316;S313步骤中产生的距离度量， 构建Graph Cut模型，利用Graph Cut模型得到最终的语义词集，该语义词集则为获取的服装图像的语义信息。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES42746574" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于语义映射的服装图像检索方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明属于计算机多媒体技术领域，具体涉及一种图像检索技术。 背景技术</p>
    <p>[0002]	伴随着我国经济的高速发展，人们生活水平的逐步提高以及互联网交易平台的逐渐发展完善，如今网络购物已经成为了日常消费的重要形式之一，而网络购物与传统购物最大的区别在于商品信息的获取方式上，传统购物可以通过询问、观察、触摸得到大量的第一手信息，通过走街串巷寻找类似的商品，但是在网络购物环境下顾客只能依靠搜索引擎来完成相似的活动，几乎国内外各大购物网站都提供了文本搜索的服务，但是文本搜索服务一般适用于标准化程度较高的产品或者消费者要有较强的领域背景，才能够比较迅速的定位到合适的商品，而对于标准化较低、个性化显著的商品，消费者所付出的搜索时间是相当可观的，而服装恰恰是这样一种个性化较强的大众日常消费品。据国内知名大型网上购物网站公布的统计数据，每天该网站商品搜索的关键字中有60%是与服装有关的，由此可以看出对于服装检索的市场需求是巨大的，而与此对应的是服装搜索手段的单一化，现存的商用服装搜索都是依靠文本检索来完成，这不但要求商家手工添加服装的描述信息，而且还要要求消费者具有一定的服装领域知识，而相比于文本搜索更为直观的图像搜索技术则由于不能有效的抽取层次化、结构化的图像语义信息，而且面向通用图像检索技术设计的语义抽取方式不能够有效的表达服装图像的语义信息，从而使得服装检索的手段一直停留在文本关键字检索的阶段。</p>
    <p>[0003]	在处理服装检索时也出现了一些图像搜索的方法。</p>
    <p>[0004]	在公开号为CN 101271476公开了一种“网络图像搜索中基于聚类的相关反馈检索方法”，该方法主要注重用户反馈信息的使用，而且以关键字为查询，对于以图像为主要表现形式的服装而言，适用性有限，并且该方法需要用户进行手工的标注，对用户的参与度要求较高，不能通过自主学习的机制判断用户的查询概念是否与图像包一致，推广性有限。</p>
    <p>[0005]	在公开号为CN 101329677公开了一种“基于图像内容的图像搜索引擎”，本发明使用图像名称、地址信息、页面文字等文本信息进行图像语义的提取，并没有将图像底层特征与图像的语义特征进行关联，不能直接通过图像底层特征获取图像语义特征，不具有稳定性，而且不利于推广，适用性有限。</p>
    <p>发明内容</p>
    <p>[0006]	本发明的目的是为了解决现有的服装检索存在的问题，提出了一种基于语义映射的服装图像检索方法。</p>
    <p>[0007]	本发明的技术方案是：一种基于语义映射的服装图像检索方法，包括构建服装领域知识库步骤、获取服装图像的语义信息步骤以及检索待查询服装图像步骤，其中，构建服装领域知识库步骤具体过程如下：</p>
    <p>[0008]	Si.提取服装图像的底层特征：对服装图像数据库中的服装图像进行预处理，在预处理的基础之上，提取服装图像的底层特征；</p>
    <p>[0009]	S2.构建服装领域知识库：依靠服装图像的底层特征构建服装领域知识库，依次对服装图像的底层特征进行特征聚类，从归属于某一类别的服装图像所对应的服装网页中，提取高频词，作为描述该类别服装特征的领域知识，在得到该类别的服装领域知识之后，从该类别服装图像中剔除掉对该类别领域知识贡献度小于设定阈值的服装图像；</p>
    <p>[0010]	获取服装图像的语义步骤具体过程如下：</p>
    <p>[0011]	S3.依靠服装领域知识库，对待获取语义信息的服装图像进行语义获取，首先提取待获取语义信息的服装图像的底层特征，将得到的底层特征分配到距离最近的服装图像类别集中，将该类别集所对应的高频词库作为初始的语义信息，通过计算与所属类别集中所有服装图像的特征距离与语义距离之积的总和，来度量待获取语义信息的服装图像与已有服装图像的图像与图像，图像与语义的关联程度，并通过计算初始语义信息的语义距离来度量待获取语义信息的服装图像与已有服装图像的语义与语义的关联程度，最后从初始的语义信息中抽取用于描述待获取语义信息的服装图像的语义信息；</p>
    <p>[0012]	检索待查询服装图像步骤具体过程如下：</p>
    <p>[0013]	S4.根据步骤S3，获取待查询服装图像的语义信息，然后依据获取到的语义信息对服装图像进行检索，依照检索的结果从服装图像数据库中提取相应的服装图像作为查询结果返回。</p>
    <p>[0014]	这里，步骤Sl中所述的底层特征包括颜色特征、纹理特征、梯度特征、形状特征以及局部点特征。</p>
    <p>[0015]	优选的，步骤S3具体采用Graph Cut模型从初始的语义信息中抽取用于描述服装图像的语义信息。</p>
    <p>[0016]	本发明的有益效果：为了满足对服装图像进行语义层次上的检索需要，本发明的方法依靠已有的服装图像和相应的文本描述信息，通过多类特征（颜色、纹理、梯度、形状以及局部点）聚类的方式构建服装领域知识库，通过Graph Cut模型融合知识库中的服装图像信息、知识库中的服装图像与描述文本的共生信息、知识库中的服装描述文本信息，对提交的待查询的图像进行服装语义的获取，并根据获取的服装语义对提交的服装图像进行相似服装图像的检索。</p>
    <p>附图说明</p>
    <p>[0017]	图1为本发明基于语义映射的服装图像检索方法流程示意图。</p>
    <p>[0018]	图2为服装图像领域知识库构建流程示意图。</p>
    <p>[0019]	图3为服装图像语义映射模型结构示意图。</p>
    <p>[0020]	图4为基于语义映射的服装图像检索结果展示图。</p>
    <p>具体实施方式</p>
    <p>[0021]	下面结合附图对本发明作进一步详细的描述：</p>
    <p>[0022]	如图1，图中描述的为基于语义映射的服装图像检索总体流程图。在构建服装领域知识库和获取服装图像的语义信息的基础上，用户提交待查询的服装图像，对待查询图像进行预处理，并且在预处理的基础之上提取待查询图像的颜色特征、纹理特征、梯度特征、形状特征以及局部点特征。根据这些提取出来的待查询服装图像的底层特征，计算与各类特征样本库中已有簇的距离，为待查询服装图像确定其底层特征的所属类别，至此待查询服装图像，在服装颜色样本库中应该确定了其颜色的所属图像簇，同理在服装纹理样本库、 服装梯度样本库、服装形状样本库以及服装局部点样本库中确定了所属样本簇，接下来根据所确定的五个样本簇到服装图像高频词库中寻找对应的高频词簇，这样待查询服装图像就分别在服装颜色高频词库、服装纹理高频词库、服装梯度高频词库、服装形状高频词库以及服装局部点高频词库中确定了其对应的高频词簇，这些高频词簇作为待查询图像初始的语义信息集，将初始的语义信息集中的每一个语义词作为一个图节点，根据已经确定的服装颜色样本簇、服装纹理样本簇、服装梯度样本簇、服装形状样本簇、服装局部特征点样本簇以及其相对应的高频词簇，分别计算服装图像与图像的相关性、服装图像与语义词共生性、服装语义与语义相关性，通过这些计算为构建Graph Cut模型提高了边权重，通过Graph Cut模型的优化操作，得到适应度最高的语义词集，至此待查询图像即被映射成为特定的语义词集，以语义词集为新的查询请求，在服装图像语义索引库中进行文本检索，根据检索的结果从服装图像数据库中取出相应的服装图像作为待查询服装图像的检索结果。</p>
    <p>[0023]	具体过程如下：</p>
    <p>[0024]	提取服装图像的底层特征：对服装图像数据库中的服装图像进行预处理，在预处理的基础之上，对服装图像进行图像底层特征的提取。具体可以采用如下分步骤：</p>
    <p>[0025]	Sll.对服装图像数据库中的服装图像进行中值滤波，滤除服装图像中存在的椒盐</p>
    <p>噪音；</p>
    <p>[0026]	S12.对滤波的服装图像进行灰度值拉伸；</p>
    <p>[0027]	S13.提取服装图像颜色特征描述符，采用RGB颜色空间的非线性混合运算作为特征描述子；</p>
    <p>[0028]	S14.提对服装图像纹理特征描述符，采用局部二值模式（LBP)作为特征描述子；</p>
    <p>[0029]	S15.提取服装图像梯度特征描述符，采用梯度直方图（HOG)作为特征描述子；</p>
    <p>[0030]	S16.提取服装图像形状特征描述符，采用形状上下文（Siape Context)作为特征描述子；</p>
    <p>[0031]	S17.提取服装图像局部特征点描述符，采用尺度不变特征点（SIFT)作为特征描述子。</p>
    <p>[0032]	服装往往是一种个性化很强的消费产品，因此对于服装的文本描述并没有标准化，通过文本搜索引擎往往用户因不知如何表达自己的服装搜索意愿或者表达错误，影响用户的购买体验，而网络上大量的服装销售网页本身就蕴含着对服装商品的通用描述，这种通用描述被服装销售方经常使用并且对于资深的服装购买用户也十分熟悉，但是对于并没有很多购买经验的普通消费者，这些通用描述是并不熟悉的，而这些通用描述就是服装的领域知识。因而可以通过利用网络中已有的服装图像和服装网页构建服装领域知识库， 即为依靠服装图像的底层特征构建服装领域知识库，依次对服装图像的底层特征进行特征聚类，从归属于某一类别的服装图像所对应的服装网页中，提取高频词，作为描述该类别服装特征的领域知识，在得到该类别的服装领域知识之后，从该类别服装图像中剔除掉对该类别领域知识贡献度小于设定阈值的服装图像。这里的阈值可以设定为1%，即将描述词集包含的高频词数量少于该领域知识高频词总量的的服装图像从该类别服装图像中剔除。</p>
    <p>[0033]	具体如图2所示，可以采用如下分步骤：</p>
    <p>[0034]	S21.对服装图像的颜色特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类颜色特征的服装颜色知识库。</p>
    <p>[0035]	颜色特征采用一种基于RGB颜色通道非线性混合运算的颜色特征描述方式。假设当前待检索的原始彩色图像为I，其三个通道的值分别为&amp;和bI;则用一个颜色特征向量v。。lOT来代表I的颜色特征，颜色特征向量的表达形式如下：</p>
    <p>[0036]	Vcolor = (rjgj, Tjb1, gjbj, r/g/, r/b/, gI2bI2, I^g1ID1)</p>
    <p>[0037]	S22.对服装图像的纹理特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类纹理特征的服装纹理知识库。</p>
    <p>[0038]	使用局部二值模式（LBP)特征来表达服装图像的纹理特性，LBP特征具有对光照和一定旋转的不敏感性。</p>
    <p>[0039]	S23.对服装图像的梯度特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类梯度特征的服装梯度知识库。</p>
    <p>[0040]	使用HOG(梯度方向的直方图）特征来表达服装图像的梯度特征，HOG是描述图像整体或局部梯度信息的特征表达方式，从图像语义上表现为可以用来表示一些图像上有很好结构信息的元素。</p>
    <p>[0041]	S24.对服装图像的形状特征进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类形状特征的服装形状知识库。</p>
    <p>[0042]	使用形状上下文（shape context)表示服装图像的形状特征，shape context对于形状具有较高的描述性，能够容忍一定程度的形变。</p>
    <p>[0043]	S25.对服装图像的局部特征点进行聚类，对每个类别中的服装图像对应的服装网页进行高频词提取，将该类高频词集作为该类局部特征点的服装局部特征点知识库。</p>
    <p>[0044]使用	SIFT Gcale-invariant Feature Transform，尺度不变特征转换）提取服装图像的局部特征点。</p>
    <p>[0045]	SIFT主要有如下特点：</p>
    <p>[0046]	1.对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</p>
    <p>[0047]	2.信息量丰富，使用于在海量特征数据库中进行快速、准确的匹配；</p>
    <p>[0048]	3.特征表达性好，对于未知结构信息的图像，哪怕含有较少的物体，也能够提取出大量的SIFT特征向量；</p>
    <p>[0049]	4.经过优化的SIFT算法，能够满足实时的要求，因此可以用在服装的在线智能分析上；</p>
    <p>[0050]	5.可以很方便地与其它特征进行融合分析。</p>
    <p>[0051]	S26.对服装颜色领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</p>
    <p>[0052]	该服装图像的过滤操作是为了保证服装颜色领域知识库语义层次与图像层次统</p>
    <p>一与一致。</p>
    <p>[0053]	S27.对服装纹理领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</p>
    <p>[0054]	该服装图像的过滤操作是为了保证服装纹理领域知识库语义层次与图像层次统</p>
    <p>一与一致。</p>
    <p>[0055]	S28.对服装梯度领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</p>
    <p>[0056]	该服装图像的过滤操作是为了保证服装梯度领域知识库语义层次与图像层次统</p>
    <p>一与一致。</p>
    <p>[0057]	S29.对服装形状领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</p>
    <p>[0058]	该服装图像的过滤操作是为了保证服装形状领域知识库语义层次与图像层次统</p>
    <p>一与一致。</p>
    <p>[0059]	S210.对服装局部特征点领域知识每一类中的服装图像进行选取，若该服装图像所对应的网页中包含一定比例的高频词则保留作为该类的样本图像，否则则将该图像移除。</p>
    <p>[0060]	该服装图像的过滤操作是为了保证服装局部特征点领域知识库语义层次与图像</p>
    <p>层次统一与一致。</p>
    <p>[0061]	这里的“包含一定比例”可以根据实际情况进行具体设定。</p>
    <p>[0062]	这里，将服装图像数据库中不符合贡献度要求的图像剔除后形成服装图像集合称为标准服装图像样本库，按照不同的底层特征的聚类结果将标准服装图像样本库分为服装颜色样本库，服装纹理样本库，服装梯度样本库，服装形状样本库以及服装局部点样本库。</p>
    <p>[0063]	依靠构建的服装领域知识库，可以对任何一张服装图像进行服装语义信息的获取。首先提取待获取语义信息的服装图像的底层特征，将得到的底层特征分配到距离最近的服装图像类别集中，将该类别集所对应的高频词库作为初始的语义信息，通过计算与所属类别集中所有服装图像的特征距离与语义距离之积的总和，来度量待获取语义信息的服装图像与已有服装图像的图像与图像，图像与语义的关联程度，并通过计算初始语义信息的语义距离来度量待获取语义信息的服装图像与已有服装图像的语义与语义的关联程度， 最后使用Graph Cut模型从初始的语义信息中抽取用于描述待获取语义信息的服装图像的语义信息；具体步骤如下：</p>
    <p>[0064]	S31.对待获取服装语义信息的服装图像进行Sll&#12316;S17步骤的操作。</p>
    <p>[0065]	S32.依据待获取服装语义信息的服装图像的颜色特征，确定其在服装颜色知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集。</p>
    <p>[0066]	S33.依据待获取服装语义信息的服装图像的纹理特征，确定其在服装纹理知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集。</p>
    <p>[0067]	S34.依据待获取服装语义信息的服装图像的梯度特征，确定其在服装梯度知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集。</p>
    <p>[0068]	S35.依据待获取服装语义信息的服装图像的形状特征，确定其在服装形状知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集。</p>
    <p>[0069]	S36.依据待获取服装语义信息的服装图像的局部特征点，确定其在服装局部特征点知识库中所属类别，将该类别的高频词集纳入初始服装语义信息集。[0070]	S37.计算待处理服装图像与初始服装语义信息集中包含的服装颜色领域知识所属类别的样本图像集中每一张服装图像的颜色特征距离。</p>
    <p>[0071]	S38.计算待处理服装图像与初始服装语义信息集中包含的服装纹理领域知识所属类别的样本图像集中每一张服装图像的纹理特征距离。</p>
    <p>[0072]	S39.计算待处理服装图像与初始服装语义信息集中包含的服装梯度领域知识所属类别的样本图像集中每一张服装图像的梯度特征距离。</p>
    <p>[0073]	S310.计算待处理服装图像与初始服装语义信息集中包含的服装形状领域知识所属类别的样本图像集中每一张服装图像的形状特征距离。</p>
    <p>[0074]	S311.计算待处理服装图像与初始服装语义信息集中包含的服装局部特征点领域知识所属类别的样本图像集中每一张服装图像的局部特征点距离。</p>
    <p>[0075]	S312.计算初始语义集中每个一语义是否出现在所属类别的样本图像的对应网页中，若出现则考虑该图像对于该语义的影响程度，否则不考虑该图像对于该语义的影响程度，最终对于每个语义加总，计算存在影响程度的样本图像与待处理服装图像的标准化距</p>
    <p>1&#190; O</p>
    <p>[0076]	S313.计算初始语义集中任何一个语义对的语义距离以及在服装网页中的共生概率，该语义距离与共生概率的乘积为语义相关程度的度量。</p>
    <p>[0077]	S314.将初始语义集中的语义词作为节点，利用S312&#12316;S313步骤中产生的距离度量，构建Graph Cut模型，利用Graph Cut模型得到最终的语义词集，该语义词集则为获取的服装图像的语义信息。</p>
    <p>[0078]	这里，初始语义集指的是通过S37&#12316;S311得到各种领域知识所属类别的总和，但是每种领域知识有多个类别，因此初始语义集是初始服装语义信息集的子集。初始语义集是初始服装语义信息集的子集，是与待处理服装图像对应的，不同的待处理服装图像有不同的初始语义集，但是都是初始服装语义信息集的子集。</p>
    <p>[0079]	检索服装图像的语义：</p>
    <p>[0080]	用户可以提交服装图像，通过服装语义映射将图像查询转变为语义文本查询，具体为：根据步骤S3获取待查询服装图像的语义信息，然后依据获取到的语义信息对服装图像进行检索，依照检索的结果从服装图像数据库中提取相应的服装图像作为查询结果返回。具体过程如下：</p>
    <p>[0081]	S41.对用户提交的待查询的服装图像进行Sll&#12316;S17步骤的操作。</p>
    <p>[0082]	S42.根据步骤S41得到的特征数据，利用构建的服装领域知识库，进行S31&#12316; S314步骤的操作，获取待查询服装图像的语义信息，完成待查询服装图像的语义映射。</p>
    <p>[0083]	S43.根据步骤S42得到的服装语义信息，对服装语义索引库进行文本检索，依据检索的结果，从服装图像数据库中得到服装图像集合作为查询结果返回。</p>
    <p>[0084]	其中，服装语义索引库是对服装图像数据库中的所有图像进行Sl&#12316;S3的操作，提取每幅服装图像的语义信息，通过倒排索引将图像的语义信息与对应服装图像建立映射， 形成服装语义索引库，以便在查询过程中，使用查询语义词在服装语义索引库中迅速定位相关服装图像在服装图像数据库中的具体位置。</p>
    <p>[0085]	构建服装领域知识库之后，可以根据服装领域知识库对待查询的服装图像进行服装语义的映射，服装语义映射将原有的服装图像查询请求，转变为服装语义查询请求，如图3为服装图像语义映射模型结构示意图。</p>
    <p>[0086]	本发明的技术方案在服装图像的语义映射过程中借鉴了 Graph Cut模型算法的基本思想，对步骤S32&#12316;S36得到的初始服装语义信息集进行最后的“分割”，选出最优的少数语义词，作为待查询服装图像语义映射的结果。设Wl，w2，. . . wE是按步骤S32&#12316;S36为待查询服装图像Q挑选出的初始服装语义信息集的语义词。结合Graph Cut算法思想，如图3 所示用一个无向图G= (V，E)来代表所涉及的数据原型。图中的S节点和T节点分别代表属于待查询服装图像Q和不属于待查询服装图像Q，中间的节点代表各个语义词，利用该模型，该算法实现的目标就是：寻求一种如图3所示的划分方式，将各个语义词Wi对应的中间节点划分到两个端点，也就最终为Q找到了优化的服装语义词集。其基本思想是建立了一个划分损耗能量函数，使得这个函数取最小值是的一个划分就是系统要寻找的关键词“分</p>
    <p>割，，方式。这个能量函数表示如下=	Σ M(MA)</p>
    <p>WGT	WGW	W1GW ,W2GT</p>
    <p>[0087]	其中S和T是分别两个划分中语义词的个数。Dw(s，w)代表将某个语义词保留所需的系统代价，Dw(t，w)表示将某个语义词w去掉所花的系统代价，这些数值的计算由步骤 S37&#12316;S312完成。M(W15W2)代表初始语义信息集中，任意两个语义词的联系相关程度，该距离的计算由步骤S313完成。最后，可以通过动态规划算法，对这个能量函数进行优化，最后得到Q的最优服装语义词集，完成了对待查询服装图像语义映射。</p>
    <p>[0088]	本发明的技术方案中，利用存在网络上的服装领域知识，构建服装领域知识库，利用领域知识库完成服装图像的查询，将服装图像的查询转变为服装语义信息的检索，这一方面利用了服装图像的直观性，另一方面充分展现了语义信息的灵活性，如图4为基于语义映射的服装图像检索结果展示图所示，不论男装或是女装，本发明的技术方案都得到了比较满意的结果，这些服装检索结果并不是图像意义上的完全一致，而是在服装语义层的一致。</p>
    <p>[0089]	本发明的技术方案挖掘利用存在网络中的服装领域知识，以服装图像为查询请求，通过利用计算机视觉技术将服装图像映射为服装语义进行服装图像的检索，和现有技术相比，本发明的技术解决方案具有较高的准确度和灵活性。</p>
    <p>[0090]	本领域的普通技术人员将会意识到，这里所述的实施例是为了帮助读者理解本发明的原理，应被理解为本发明的保护范围并不局限于这样的特别陈述和实施例。本领域的普通技术人员可以根据本发明公开的这些技术启示做出各种不脱离本发明实质的其它各种具体变形和组合，这些变形和组合仍然在本发明的保护范围内。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1936892A?cl=zh">CN1936892A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2006年10月17日</td><td class="patent-data-table-td patent-date-value">2007年3月28日</td><td class="patent-data-table-td ">浙江大学</td><td class="patent-data-table-td ">图像内容语义标注方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101692224A?cl=zh">CN101692224A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年7月8日</td><td class="patent-data-table-td patent-date-value">2010年4月7日</td><td class="patent-data-table-td ">南京师范大学</td><td class="patent-data-table-td ">融合空间关系语义的高分辨率遥感图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102663092A?cl=zh">CN102663092A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月11日</td><td class="patent-data-table-td patent-date-value">2012年9月12日</td><td class="patent-data-table-td ">哈尔滨工业大学</td><td class="patent-data-table-td ">一种基于服装组图的风格元素挖掘和推荐方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102663092B?cl=zh">CN102663092B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月11日</td><td class="patent-data-table-td patent-date-value">2015年1月28日</td><td class="patent-data-table-td ">哈尔滨工业大学</td><td class="patent-data-table-td ">一种基于服装组图的风格元素挖掘和推荐方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103020172A?cl=zh">CN103020172A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月28日</td><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">北京京东世纪贸易有限公司</td><td class="patent-data-table-td ">一种利用视频信息搜索物品的方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103020172B?cl=zh">CN103020172B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月28日</td><td class="patent-data-table-td patent-date-value">2015年8月19日</td><td class="patent-data-table-td ">北京京东世纪贸易有限公司</td><td class="patent-data-table-td ">一种利用视频信息搜索物品的方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103049754A?cl=zh">CN103049754A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月7日</td><td class="patent-data-table-td patent-date-value">2013年4月17日</td><td class="patent-data-table-td ">东软集团股份有限公司</td><td class="patent-data-table-td ">社交网络的图片推荐方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103106265A?cl=zh">CN103106265A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月30日</td><td class="patent-data-table-td patent-date-value">2013年5月15日</td><td class="patent-data-table-td ">北京工商大学</td><td class="patent-data-table-td ">相似图像分类方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103345645A?cl=zh">CN103345645A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年6月27日</td><td class="patent-data-table-td patent-date-value">2013年10月9日</td><td class="patent-data-table-td ">复旦大学</td><td class="patent-data-table-td ">面向网购平台的商品图像类别预测方法</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=GvmaBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2011年11月23日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年1月4日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Granted</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/96e10e9e648fee1ee4f2/CN102254043A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E6%98%A0%E5%B0%84%E7%9A%84%E6%9C%8D%E8%A3%85%E5%9B%BE%E5%83%8F.pdf?id=GvmaBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U129d4u7yveMjq2kgRS8jR8BYKIBQ"},"sample_url":"https://www.google.com/patents/reader?id=GvmaBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>