<!DOCTYPE html><html><head><title>专利 CN103473327A - 图像检索方法与系统 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_50a6672b5f82ffbd39b7a9e87fd4594c/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_50a6672b5f82ffbd39b7a9e87fd4594c__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="图像检索方法与系统"><meta name="DC.contributor" content="钟海兰" scheme="inventor"><meta name="DC.contributor" content="广东图图搜网络科技有限公司" scheme="assignee"><meta name="DC.date" content="2013-9-13" scheme="dateSubmitted"><meta name="DC.description" content="本发明公开了一种图像检索方法与系统，针对给定的查询文本和/或查询图片，分别根据文本相关性和图片内容相关性得出多个库内图片的相似度排序列表，然后结合得出的多个排序列表，综合考虑文本相似度和图片内容相似度，返回一个综合的排序列表。这种多模态的混合检索机制避免了以往单模态检索机制的不足，发挥了文本检索方法和图像内容检索方法各自的优势，大大提高了图片检索的准确率。由于仅对各单项检索模型的排序结果进行融合，因此可以方便地增减、替换单项检索模型，实现了文本和图像内容特征检索模型的灵活配置，提高了图像检索系统的性能。"><meta name="DC.date" content="2013-12-25"><meta name="DC.relation" content="CN:101419606:A" scheme="references"><meta name="DC.relation" content="CN:101901249:A" scheme="references"><meta name="DC.relation" content="CN:101984420:A" scheme="references"><meta name="DC.relation" content="CN:102129477:A" scheme="references"><meta name="DC.relation" content="CN:102262642:A" scheme="references"><meta name="DC.relation" content="CN:102402593:A" scheme="references"><meta name="DC.relation" content="CN:1920818:A" scheme="references"><meta name="citation_patent_publication_number" content="CN:103473327:A"><meta name="citation_patent_application_number" content="CN:201310420287"><link rel="canonical" href="https://www.google.com/patents/CN103473327A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN103473327A?cl=zh"/><meta name="title" content="专利 CN103473327A - 图像检索方法与系统"/><meta name="description" content="本发明公开了一种图像检索方法与系统，针对给定的查询文本和/或查询图片，分别根据文本相关性和图片内容相关性得出多个库内图片的相似度排序列表，然后结合得出的多个排序列表，综合考虑文本相似度和图片内容相似度，返回一个综合的排序列表。这种多模态的混合检索机制避免了以往单模态检索机制的不足，发挥了文本检索方法和图像内容检索方法各自的优势，大大提高了图片检索的准确率。由于仅对各单项检索模型的排序结果进行融合，因此可以方便地增减、替换单项检索模型，实现了文本和图像内容特征检索模型的灵活配置，提高了图像检索系统的性能。"/><meta property="og:title" content="专利 CN103473327A - 图像检索方法与系统"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN103473327A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN103473327A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=eQHpCAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN103473327A&amp;usg=AFQjCNH3mP2vAH2177PUoUfTi5CPanKSPg" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/bb0366c71e16c64afc9a/CN103473327A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/bb0366c71e16c64afc9a/CN103473327A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN103473327A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN103473327A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN103473327A?cl=zh" style="display:none"><span itemprop="description">本发明公开了一种图像检索方法与系统，针对给定的查询文本和/或查询图片，分别根据文本相关性和图片内容相关性得出多个库内图片的相似度排序列表，然后结合得出的多个排序列表，综合考虑文本相似度和图片内容相似度 ...</span><span itemprop="url">https://www.google.com/patents/CN103473327A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN103473327A - 图像检索方法与系统</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN103473327A - 图像检索方法与系统" title="专利 CN103473327A - 图像检索方法与系统"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN103473327 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201310420287</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年12月25日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2013年9月13日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2013年9月13日</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201310420287.9, </span><span class="patent-bibdata-value">CN 103473327 A, </span><span class="patent-bibdata-value">CN 103473327A, </span><span class="patent-bibdata-value">CN 201310420287, </span><span class="patent-bibdata-value">CN-A-103473327, </span><span class="patent-bibdata-value">CN103473327 A, </span><span class="patent-bibdata-value">CN103473327A, </span><span class="patent-bibdata-value">CN201310420287, </span><span class="patent-bibdata-value">CN201310420287.9</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%92%9F%E6%B5%B7%E5%85%B0%22">钟海兰</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%B9%BF%E4%B8%9C%E5%9B%BE%E5%9B%BE%E6%90%9C%E7%BD%91%E7%BB%9C%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">广东图图搜网络科技有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN103473327A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103473327A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103473327A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (7),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (1),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (2)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=eQHpCAABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201310420287&amp;usg=AFQjCNHCJNT5OhJWl2zvC5QZ4d98VOMsKQ"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=eQHpCAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D103473327A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNF4VDgAR1-zwB_Au5KI0LazYsG1ig"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT132105858" lang="ZH" load-source="patent-office">图像检索方法与系统</invention-title>
      </span><br><span class="patent-number">CN 103473327 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA128282871" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明公开了一种图像检索方法与系统，针对给定的查询文本和/或查询图片，分别根据文本相关性和图片内容相关性得出多个库内图片的相似度排序列表，然后结合得出的多个排序列表，综合考虑文本相似度和图片内容相似度，返回一个综合的排序列表。这种多模态的混合检索机制避免了以往单模态检索机制的不足，发挥了文本检索方法和图像内容检索方法各自的优势，大大提高了图片检索的准确率。由于仅对各单项检索模型的排序结果进行融合，因此可以方便地增减、替换单项检索模型，实现了文本和图像内容特征检索模型的灵活配置，提高了图像检索系统的性能。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(10)</span></span></div><div class="patent-text"><div mxw-id="PCLM57430702" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.一种图像检索方法，其特征在于，包括步骤:  接收用户提交的查询图片和/或查询文本；  提取所述查询图片的各种内容特征，并对所述查询文本进行分词；  将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；   根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的图像检索方法，其特征在于，  图片的内容特征包括颜色特征、纹理特征和形状特征。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1或2所述的图像检索方法，其特征在于，  采用隐式马尔科夫模型对所述查询文本进行分词。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求1或2所述的图像检索方法，其特征在于，  采用统计语言建模方法度量分词后的所述查询文本与数据库中每张图片对应的描述性文档的相似度；  采用欧氏距离计算所述查询图片的各种内容特征与数据库中每张图片的相应内容特征的相似度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求1或2所述的图像检索方法，其特征在于，  采用遗传算法或退火算法设置每个列表的权重。</div>
    </div>
    </div> <div class="claim"> <div num="6" class="claim">
      <div class="claim-text">6.一种图像检索系统，其特征在于，包括:  查询信息接收端，用于接收用户提交的查询图片和/或查询文本；  查询信息处理模块，用于提取所述查询图片的各种内容特征，并对所述查询文本进行分词；  相似度单项排序模块，用于将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；  相似度综合排序模块，用于根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.根据权利要求6所述的图像检索系统，其特征在于，  图片的内容特征包括颜色特征、纹理特征和形状特征。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.根据权利要求6或7所述的图像检索系统，其特征在于，  所述查询信息处理模块采用隐式马尔科夫模型对所述查询文本进行分词。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求6或7所述的图像检索系统，其特征在于，  所述相似度单项排序模块采用统计语言建模方法度量分词后的所述查询文本与数据库中每张图片对应的描述性文档的相似度；采用欧氏距离计算所述查询图片的各种内容特征与数据库中每张图片的相应内容特征的相似度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10.根据权利要求6或7所述的图像检索系统，其特征在于，所述相似度综合排序模块 采用遗传算法或退火算法设置每个列表的权重。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES64392818" lang="ZH" load-source="patent-office" class="description">
    <p>图像检索方法与系统技术领域</p>
    <p>[0001]	本发明涉及信息检索技术领域，特别是涉及一种图像检索方法与系统。</p>
    <p>背景技术</p>
    <p>[0002]	近十年来，图像检索一直是多媒体领域的热点研究主题。图像检索系统是根据图像的描述性文本或者视觉特征(即图像内容)为用户提供互联网上相关图形图像资料检索服务的一类专业搜索引擎系统。例 如，谷歌，百度等一系列搜索引擎都能够提供图片搜索服务。</p>
    <p>[0003]	传统的图像检索依赖于图片的描述性文本，一般是根据关键字对数据库中的图片进行搜索。但是，文本关键字很多情况下无法准确描述图像的视觉特征(例如某种特定的花纹)，所以出现了基于图像内容的检索技术。目前，描述图像内容的特征有很多，例如颜色特征，纹理特征，形状特征等等。然而，图像内容特征虽然能捕捉图片的视觉相似性，但是视觉相似并不一定代表语义相似，即存在“语义鸿沟”问题。因此，基于文本的图像检索和基于内容的图像检索各有利弊，均不能很好地满足用户需求。</p>
    <p>发明内容</p>
    <p>[0004]	基于上述情况，本发明提出了一种图像检索方法与系统，以提高图像检索的准确性。</p>
    <p>[0005]	一种图像检索方法，包括步骤:</p>
    <p>[0006]	接收用户提交的查询图片和/或查询文本；</p>
    <p>[0007]	提取所述查询图片的各种内容特征，并对所述查询文本进行分词；</p>
    <p>[0008]	将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；</p>
    <p>[0009]	根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</p>
    <p>[0010]	一种图像检索系统，包括:</p>
    <p>[0011]	查询信息接收端，用于接收用户提交的查询图片和/或查询文本；</p>
    <p>[0012]	查询信息处理模块，用于提取所述查询图片的各种内容特征，并对所述查询文本进行分词；</p>
    <p>[0013]	相似度单项排序模块，用于将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；</p>
    <p>[0014]	相似度综合排序模块，用于根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</p>
    <p>[0015]	本发明图像检索方法与系统，针对给定的查询文本和/或查询图片，分别根据文本相关性和图片内容相关性得出多个库内图片的相似度排序列表，然后结合得出的多个排序列表，综合考虑文本相似度和图片内容相似度，返回一个综合的排序列表。这种多模态的混合检索机制避免了以往单模态检索机制的不足，发挥了文本检索方法和图像内容检索方法各自的优势，大大提高了图片检索的准确率。由于仅对各单项检索模型的排序结果进行融合，因此可以方便地增减、替换单项检索模型，实现了文本和图像内容特征检索模型的灵活配置，提高了图像检索系统的性能。</p>
    <p>附图说明</p>
    <p>[0016]	图1为本发明图像检索方法的流程示意图；</p>
    <p>[0017]	图2为本发明图像检索方法中图像内容和文本联合检索的流程图；</p>
    <p>[0018]	图3为应用本发明图像检索方法与传统检索方法的检索结果对比；</p>
    <p>[0019]	图4为本发明图像检索系统的结构示意图。</p>
    <p>具体实施方式</p>
    <p>[0020]	为了使本发明的目的、技术方案及优点更加清楚明白，以下结合附图及实施例，对本发明进行进一步的详细说明。应当理解，此处所描述的具体实施方式仅仅用以解释本发明，并不限定本发明的保护范围。</p>
    <p>[0021]	本发明的图像检索方法，如图1所示，包括步骤:</p>
    <p>[0022]	步骤S101、接收用户提交的查询图片和/或查询文本；</p>
    <p>[0023]	步骤S102、提取所述查询图片的各种内容特征，并对所述查询文本进行分词；</p>
    <p>[0024]	步骤S103、将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；</p>
    <p>[0025]	步骤S104、根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</p>
    <p>[0026]	传统的检索方法，根据用户提交的文本描述信息进行检索，或者从用户提交的图片中提取一种特征进行检索，即单模态检索。而采用本检索方法，用户既可以单纯根据图片或文本描述信息进行检索，还可以同时根据图片和文本描述信息进行联合检索。在用户只提交图片的情况下，如步骤S102所述，本检索方法提取的不是只有一种内容特征，而是提取多种内容特征，并进行综合排名。总之，相比传统检索方法，本检索方法是一种多模态的混合检索方法。实验证明，这种混合检索机制较以往单模态的检索机制在返回结果准确率方面有很大的提高。下面对上述几个步骤进行详述。</p>
    <p>[0027]	用户提交查询信息后，在步骤S102中，对提交的查询图片提取内容特征，并对提交的文本进行分词。在本发明实施例中，图像内容特征优选地包括颜色特征，纹理特征和形状特征，这三种特征是目前常用的比较典型的反应图片内容的特征。分词采取的方法是隐式马尔科夫模型(HMM)。设状态集合为Q=(qi，q2，…(^)，即标注的词性(例如词首、词中、词尾)全集；观测集合为V = (v1； V2,…qM)，即用户输入的待分词字符的全集；观测序列为O= (o1； O2,…οτ)，即输入的待分字符序列；其状态序列为I = (i1； i2,…^)，即待分字符序列可能的词性标签序列。首先确定使用的语料库，然后通过统计的方法就会得到隐式马尔科夫模型的三个参数，分别是状态转移概率矩阵A = [au]NXN，观测概率矩阵B=[h(k)]NXM，初始状态概率向量π = O i)。其中:</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00061.png"> <img id="idf0001" file="CN103473327AD00061.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00061.png" class="patent-full-image" alt="Figure CN103473327AD00061"> </a> </div>
    <p>[0031]	其中count代表频率，由训练数据获得。</p>
    <p>[0032]	当HMM模型λ = (A, B, π )确定后，使用维特比(viterbi)算法进行分词。定义时刻t状态为i的所有单个路径(i1; i2，…it_1; it，)中概率最大值为δ t(i)并设概率最大的路径的第t-Ι个节点为Wt(i)。首先初始化，令=	…，No然后递推，对t = 2,…，T，分别计算:</p>
    <p>[0033]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00062.png"> <img id="idf0002" file="CN103473327AD00062.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00062.png" class="patent-full-image" alt="Figure CN103473327AD00062"> </a> </div>
    <p>[0036]	最后，令Pi=Hiax1</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00063.png"> <img id="idf0003" file="CN103473327AD00063.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00063.png" class="patent-full-image" alt="Figure CN103473327AD00063"> </a> </div>
    <p>表不最优路径的</p>
    <p>概率，G表示最优路径的终点。找到最优路径的终点后，进行回溯，对t = T-1, T-2，…1，</p>
    <p>令</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00064.png"> <img id="idf0004" file="CN103473327AD00064.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00064.png" class="patent-full-image" alt="Figure CN103473327AD00064"> </a> </div>
    <p>求得最优路径/* = Qir2l-&#943;τ),最优路径即输出的隐藏状态序列，</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00065.png"> <img id="idf0005" file="CN103473327AD00065.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00065.png" class="patent-full-image" alt="Figure CN103473327AD00065"> </a> </div>
    <p>也就是对应的分词结果。利用相同方法对库内图片的文档信息进行分词，并利用经典的倒排索引对文档进行索引，以便高效检索。</p>
    <p>[0037]	接着我们提取查询图片的内容特征，包括颜色特征、纹理特征和形状特征。首先我们建立颜色直方图。当用户提交查询图片Q时，先对提交的图片Q进行预处理，然后在颜色向量空间上统计直方图，颜色直方图是一个一维的离散函数，即:</p>
    <p>[0038]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00066.png"> <img id="idf0006" file="CN103473327AD00066.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00066.png" class="patent-full-image" alt="Figure CN103473327AD00066"> </a> </div>
    <p>[0039]	式中，nk为量化后颜色特征值为k的像素个数，N为图像像素的总个数，I为量化后颜色特征值数，即一维矢量H的维数。由此得到查询图像Q的颜色直方图向量Hq。在离线情况下，对库内图片也同样提取颜色直方图特征并建立索引。</p>
    <p>[0040]	接着我们提取尺度不变特征(Scale-1nvariant Feature, sift),描述图片的纹理特征。当用户提交查询图片I (X，y)后，查询图片的尺度空间为:</p>
    <p>[0041]	L(x, y, ο )=G(x, y, o)*I(x,y)</p>
    <p>[0042]	G(x, y, σ )为高斯函数(sigma为尺度参数)。然后计算相邻尺度图像的高斯差分(Difference of Gaussian),即:</p>
    <p>[0043]	D(x, y, σ ) = L(x, y, k σ )-L(x, y, σ )</p>
    <p>[0044]其中，k	一般取 21/3。</p>
    <p>[0045]	计算出相邻尺度图像的高斯差分之后，得到一系列图像，并在该图像空间中求极值点。分别比较每一幅高斯差分图中的一个像素点和它所有的相邻点，看其是否比它的图像域和尺度域的相邻点大或者小。求出极值点后，需要对尺度空间DoG函数进行曲线拟合，来筛选极值点，去除低对比度和边缘上的点:</p>
    <p>[0046]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00071.png"> <img id="idf0007" file="CN103473327AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00071.png" class="patent-full-image" alt="Figure CN103473327AD00071"> </a> </div>
    <p>[0047]其中，</p>
    <p>[0048]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00072.png"> <img id="idf0008" file="CN103473327AD00072.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00072.png" class="patent-full-image" alt="Figure CN103473327AD00072"> </a> </div>
    <p>表不的是样本点的偏移，又为X的极值。对每</p>
    <p>一个候选极值点的DOO进行判断，如果其值小于某个阈值(一般取0.03即可)，则判定该候</p>
    <p>选极值点为对比度低的不稳定极值点，予以去除。</p>
    <p>[0049]	为了得到稳定的极值点，还应去除边缘的影响，当</p>
    <p>[0050]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00073.png"> <img id="idf0009" file="CN103473327AD00073.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00073.png" class="patent-full-image" alt="Figure CN103473327AD00073"> </a> </div>
    <p>[0051]	时，关键点保留，反之剔除。关键点即我们要找的特征点。其中，</p>
    <p>     [0052]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00074.png"> <img id="idf0010" file="CN103473327AD00074.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00074.png" class="patent-full-image" alt="Figure CN103473327AD00074"> </a> </div>
    <p>[0053]	是hession矩阵，Dxx是在DoG空间中某一尺度的图像X方向求导两次。THH)是H矩阵的迹，Det (H)是矩阵H的行列式。α是矩阵H较大的特征值，β是H矩阵较小的特征值，Y = α / β。</p>
    <p>[0054]	确定了图像特征点的位置以后，接下来我们通过求每个特征点邻域的梯度为图像的特征点赋一个方向，那么梯度幅值m(x, y)与梯度方向θ (X, y)定义为:</p>
    <p>[0055]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00075.png"> <img id="idf0011" file="CN103473327AD00075.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00075.png" class="patent-full-image" alt="Figure CN103473327AD00075"> </a> </div>
    <p>[0057]以特征点为中心，划定一个区域，利用所有在此区域内的点的梯度形成一个方向直方图。从直方图中选出纵坐标值最大的一项作为该特征点的主方向。如果存在其它方向，其纵坐标的大小大于主方向纵坐标的80%，也将该方向作为该特征点的方向。[0058]	特征点检测完毕后，接下来确定特征点的描述子。首先，以特征点为圆心将特征点邻域旋转θ° (调整至0° )，其中Θ为特征点的方向。在旋转后的图像中，以特征点为中心取16X16的邻域窗口，每个小格代表特征点邻域窗口中的一个像素。将16X16的矩形窗口均匀分为16个子区域，采用高斯模糊的方法，增加与特征点较近邻域的权重值，并降低与特征点较远邻域的权重值，然后计算每个区域中8个方向的梯度直方图，得到特征点描述子的特征向量，为4X4X8=128维向量。接着，将特征点描述子进行归一化处理，设D是特征点描述子，即D = ((I1, d2，…d128)，归一化后得到:</p>
    <p>[0059]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00081.png"> <img id="idf0012" file="CN103473327AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00081.png" class="patent-full-image" alt="Figure CN103473327AD00081"> </a> </div>
    <p>, 为了减少大梯度值影响，为其设定一个阈</p>
    <p>值0.2，若向量中某一维的值大于0.2，则将其置为0.2，并重新进行归一化处理。</p>
    <p>[0060]	在离线情况下，同样利用上述步骤得到库内图片的所有特征点描述子，并将这些描述子聚类。将得到的聚类作为“视觉词”，并应用词袋模型(Bag of Words)对库内图片进行倒排索引。然后应用相同的词袋模型得到查询图片的特征向量表达。</p>
    <p>[0061]	最后我们对图片的全局形状特征进行索引。用户提交查询图片后，首先利用Gabor</p>
    <p>滤波器对查询图片按照下式进行采样滤波:</p>
    <p>[0062]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00082.png"> <img id="idf0013" file="CN103473327AD00082.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00082.png" class="patent-full-image" alt="Figure CN103473327AD00082"> </a> </div>
    <p>[0065]	I为滤波器的尺度；K为正常数；O为高斯函数的标准差；Θ i= JI (1-1) /Q1,1=1, 2，...，θ 1； θ χ为尺度I下的方向总数。将图像与Gabor滤波器进行卷积，得到滤波后的图像为:</p>
    <p>[0066]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00083.png"> <img id="idf0014" file="CN103473327AD00083.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00083.png" class="patent-full-image" alt="Figure CN103473327AD00083"> </a> </div>
    <p>[0067]	把滤波后的图片分成4X4的网格，在每个网格内取平均值，最后把各方向、各尺度网格内得到的平均值放在一个向量里，作为查询图片的形状特征。在离线索引步骤，对库内图片做同样的计算得到形状特征索引(建立k_d树索引和哈希索引以便高效检索)，以便匹配图片形状特征。</p>
    <p>[0068]	当得到分词后的文本信息和查询图片的内容特征信息后，在步骤S103中，根据已得到的信息在文本和图像的联合索引中搜索查找相关图片。</p>
    <p>[0069]	图2是本检索方法较为直观的一个流程示意图，并示出了步骤S103的具体实施方法:我们分别建立基于每一个单模态(单项)特征的检索(IR)模型(步骤S201)。其中，每个IR模型是独立运行而且可以自由配置的，因此可以根据实际情况选择不同的IR模型进行组合，最后各自根据相应的排序算法返回一个包含结果的列表。然后在步骤S202中将分词后的文本信息和/或从查询图片中提取的内容特征输入相应IR模型得到多个排序列表，步骤S203对这些排序列表进行融合，最终得到综合排序列表并返回给用户。该联合搜索方法实施例结合基于文本的检索模型输出的结果和基于图像内容的检索模型输出的结果，得到一个综合的排序列表，其中包含的是返回的图片结果，并且按与查询信息相关度的降序排列。</p>
    <p>[0070]	具体来讲，在步骤S201中，首先建立基于文本的IR模型。优选地，采用统计语言建模技术(Statistical Language Modeling)建立文本IR模型。设V表示某语言的词典(vocabulary), V = {ω17 ω2,…，ω |ν|},称 Coi 为一个词项(term), D 为文档集 C 中的一篇文档,D = (I1Clfd1^di e V0在统计翻译模型中，当用户提交文本查询信息Q时,Q =</p>
    <p>qm，qi e V，文档D “翻译”成查询信息Q的概率为:</p>
    <p>[0071]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00091.png"> <img id="idf0015" file="CN103473327AD00091.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00091.png" class="patent-full-image" alt="Figure CN103473327AD00091"> </a> </div>
    <p>[0072]	其中，Ρ(ω |D)是基本的文档语言模型，t(Qi| ω)表示的是翻译概率。当计算出P(QlD)后，需要返回文档集中文档的排名。此时，我们需要估计后验概率P(D|Q)，根据贝叶斯公式有:</p>
    <p>[0073]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00092.png"> <img id="idf0016" file="CN103473327AD00092.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00092.png" class="patent-full-image" alt="Figure CN103473327AD00092"> </a> </div>
    <p>[0074]	其中，P(D)可以采用某个与查询无关的量，模型中不考虑这一项。计算出后验概率P (DIQ)后，就可以根据概率值对文档集中的文档进行排名，返回一个排序列表，如步骤S202。</p>
    <p>[0075]	当用户提交查询图片后，在步骤S201中，建立多个基于图像内容特征的IR模型。每一个IR模型对应一种图像特征，包括颜色特征，纹理特征，形状特征等。如前所述，这些图像内容特征都被表示在向量空间中。因此，需要衡量特征向量之间的相似度。优选地利用欧式距离计算相似度。两个η维向量(Χη，Χ12，*..Χιη)与(Χ21，Χ22，*..Χ2η)之间的欧式距离为:</p>
    <p>[0076]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00093.png"> <img id="idf0017" file="CN103473327AD00093.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00093.png" class="patent-full-image" alt="Figure CN103473327AD00093"> </a> </div>
    <p>[0077]	距离越大表示越不相关。相似度计算结束后，按相关度的降序返回排序列表，如步骤 S202。</p>
    <p>[0078]	需要注意的是，对图片提取几种特征，就返回几个列表。例如，如果分别提取了图像的颜色和纹理两种特征，那么就返回两个排序列表。每个列表根据与查询图片在相应视觉特征上的相关度进行降序排列。</p>
    <p>[0079]	在得出若干个图片相关度排序列表后，分别给列表上的每一张图片&amp;分配一个得分SiiLPIEM，公式如下:</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00101.png"> <img id="idf0018" file="CN103473327AD00101.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00101.png" class="patent-full-image" alt="Figure CN103473327AD00101"> </a> </div>
    <p>[0081]	其中，Ψ (x,H)表示列表H中图片X的位置，Ia是一个指示函数，当a是真时,也就</p>
    <p>是当Clj属于列表Li时取1，否则取O。a i是第i个IR模型的权重，并且</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00102.png"> <img id="idf0019" file="CN103473327AD00102.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103473327A/CN103473327AD00102.png" class="patent-full-image" alt="Figure CN103473327AD00102"> </a> </div>
    <p>观上讲，出现在多个列表中靠前位置的图片将会获得较高的分数，得分越高的图片与查询信息的相关度越大。计算上式定义的分数并以该分数进行排序，如步骤S203和S204。</p>
    <p>[0082]	上式中，如果模型中的某一个IR模型性能比其它IR模型好的话，应为之分配较高的权重值，以此来提升整个系统的性能。在本发明实例中，采用自动优化的方法来设置权重ai0可选的自动优化方法包括遗传算法，退火算法等。例如，在遗传算法中，我们首先初始化每个模型的权值，通过多次随机初始化产生初始权值向量群体。接着，对群体中每个个体的适应性进行度量。在图像检索中，一个权值向量的适应性由该权值向量产生的搜索结果的性能来衡量，即给定一些测试查询并得到这些查询对应的相关图片集，接着根据相关图片集计算程序输出搜索结果的性能作为适应性指标。衡量搜索结果性能的指标有很多种，如 Fl 分值、Normalized Discounted Cumulative Gain、Mean Average Precision 等。然后我们根据适应性对个体进行选择、交叉和变异，产生新一代的种群。新一代的种群信息优于上一代。周而复始，权值向量的适应度不断提高，直到符合算法终止条件:在最大迭代次数限制内，得到满足预先设置的权值向量适应度目标值的个体；或者达到最大迭代次数，此时返回所有产生个体中适应度最高的个体。</p>
    <p>[0083]	本发明实施例还可以包括步骤S105:如果用户对输出的结果满意，则搜索过程结束；如果用户对输出结果不是很满意，或者跟自己预先的想法有偏差，那么可以在之前的查询信息基础上补充或修改查 询文本和/或查询图片。本方法会再对修改后的文本和/或图片进行分词和特征提取，重复步骤S102至步骤S105，直到输出用户满意的结果为止。</p>
    <p>[0084]	图3示出了本发明的检索机制同单模态图像检索机制的结果对比。图中右半部分的上部显示的是单纯文本检索的结果，中部显示的是单纯图像内容检索的结果，下部是本发明提出的混合检索机制返回的结果。从返回的图像结果可以看出，本发明的性能较以往单模态检索方法的性能有了很大的提高，更大程度的满足了用户的要求。</p>
    <p>[0085]	本发明的图像检索系统是与上述方法对应的系统，如图4所示，包括:</p>
    <p>[0086]	查询信息接收端，用于接收用户提交的查询图片和/或查询文本；</p>
    <p>[0087]	查询信息处理模块，用于提取所述查询图片的各种内容特征，并对所述查询文本进行分词；</p>
    <p>[0088]	相似度单项排序模块，用于将所述查询图片的各种内容特征与数据库中每张图片的相应内容特征比较，按照相似度对数据库中的图片进行排序，得到内容相似度的各个列表；将分词后的所述查询文本与数据库每张图片对应的描述性文档比较，按照相似度对数据库中的图片进行排序，得到文本相似度的列表；</p>
    <p>[0089]	相似度综合排序模块，用于根据在各个列表中的位置及所在列表的权重，为数据库中的每个图片赋分，并按照赋分重新排序，得到相似度综合排序列表，将该列表返回用户。</p>
    <p>[0090]	作为一个优选的实施例，图片的内容特征包括颜色特征、纹理特征和形状特征。[0091]	作为一个优选的实施例，所述查询信息处理模块采用隐式马尔科夫模型对所述查询文本进行分词。</p>
    <p>[0092]	作为一个优选的实施例，所述相似度单项排序模块采用统计语言建模方法度量分词后的所述查询文本与数据库中每张图片对应的描述性文档的相似度；采用欧氏距离计算所述查询图片的各种内容特征与数据库中每张图片的相应内容特征的相似度。</p>
    <p>[0093]	作为一个优选的实施例，所述相似度综合排序模块采用遗传算法或退火算法设置每个列表的权重。</p>
    <p>[0094]	综上，本发明的有益效果如下:</p>
    <p>[0095]	本发明提升了图像搜索的实用性:首先，以往的搜索方法大都是基于单模态的搜索，在一定程度上限制了用户表达查询意图的方式。其次，基于内容的图像检索面临语义鸿沟问题。而本发明通过结合文本和图像内容多模态的信息，切实的改变了这一现状。</p>
    <p>[0096]	本发明提升了图像搜索的灵活性:以往的图像搜索方法大都是利用固定的几种特征进行搜索，而本发明的特点在于可以灵活地自由配置特征组合。</p>
    <p>[0097]	本发明提升了图像搜索的准确率:本方法通过结合基于文本图像检索方法和基于内容图像检索方法的返回结果，进而得出更精准的图片相关度排序列表，大大提升了返回结果的准确率。</p>
    <p>[0098]	以上所述实施例仅表达了本发明的几种实施方式，其描述较为具体和详细，但并不能因此而理解为对本发明专利范围的限制。应当指出的是，对于本领域的普通技术人员来说，在不脱离本发明构思的前提下，还可以做出若干变形和改进，这些都属于本发明的保护范围。因此，本发明专利的保护范围应以所附权利要求为准。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1920818A?cl=zh">CN1920818A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2006年9月14日</td><td class="patent-data-table-td patent-date-value">2007年2月28日</td><td class="patent-data-table-td ">浙江大学</td><td class="patent-data-table-td ">基于多模态信息融合分析的跨媒体检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101419606A?cl=zh">CN101419606A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年11月13日</td><td class="patent-data-table-td patent-date-value">2009年4月29日</td><td class="patent-data-table-td ">浙江大学</td><td class="patent-data-table-td ">一种基于语义和内容的半自动图像标注方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101901249A?cl=zh">CN101901249A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月12日</td><td class="patent-data-table-td patent-date-value">2010年12月1日</td><td class="patent-data-table-td ">复旦大学</td><td class="patent-data-table-td ">一种图像检索中基于文本的查询扩展与排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101984420A?cl=zh">CN101984420A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年9月3日</td><td class="patent-data-table-td patent-date-value">2011年3月9日</td><td class="patent-data-table-td ">百度在线网络技术（北京）有限公司</td><td class="patent-data-table-td ">一种基于拆词处理进行图片搜索的方法与设备</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102129477A?cl=zh">CN102129477A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年4月23日</td><td class="patent-data-table-td patent-date-value">2011年7月20日</td><td class="patent-data-table-td ">山东大学</td><td class="patent-data-table-td ">一种多模态联合的图像重排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102262642A?cl=zh">CN102262642A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年1月28日</td><td class="patent-data-table-td patent-date-value">2011年11月30日</td><td class="patent-data-table-td ">北京理工大学</td><td class="patent-data-table-td ">一种Web图像搜索引擎及其实现方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102402593A?cl=zh">CN102402593A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月4日</td><td class="patent-data-table-td patent-date-value">2012年4月4日</td><td class="patent-data-table-td ">微软公司</td><td class="patent-data-table-td ">对于搜索查询输入的多模态方式</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=eQHpCAABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年12月25日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2014年1月22日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/bb0366c71e16c64afc9a/CN103473327A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_50a6672b5f82ffbd39b7a9e87fd4594c.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F.pdf?id=eQHpCAABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U3t1sDA8twC5ysvmpghyhpzxpiXEw"},"sample_url":"https://www.google.com/patents/reader?id=eQHpCAABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>