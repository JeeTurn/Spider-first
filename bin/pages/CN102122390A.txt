<!DOCTYPE html><html><head><title>专利 CN102122390A - 基于深度图像进行人体检测的方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="基于深度图像进行人体检测的方法"><meta name="DC.contributor" content="于仕琪" scheme="inventor"><meta name="DC.contributor" content="于仕琪" scheme="assignee"><meta name="DC.date" content="2011-1-25" scheme="dateSubmitted"><meta name="DC.description" content="本发明揭示了一种基于深度图像进行人体检测的方法，包括：根据所采集的深度图像的像素提取图像特征；将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。本发明提出的一种基于深度图像进行人体检测的方法，利用深度图像的像素提取图像特征，以实现人体检测，降低了误检测率。"><meta name="DC.date" content="2011-7-13"><meta name="DC.relation" content="CN:101339607:A" scheme="references"><meta name="DC.relation" content="CN:101398886:A" scheme="references"><meta name="DC.relation" content="CN:101625762:A" scheme="references"><meta name="DC.relation" content="US:20040037450:A1" scheme="references"><meta name="DC.relation" content="WO:2003073359:A2" scheme="references"><meta name="citation_patent_publication_number" content="CN:102122390:A"><meta name="citation_patent_application_number" content="CN:201110026465"><link rel="canonical" href="https://www.google.com/patents/CN102122390A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102122390A?cl=zh"/><meta name="title" content="专利 CN102122390A - 基于深度图像进行人体检测的方法"/><meta name="description" content="本发明揭示了一种基于深度图像进行人体检测的方法，包括：根据所采集的深度图像的像素提取图像特征；将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。本发明提出的一种基于深度图像进行人体检测的方法，利用深度图像的像素提取图像特征，以实现人体检测，降低了误检测率。"/><meta property="og:title" content="专利 CN102122390A - 基于深度图像进行人体检测的方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102122390A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102122390A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=tFSXBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102122390A&amp;usg=AFQjCNFLlRIv_4OSQfz3DYIP2DC5rcVaHQ" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/c991ce5aceecf8d7a040/CN102122390A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/c991ce5aceecf8d7a040/CN102122390A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102122390A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102122390A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102122390A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102122390B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102122390A?cl=zh" style="display:none"><span itemprop="description">本发明揭示了一种基于深度图像进行人体检测的方法，包括：根据所采集的深度图像的像素提取图像特征；将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。本发明提出的一种基于深度图像进行人体检测...</span><span itemprop="url">https://www.google.com/patents/CN102122390A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102122390A - 基于深度图像进行人体检测的方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102122390A - 基于深度图像进行人体检测的方法" title="专利 CN102122390A - 基于深度图像进行人体检测的方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102122390 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110026465</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2011年7月13日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年1月25日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2011年1月25日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102122390B?hl=zh-CN&amp;cl=zh">CN102122390B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110026465.0, </span><span class="patent-bibdata-value">CN 102122390 A, </span><span class="patent-bibdata-value">CN 102122390A, </span><span class="patent-bibdata-value">CN 201110026465, </span><span class="patent-bibdata-value">CN-A-102122390, </span><span class="patent-bibdata-value">CN102122390 A, </span><span class="patent-bibdata-value">CN102122390A, </span><span class="patent-bibdata-value">CN201110026465, </span><span class="patent-bibdata-value">CN201110026465.0</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E4%BA%8E%E4%BB%95%E7%90%AA%22">于仕琪</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E4%BA%8E%E4%BB%95%E7%90%AA%22">于仕琪</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102122390A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102122390A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102122390A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=tFSXBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110026465&amp;usg=AFQjCNGhqOqAQq5b0oBoF-eYDrI_u4lytw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=tFSXBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102122390A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEjAtc43xgg2ncrB8dCyGG1Bcd_GQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT104269568" lang="ZH" load-source="patent-office">基于深度图像进行人体检测的方法</invention-title>
      </span><br><span class="patent-number">CN 102122390 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA86172632" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明揭示了一种基于深度图像进行人体检测的方法，包括：根据所采集的深度图像的像素提取图像特征；将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。本发明提出的一种基于深度图像进行人体检测的方法，利用深度图像的像素提取图像特征，以实现人体检测，降低了误检测率。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(10)</span></span></div><div class="patent-text"><div mxw-id="PCLM37790080" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种基于深度图像进行人体检测的方法，其特征在于，包括：根据所采集的深度图像的像素提取图像特征；将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.如权利要求1所述的基于深度图像进行人体检测的方法，其特征在于，所述根据所 采集的深度图像的像素提取图像特征包括：对所述深度图像的像素进行深度差运算或局部二值运算。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.如权利要求2所述的基于深度图像进行人体检测的方法，其特征在于，所述对深度 图像的像素进行深度差运算包括：根据下列程式计算各像素的深度差：Gx (x, y) = D(x+l,y)-D(x-l,y),Gy(x,y) = D (x，y+1)-D (x，y_l)，所述(x，y)为(χ, y)位置的X方向深度差，Gy (x，y)为（x，y)位置的Y方向深度差，D(x，y)为（x，y)位置的 深度值；统计所有像素的深度差，形成图像特征。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.如权利要求3所述的基于深度图像进行人体检测的方法，其特征在于，所述统计所 有像素的深度差包括：以预设的角度值为单位，累加各单位的深度差；组合所有单位的深度差。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.如权利要求1至4中任一项所述的基于深度图像进行人体检测的方法，其特征在于， 在执行所述根据所采集的深度图像的像素提取图像特征之前，还包括：在所述深度图像中选择一或多个区域；或检测所述深度图像中变化的区域，在该变化 的区域中选择一或多个区域。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.如权利要求5所述的基于深度图像进行人体检测的方法，其特征在于，所述将图像 特征输入预设的分类模型包括：当选择的区域为多个时，分别将各区域的图像特征输入预设的分类模型，以得出各区 域中是否包含人体。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.如权利要求6所述的基于深度图像进行人体检测的方法，其特征在于，在执行分别 将各区域的图像特征输入预设的分类模型之后，还包括：保存包含人体的区域的位置和大小；合并所有包含人体的区域的位置和大小，得到所述人体的信息，所述信息包括人体的 位置、大小和/或数量。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.如权利要求1至4中任一项所述的基于深度图像进行人体检测的方法，其特征在于， 在执行所述根据所采集的深度图像的像素提取图像特征之前，还包括：采集深度图像，该深度图像包括人体区域和非人体区域；根据所述人体区域和非人体区域的像素提取图像特征；根据提取的图像特征进行训练并建模，获得所述分类模型。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.如权利要求1至4中任一项所述的基于深度图像进行人体检测的方法，其特征在于， 所述图像特征包括图像纹理特征。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10.如权利要求1至4中任一项所述的基于深度图像进行人体检测的方法，其特征在 于，所述分类模型为支持向量机模型。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES43369120" lang="ZH" load-source="patent-office" class="description">
    <p>基于深度图像进行人体检测的方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及到图像处理领域，特别涉及到一种基于深度图像进行人体检测的方法。</p>
    <p>背景技术</p>
    <p>[0002]	目前对图像进行人体检测的算法都是在普通图像（二维数据）上进行，图像中的 每个像素点的值表示的是物体的亮度，如白色衣服的亮度比黄色皮肤的亮度要高。因此普 通图像的像素值只与物体表面颜色、反射的光线强度或发射的光线强度有关，与物体到相 机的距离无直接关系，因此而造成的缺陷是难以克服光照变化和复杂背景干扰，普通图像 中因光照引起的人的阴影、图像背景上的复杂纹理（如墙上画的人体形状），都会对人体检 测造成干扰，将一些非人体区域误识别为人体，误检测率高。</p>
    <p>发明内容</p>
    <p>[0003]	本发明的主要目的是提出一种基于深度图像进行人体检测的方法，利用深度图像 的像素提取图像特征，以实现人体检测，降低了误检测率。</p>
    <p>[0004]	本发明提出的一种基于深度图像进行人体检测的方法，包括：</p>
    <p>[0005]	根据所采集的深度图像的像素提取图像特征；</p>
    <p>[0006]	将所述图像特征输入预设的分类模型，以得出所述深度图像是否包含人体。</p>
    <p>[0007]	优选地，所述根据所采集的深度图像的像素提取图像特征包括：</p>
    <p>[0008]	对所述深度图像的像素进行深度差运算或局部二值运算。</p>
    <p>[0009]	优选地，所述对深度图像的像素进行深度差运算包括：</p>
    <p>[0010]	根据下列程式计算各像素的深度差：</p>
    <p>[0011]	Gx (x, y) = D(x+l,y)-D(x-l,y),Gy(x,y) = D (x，y+1)-D (x，y_l)，所述(x，y)为 (x，y)位置的X方向深度差，Gy(x，y)为（x，y)位置的Y方向深度差，D(x，y)为（x，y)位 置的深度值；</p>
    <p>[0012]	统计所有像素的深度差，形成图像特征。</p>
    <p>[0013]	优选地，所述统计所有像素的深度差包括：</p>
    <p>[0014]	以预设的角度值为单位，累加各单位的深度差；</p>
    <p>[0015]	组合所有单位的深度差。</p>
    <p>[0016]	优选地，在执行所述根据所采集的深度图像的像素提取图像特征之前，还包括：</p>
    <p>[0017]	在所述深度图像中选择一或多个区域；或检测所述深度图像中变化的区域，在该 变化的区域中选择一或多个区域。</p>
    <p>[0018]	优选地，所述将图像特征输入预设的分类模型包括：</p>
    <p>[0019]	当选择的区域为多个时，分别将各区域的图像特征输入预设的分类模型，以得出 各区域中是否包含人体。</p>
    <p>[0020]	优选地，在执行分别将各区域的图像特征输入预设的分类模型之后，还包括：[0021]	保存包含人体的区域的位置和大小；</p>
    <p>[0022]	合并所有包含人体的区域的位置和大小，得到所述人体的信息，所述信息包括人 体的位置、大小和/或数量。</p>
    <p>[0023]	优选地，在执行所述根据所采集的深度图像的像素提取图像特征之前，还包括：</p>
    <p>[0024]	采集深度图像，该深度图像包括人体区域和非人体区域；</p>
    <p>[0025]	根据所述人体区域和非人体区域的像素提取图像特征；</p>
    <p>[0026]	根据提取的图像特征进行训练并建模，获得所述分类模型。</p>
    <p>[0027]	优选地，所述图像特征包括图像纹理特征。</p>
    <p>[0028]	优选地，所述分类模型为支持向量机模型。</p>
    <p>[0029]	本发明提出的一种基于深度图像进行人体检测的方法，在深度图像（三维数据） 上进行人体检测，基于深度图像的像素建立模型，由于深度图像的像素值只与距离有关，与 物体表面的亮度和颜色无关，因此本发明可以去除光照变化和复杂背景的干扰，使人体检 测准确率高，误检测率低。</p>
    <p>附图说明</p>
    <p>[0030]	图1为本发明一种基于深度图像进行人体检测的方法一实施例的流程示意图；</p>
    <p>[0031]	图2为本发明一种基于深度图像进行人体检测的方法一实施例中计算深度差的 流程示意图；</p>
    <p>[0032]	图3为本发明一种基于深度图像进行人体检测的方法一实施例的深度差的示意 图；</p>
    <p>[0033]	图4为本发明一种基于深度图像进行人体检测的方法一实施例的深度差直方图；</p>
    <p>[0034]	图5为本发明一种基于深度图像进行人体检测的方法一实施例中统计深度图像 所有像素的深度差的流程示意图；</p>
    <p>[0035]	图6为本发明一种基于深度图像进行人体检测的方法又一实施例的流程示意图；</p>
    <p>[0036]	图7为本发明一种基于深度图像进行人体检测的方法另一实施例的流程示意图；</p>
    <p>[0037]	图8为本发明一种基于深度图像进行人体检测的方法再一实施例的流程示意图。</p>
    <p>[0038]	本发明目的的实现、功能特点及优点将结合实施例，参照附图做进一步说明。</p>
    <p>具体实施方式</p>
    <p>[0039]	应当理解，此处所描述的具体实施例仅仅用以解释本发明，并不用于限定本发明。</p>
    <p>[0040]	参照图1，提出本发明一种基于深度图像进行人体检测的方法一实施例，包括：</p>
    <p>[0041]	步骤S101，根据所采集的深度图像的像素提取图像特征；</p>
    <p>[0042]	利用深度相机或激光测距扫描仪等设备采集深度图像。用深度相机或激光测距 扫描仪进行拍摄，可记录下环境的三维数据，也即深度信息，深度信息以深度图像（三维数 据）的方式存储。深度图像中每个像素点的值表示物体到相机的距离，像素值越大，表示物 体离相机越远，深度图像的像素值只与物体到相机的距离有关，与物体表面的亮度和颜色 无关。根据采集到的深度图像的像素提取图像特征，提取的图像特征通常为图像纹理特征， 可以表现为深度差直方图或局部二值方图，该深度差直方图或局部二值方图表示为一个向 量（数组）。[0043]	步骤S102，将图像特征输入预设的分类模型，以得出深度图像是否包含人体。</p>
    <p>[0044]	将提取的图像特征输入预设的分类模型，分类模型可以是支持向量机模型，也可 以是AdaBoost模型等，以判断深度图像中是否包括人体。所述人体可以是整体或局部如 头、肩、人体上半身等。分类模型为预先设置，通常是预设采集一个同时包含人体和环境的 深度图像，对该深度图像提取图像特征进行训练并建模获得。</p>
    <p>[0045]	本发明提出的一种基于深度图像进行人体检测的方法，在深度图像（三维数据） 上进行人体检测，基于深度图像的像素建立模型，由于深度图像的像素值只与距离有关，与 物体表面的亮度和颜色无关，因此本发明可以去除光照变化和复杂背景的干扰，使人体检 测准确率高，误检测率低。本发明能够自动判断周围环境中是否有人存在，具有更高的智能 性，可以应用于汽车、机器人或监控系统等多个领域，提高系统的智能性。</p>
    <p>[0046]	在本发明一种基于深度图像进行人体检测的方法一实施例中，步骤SlOl可包括：</p>
    <p>[0047]	对深度图像的像素进行深度差运算或局部二值运算。</p>
    <p>[0048]	提取深度图像的图像特征有多种方式，较佳方式是根据深度图像的像素进行深度 差运算，也可采用局部二值运算，局部二值运算是按照一定的规则将整幅深度图像划分为N 个窗口，对这N个窗口中的每一个窗口再按照一个统一的阈值T将该窗口内的像素划分为 两部分，进行二值化处理。除深度差运算和局部二值运算以外的其它可利用深度图像的像 素提取图像特征的方式也适用于本发明。</p>
    <p>[0049]	本实施例中，采用深度差运算或局部二值运算提取深度图像的图像特征，较好的 实现了图像特征的提取。</p>
    <p>[0050]	参照图2，在本发明一种基于深度图像进行人体检测的方法一实施例中，对深度图 像的像素进行深度差运算可包括：</p>
    <p>[0051]	步骤S1011，根据下列程式计算各像素的深度差：</p>
    <p>[0052]	Gx (x, y) = D (x+1, y)-D (χ-l, y), Gy (χ, y) = D (χ，y+1)-D (χ，y_l)，所述(χ，y)为 (x，y)位置的X方向深度差，Gy(x，y)为（x，y)位置的Y方向深度差，D(x，y)为（x，y)位 置的深度值；</p>
    <p>[0053]	将深度图像分割成M*N个区域；M和N为大于或等于1的自然数。对每一区域里 的所有像素进行深度差计算。深度差有两个方向，X方向（横向）和Y方向（纵向）的深 度差，分别为：</p>
    <p>[0054]	Gx (x, y) = D (χ+1，y) -D (χ-1，y)</p>
    <p>[0055]	Gy (x，y) = D (χ, y+1) -D (χ, y-1)</p>
    <p>[0056]	其中，(ix(X，y)为（x，y)位置的X方向深度差，Gy (x，y)为（x，y)位置的Y方向深 度差，D(x, y)为（X，y)位置的深度值，计算得出的深度差可以用如图3所示带有方向和大 小的向量表示，可用如图4所示的深度差直方图直观表示。</p>
    <p>[0057]	步骤S1012，统计所有像素的深度差，形成图像特征。</p>
    <p>[0058]	统计每一区域所有像素的深度差，再将M*N个区域的深度差直方图连成一个大的 向量（数组），形成深度图像的图像特征。</p>
    <p>[0059]	本实施例中，设置深度图像的像素的深度差运算公式，并以深度差直方图形式直 观表现。</p>
    <p>[0060]	参照图5，在本发明一种基于深度图像进行人体检测的方法一实施例中，步骤S1012可包括：</p>
    <p>[0061]	步骤S10121，以预设的角度值为单位，累加各单位的深度差；</p>
    <p>[0062]	步骤S10122，组合所有单位的深度差。</p>
    <p>[0063]	由于深度差的方向范围在0&#12316;360度，可以以一预设值例如40度（或其他值）为 一个范围，对不同角度的深度差进行统计，统计出的深度差直方图可表示为一个向量（数 组）。然后将这些直方图连成一个大的向量（数组），得到最终的深度图像的深度差直方图。</p>
    <p>[0064]	本实施例中，以预设角度统计深度图像各像素的深度差并组合。</p>
    <p>[0065]	参照图6，提出本发明一种基于深度图像进行人体检测的方法又一实施例，在上述 实施例中，在执行步骤SlOl之前，还包括：</p>
    <p>[0066]	步骤S1001，在深度图像中选择一或多个区域；或检测深度图像中变化的区域，在 该变化的区域中选择一或多个区域。</p>
    <p>[0067]	当深度图像的数据量大时，可在深度图像中选择一或多个区域，分别提取所选择 的一或多个区域的图像特征，通常的方式是以一预设的方式对深度图像进行扫描，例如从 左上角开始，以一预设的区域范围进行扫描，以进行后续的特征提取，该方法全面精确，避 免遗漏。或者可检测深度图像中变化的区域，假设该变化的区域为人体，再在该变化的区域 中选择一或多个区域提取图像特征，该方法实现快速判断，提高了效率。</p>
    <p>[0068]	本实施例中，可根据需要采用按区域提取图像特征的方式。</p>
    <p>[0069]	在本发明一种基于深度图像进行人体检测的方法又一实施例中，将图像特征输入 预设的分类模型包括：</p>
    <p>[0070]	当选择的区域为多个时，分别将各区域的图像特征输入预设的分类模型，以得出 各区域中是否包含人体。</p>
    <p>[0071]	本实施例中，当选择多个区域对深度图像提取图像特征时，分别判断各区域的图 像特征是否包含人体。</p>
    <p>[0072]	参照图7，提出本发明一种基于深度图像进行人体检测的方法另一实施例，在上述 实施例中，在执行步骤S102之后，还包括：</p>
    <p>[0073]	步骤S103，保存包含人体的区域的位置和大小；</p>
    <p>[0074]	当某一选择的区域中包含人体时，保存该区域的位置和大小。</p>
    <p>[0075]	步骤S104，合并所有包含人体的区域的位置和大小，得到人体的信息，该信息包括 人体的位置、大小和/或数量。</p>
    <p>[0076]	扫描完所有选择的区域后，合并所有包含人体的区域的位置和大小，也可进一步 对包含人体的区域进行分析和去噪，上述合并的操作可以去除多个区域中重复的部分，得 到最终的人体的信息，该信息包括人体的位置、大小和/或数量。</p>
    <p>[0077]	合并过程可如下：</p>
    <p>[0078]	假设扫描深度图像后，得到包含人体的区域Rl，R2，. . .，to。这些区域都是矩形， 矩形可用（x，y，w，h)表示，其中（x，y)表示左上角坐标，w和h分别表示宽度和高度。对于 任意两个包含人体的矩形区域Ri和Rj，如上图所示，如果两个矩形区域重叠的面积超过任 一矩形区域面积的一定比例（如60%)，则认为这两个矩形区域都是指向同一个人体。将 两个矩形区域合并为一个矩形，合并得到的新矩形区域为&amp;_，7_，《_，11_)，其中：x_ =</p>
    <p>Xi+Xj/2, ynew = Yi+yj/2, Wnew = Wi+w/2, hnew = Vh/2。[0079]	本实施例中，在判断区域是否有人体后，还保存该区域的位置和大小，并进一步作 优化处理，得到关于深度图像中关于人体的准确信息。</p>
    <p>[0080]	参照图8，提出本发明一种基于深度图像进行人体检测的方法再一实施例，在上述 实施例中，在执行步骤SlOl之前，还包括：</p>
    <p>[0081]	步骤S98，采集深度图像，该深度图像包括人体区域和非人体区域；</p>
    <p>[0082]	采集作为训练用的深度图像，该深度图像包括人体区域和非人体区域即环境区 域。</p>
    <p>[0083]	步骤S99，根据人体区域和非人体区域的像素提取图像特征；</p>
    <p>[0084]	从深度图像中标注出人体区域，将人体区域裁剪出，裁剪出的人体深度图像，可裁 剪出大量人体区域深度图像，作为训练的正样本。再从深度图像中标注出非人体区域即环 境区域，将非人体区域裁剪出，可裁剪出大量非人体区域，作为训练的负样本。将所有的正 样本和负样本归一化到相同的宽度和高度。对所有正负样本进行图像特征的提取操作，以 深度差运算为例，每个样本提取到一个深度差直方图。</p>
    <p>[0085]	步骤S100，根据提取的图像特征进行训练并建模，获得分类模型。</p>
    <p>[0086]	将所有样本的深度差直方图输入机器学习分类模型中（比如支持向量机模型）， 进行模型训练，最后得到一个分类模型，为人体检测作准备。</p>
    <p>[0087]	本实施例中，对三维物体的表面建立分类模型为人体检测作准备，当人体进入拍 摄场景中时，可自动将人体从环境中分离出来。</p>
    <p>[0088]	在上述实施例中，图像特征包括但不限定于图像纹理特征。图像纹理特征既包括 通常意义上物体表面的细小变化即物体表面上所呈现凹凸不平的沟纹，同时也包括在物体 的光滑表面上的彩色图案。深度图像的图像纹理特征是指局部（小范围）的深度变化。</p>
    <p>[0089]	在上述实施例中，分类模型为支持向量机模型，也可以是其它适用于本发明的机 器学习分类模型。</p>
    <p>[0090]	以上所述仅为本发明的优选实施例，并非因此限制本发明的专利范围，凡是基于 本发明说明书及附图内容所作的等效结构或等效流程变换，或直接或间接运用在其他相关 的技术领域，均同理包括在本发明的专利保护范围内。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101339607A?cl=zh">CN101339607A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年8月15日</td><td class="patent-data-table-td patent-date-value">2009年1月7日</td><td class="patent-data-table-td ">北京中星微电子有限公司</td><td class="patent-data-table-td ">人脸识别方法及系统、人脸识别模型训练方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101398886A?cl=zh">CN101398886A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年3月17日</td><td class="patent-data-table-td patent-date-value">2009年4月1日</td><td class="patent-data-table-td ">杭州大清智能技术开发有限公司</td><td class="patent-data-table-td ">一种基于双目被动立体视觉的快速三维人脸识别方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101625762A?cl=zh">CN101625762A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年6月19日</td><td class="patent-data-table-td patent-date-value">2010年1月13日</td><td class="patent-data-table-td ">深圳市中瀛鑫科技发展有限公司</td><td class="patent-data-table-td ">目标分割方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20040037450">US20040037450</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2002年8月22日</td><td class="patent-data-table-td patent-date-value">2004年2月26日</td><td class="patent-data-table-td ">Bradski Gary R.</td><td class="patent-data-table-td ">Method, apparatus and system for using computer vision to identify facial characteristics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003073359A2?cl=zh">WO2003073359A2</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2003年2月26日</td><td class="patent-data-table-td patent-date-value">2003年9月4日</td><td class="patent-data-table-td ">Canesta, Inc.</td><td class="patent-data-table-td ">Method and apparatus for recognizing objects</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521646A?cl=zh">CN102521646A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月11日</td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td ">浙江捷尚视觉科技有限公司</td><td class="patent-data-table-td ">基于深度信息聚类的复杂场景人数统计算法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521646B?cl=zh">CN102521646B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月11日</td><td class="patent-data-table-td patent-date-value">2015年1月21日</td><td class="patent-data-table-td ">浙江捷尚视觉科技股份有限公司</td><td class="patent-data-table-td ">基于深度信息聚类的复杂场景人数统计方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103164894A?cl=zh">CN103164894A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年12月8日</td><td class="patent-data-table-td patent-date-value">2013年6月19日</td><td class="patent-data-table-td ">鸿富锦精密工业（深圳）有限公司</td><td class="patent-data-table-td ">票闸控制装置及方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103345625A?cl=zh">CN103345625A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年7月16日</td><td class="patent-data-table-td patent-date-value">2013年10月9日</td><td class="patent-data-table-td ">江苏云知智能科技有限公司</td><td class="patent-data-table-td ">一种三维图像分析方法及系统</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=tFSXBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009620000">G06K9/62</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=tFSXBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0007000000">G06T7/00</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2011年7月13日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2011年8月24日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年11月14日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Granted</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/c991ce5aceecf8d7a040/CN102122390A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E4%BA%BA%E4%BD%93%E6%A3%80%E6%B5%8B%E7%9A%84.pdf?id=tFSXBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U1bQ61KOiT8GuzOMEoZWBZu-Mt9og"},"sample_url":"https://www.google.com/patents/reader?id=tFSXBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>