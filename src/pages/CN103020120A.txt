<!DOCTYPE html><html><head><title>专利 CN103020120A - 一种基于超图的图像混合摘要生成方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于超图的图像混合摘要生成方法"><meta name="DC.contributor" content="唐金辉" scheme="inventor"><meta name="DC.contributor" content="李&#26107;先" scheme="inventor"><meta name="DC.contributor" content="南京理工大学" scheme="assignee"><meta name="DC.date" content="2012-11-16" scheme="dateSubmitted"><meta name="DC.description" content="本发明提供了一种基于超图的图像混合摘要生成方法，具体包括：步骤1：输入原始图像；步骤2：输入标签列表；步骤3：抽取视觉特征；步骤4：建立超图；步骤5：超图分割；步骤6：选取混合摘要。本发明使用的超图模型，不仅可以利用图像与图像、标签与标签之间的同质关系，同时还可以利用图像与标签之间的异质关系；本发明提出的选取图像摘要和标签摘要的方法，同时考虑了语义和视觉的代表性，选取的图像摘要和标签摘要能够较好地代表所属分组。"><meta name="DC.date" content="2013-4-3"><meta name="DC.relation" content="CN:101853299:A" scheme="references"><meta name="DC.relation" content="US:20090290802:A1" scheme="references"><meta name="citation_reference" content="IAN SIMON等: &quot;Scene Summarization for Online Image Collections&quot;, 《PROCEEDINGS OF THE IEEE 11TH INTERNATIONAL CONFERENCE ON COMPUTER VISION》, 31 December 2007 (2007-12-31), pages 1 - 8, XP031194352"><meta name="citation_reference" content="M.REGE等: &quot;CO-CLUSTERING IMAGE FEATURES AND SEMANTIC CONCEPTS&quot;, 《PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING》, 31 December 2006 (2006-12-31), pages 137 - 140, XP031048592"><meta name="citation_reference" content="李&#26107;先等: &quot;基于视觉显著性近邻投票的标签排序方法&quot;, 《南京理工大学学报》, vol. 36, no. 4, 31 August 2012 (2012-08-31), pages 561 - 566"><meta name="citation_patent_publication_number" content="CN:103020120:A"><meta name="citation_patent_application_number" content="CN:201210464502"><link rel="canonical" href="https://www.google.com/patents/CN103020120A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN103020120A?cl=zh"/><meta name="title" content="专利 CN103020120A - 一种基于超图的图像混合摘要生成方法"/><meta name="description" content="本发明提供了一种基于超图的图像混合摘要生成方法，具体包括：步骤1：输入原始图像；步骤2：输入标签列表；步骤3：抽取视觉特征；步骤4：建立超图；步骤5：超图分割；步骤6：选取混合摘要。本发明使用的超图模型，不仅可以利用图像与图像、标签与标签之间的同质关系，同时还可以利用图像与标签之间的异质关系；本发明提出的选取图像摘要和标签摘要的方法，同时考虑了语义和视觉的代表性，选取的图像摘要和标签摘要能够较好地代表所属分组。"/><meta property="og:title" content="专利 CN103020120A - 一种基于超图的图像混合摘要生成方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN103020120A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN103020120A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=dgftBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN103020120A&amp;usg=AFQjCNHyET31lncK3da_bPYIz8lrWiD4Kg" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/1d5ebf932a1411878222/CN103020120A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/1d5ebf932a1411878222/CN103020120A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN103020120A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN103020120A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN103020120A?cl=zh" style="display:none"><span itemprop="description">本发明提供了一种基于超图的图像混合摘要生成方法，具体包括：步骤1：输入原始图像；步骤2：输入标签列表；步骤3：抽取视觉特征；步骤4：建立超图；步骤5：超图分割；步骤6：选取混合摘要。本发明使用的超图模型，不仅...</span><span itemprop="url">https://www.google.com/patents/CN103020120A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN103020120A - 一种基于超图的图像混合摘要生成方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN103020120A - 一种基于超图的图像混合摘要生成方法" title="专利 CN103020120A - 一种基于超图的图像混合摘要生成方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN103020120 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210464502</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年4月3日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年11月16日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年11月16日</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210464502.0, </span><span class="patent-bibdata-value">CN 103020120 A, </span><span class="patent-bibdata-value">CN 103020120A, </span><span class="patent-bibdata-value">CN 201210464502, </span><span class="patent-bibdata-value">CN-A-103020120, </span><span class="patent-bibdata-value">CN103020120 A, </span><span class="patent-bibdata-value">CN103020120A, </span><span class="patent-bibdata-value">CN201210464502, </span><span class="patent-bibdata-value">CN201210464502.0</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%94%90%E9%87%91%E8%BE%89%22">唐金辉</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9D%8E%E6%97%BB%E5%85%88%22">李&#26107;先</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%8D%97%E4%BA%AC%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%22">南京理工大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN103020120A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103020120A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN103020120A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#npl-citations">非专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (2)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=dgftBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210464502&amp;usg=AFQjCNHyPBu004Z56Y39KaIibiloAU44tg"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=dgftBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D103020120A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEyF8n2YIWQmEsQiUGRn6NbCFtuDw"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT122958468" lang="ZH" load-source="patent-office">一种基于超图的图像混合摘要生成方法</invention-title>
      </span><br><span class="patent-number">CN 103020120 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA109838606" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明提供了一种基于超图的图像混合摘要生成方法，具体包括：步骤1：输入原始图像；步骤2：输入标签列表；步骤3：抽取视觉特征；步骤4：建立超图；步骤5：超图分割；步骤6：选取混合摘要。本发明使用的超图模型，不仅可以利用图像与图像、标签与标签之间的同质关系，同时还可以利用图像与标签之间的异质关系；本发明提出的选取图像摘要和标签摘要的方法，同时考虑了语义和视觉的代表性，选取的图像摘要和标签摘要能够较好地代表所属分组。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(5)</span></span></div><div class="patent-text"><div mxw-id="PCLM52451411" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种基于超图的图像混合摘要生成方法，其特征在于，包括以下步骤：步骤1、输入N张原始图像，每一张原始图像应满足以下条件：附带的标签个数不少于I个；步骤2、输入标签列表：.2.1)输入原始图像的标签列表：每一张原始图像附带的标签的集合，称为该张原始图像的标签列表；.2.2)从所有原始图像的标签列表中统计出独立标签集合L = (I1,12，. . .，Ii,. . .，1M}， 其中独立标签集合L中任意两个标签I都不重复,M为独立标签的个数；步骤3、抽取原始图像的视觉特征集合：抽取原始图像的视觉特征集合《F ,Ψ = [F1,F2,...,Fi,...,Fn} A = [fU’U}，其中 k表示 k种视觉特征，f 表示第 i张图像的第j种视觉特征；步骤4、建立超图，建立超图的过程包括以下两个步骤：.4.1)建立超图的顶点集合V，顶点集合V包括V1，V2 =V1表示第I种类型的顶点即原始图像的视觉特征集1、厂V2表示第2种类型的顶点即原始图像的独立标签集合L ：.4.	2)建立超图的超边集合E，超边集合E包括E1，E2, E3 =E1表示第I种超边，连接的是 V1类型的顶点；E2表示第2种超边，连接的是V2类型的顶点；E3表示第3种超边，连接的是 V1和V2类型的顶点；步骤5、使用超图谱分解技术对超图的顶点集合V进行分割，将超图的顶点集合V分割成若干组；步骤6、选取摘要，选取摘要的过程包括以下两个步骤：.6.1)在每一组中选取m个V2类型的顶点对应的独立标签，作为该组的标签摘要集合T ；.6.	2)在每一组中选取η个V1类型的顶点对应的原始图像，作为该组的图像摘要集合I。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的基于超图的图像混合摘要生成方法，其特征在于：步骤3中所述的视觉特征包括颜色直方图、颜色相关图、边缘方向直方图、小波纹理和颜色矩。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1所述的基于超图的图像混合摘要生成方法，其特征在于：步骤4.	2 中所述的E\E2，E3分别定义如下：1)	E1 =	4，…，4丨，超边e丨定义为：1-Y =U WF1-FjW1^m1 -"./=1 &#8212; I O,丨丨/';-/;^’//1V	JTH1表示阈值，I I &#183; I |2表示2-范数；4的权重w丨定义为：2)	t	超边&lt;定义为：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00031.png"> <img id="icf0001" file="CN103020120AC00031.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00031.png" class="patent-full-image" alt="Figure CN103020120AC00031"> </a> </div>TH2表示阈值，Clij表示标签Ii和标签L之间的关联程度，定义为： <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00032.png"> <img id="icf0002" file="CN103020120AC00032.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00032.png" class="patent-full-image" alt="Figure CN103020120AC00032"> </a> </div>f(li)和f (Ij)表示N张原始图像中分别包含标签Ii和标签Ij的图像数量，f(li; Ij) 表示N张原始图像中同时包括标签Ii和标签Ij的图像数量；ef的权重w；2定义为：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00033.png"> <img id="icf0003" file="CN103020120AC00033.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00033.png" class="patent-full-image" alt="Figure CN103020120AC00033"> </a> </div>3)五3	，超边&lt;定义为：3IM _ / I,第/张图像的标签列表中有第/个标签 eIfh=1-\o,第/张图像的标签列表中没有&#31620;/个标签e的权重wf定义为= I。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求1所述的基于超图的图像混合摘要生成方法，其特征在于：步骤6.1 中所述的在每一组中选取m个V2类型的顶点对应的独立标签，作为该组的标签摘要集合T， 具体如下：令s(l，c)表示标签I描述第c类的代表性分数，则s (l，c)可定义为：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00034.png"> <img id="icf0004" file="CN103020120AC00034.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00034.png" class="patent-full-image" alt="Figure CN103020120AC00034"> </a> </div>其中， K(I)表示与标签I共同出现的标签中，频次最高的k个标签的集合；1表示K(I)的所有标签中出现频次最闻的标签；C(l, I)表不标签I和标签I之间的共同出现的频次； Threshold表示共同出现频次的阈值；最后将每个标签的代表性分数s (1，c)由高到低来排序，则可以选择m个代表性分数最高的标签作为该组的标签摘要集合T。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求1所述的基于超图的图像混合摘要生成方法，其特征在于：步骤6.	2 中所述的在每一组中选取η个V1类型的顶点对应的原始图像，作为该组的图像摘要集合I， 具体如下：令P (F，t，c)表示视觉特征F对应的原始图像描述第c类的代表性分数，t为视觉特征F对应的原始图像附带的标签集合，则P (F，t，c)可定义如下：<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00035.png"> <img id="icf0005" file="CN103020120AC00035.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AC00035.png" class="patent-full-image" alt="Figure CN103020120AC00035"> </a> </div>其中，]?表示第C类中的平均视觉特征，d(t，T)表示标签集合t与标签摘要集合T的关联程度，a表示视觉代表性和语义代表性的平衡因子；最后在该类中，将每张图像的代表性分数按由高到低的顺序进行排序，选择η个代表性分数最高的图像作为该类的图像范例I。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES59514002" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于超图的图像混合摘要生成方法</p>
    <p>一、技术领域</p>
    <p>[0001]	本发明属于图像处理技术领域，特别是一种基于超图的图像混合摘要生成方法。</p>
    <p>二、背景技术</p>
    <p>[0002]	随着各种数字化成像设备（如数码相机,手机等）的普及、大容量存储设备成本的降低以及互联网技术的革新，现代生活中多媒体信息（包括图像、视频等）的数量在迅猛增长，特别是数字图像充斥在人们日常生活的方方面面，并且随之诞生了一批图像分享网站(如Flick^Picasa等）。但是在这些网站上，存在着明显的缺陷：海量的图像无法得到有效的组织，给用户寻找想要的图片以及浏览图片集合造成了不便。 </p>
    <p>[0003]	为了更好地组织图像和将图像有效可视化，研究人员提出了图像混合摘要技术。所谓的图像混合摘要，是指从海量的图像集合中选取少量具有代表性的图像和语义标签作为整个集合的代表，方便用户快速浏览和查询。这里的语义标签指的是对图像内容进行描述的文本单词，语义标签简称为标签。</p>
    <p>[0004]	近年来，研究人员提出了许多针对互联网图像的图像混合摘要技术：</p>
    <p>[0005]	文献 I (Jaffe A, Naaman M, Tassa T, and Davis M. Generating Summaries forLargeCoIIections of Geo-Referenced Photographs.1n Proceedings of InternationalConference onfforld Wide Web, pages 853-854, 2006.) Jaff 等使用标签和地理信息来产生一个图像摘要集；</p>
    <p>[0006]	文献 2(Simon I, Snavely N, and Seitz S M. Scene Summarization for OnlineImageCollections.1n Proceedings of IEEE IIth International Conference onComputer Vision, 2007,1-8.) Simon等使用了一个贪婪k-means算法选择了一系列的典型性视图来形成场景摘要；</p>
    <p>[0007]文献	3 (Fan J, Gao Y, Luo H, Keim D A, and Li Z. A Novel Approach toEnableSemantic and Visual Image Summarization for Exploratory Image Search.1nProceedings ofMultimedia Information Retrieval, 2008, 358-365.)Fan 等首先根据图像的主题关键词形成一个主题词网络，然后再使用混合核和基于代表性的图像采用方法产生图像摘要。</p>
    <p>[0008]	尽管以上文献I&#12316;文献3的图像混合摘要生成方法取得了一些成果，但是这些方法由于所用模型的缺陷，生成图像混合摘要的过程中，只能考虑图像与图像之间的关系、标签与标签之间的关系，无法考虑完全以下三种关系：图像与图像之间的关系、标签与标签之间的关系、图像与标签之间的关系，因此生成的图像混合摘要并不理想。</p>
    <p>三、发明内容</p>
    <p>[0009]	本发明的目的在于提供一种不仅可以利用图像与图像、标签与标签之间的同质关系，同时还可以利用图像与标签之间的异质关系对图像进行有效分类的基于超图的图像混合摘要生成方法。[0010]	实现本发明目的的技术解决方案：一种基于超图的图像混合摘要生成方法，具体包括以下步骤：</p>
    <p>[0011]	步骤1、输入N张原始图像，每一张原始图像应满足以下条件：附带的标签个数不少于I个；</p>
    <p>[0012]	步骤2、输入标签列表：</p>
    <p>[0013]	2.1)输入原始图像的标签列表：每一张原始图像附带的标签的集合，称为该张原始图像的标签列表； [0014]	2. 2)从所有原始图像的标签列表中统计出独立标签集合L = (IijI2,. . . ,Ii,...,1M}，其中独立标签集合L中任意两个标签I都不重复，M为独立标签的个数；</p>
    <p>[0015]	步骤3、抽取原始图像的视觉特征集合：抽取原始图像的视觉特征集合《F,Ψ = {F{,F2,...,Ft,...,Fn}	，其中 k 表示 k 种视觉特征广表示第 i 张图像的第j种视觉特征；</p>
    <p>[0016]	步骤4、建立超图，建立超图的过程包括以下两个步骤：</p>
    <p>[0017]	4.1)建立超图的顶点集合V，顶点集合V包括V1，V2 =V1表示第I种类型的顶点即原始图像的视觉特征集合《F ; V2表示第2种类型的顶点即原始图像的独立标签集合L ；</p>
    <p>[0018]	4.2)建立超图的超边集合E，超边集合E包括E1，E2, E3 =E1表示第I种超边，连接的是V1类型的顶点；E2表示第2种超边，连接的是V2类型的顶点；E3表示第3种超边，连接的是V1和V2类型的顶点；</p>
    <p>[0019]	步骤5、使用超图谱分解技术对超图的顶点集合V进行分割，将超图的顶点集合V分割成若干组；</p>
    <p>[0020]	步骤6、选取摘要，选取摘要的过程包括以下两个步骤：</p>
    <p>[0021]	6.1)在每一组中选取m个V2类型的顶点对应的独立标签，作为该组的标签摘要集合T ;</p>
    <p>[0022]	6. 2)在每一组中选取η个V1类型的顶点对应的原始图像，作为该组的图像摘要集合I。</p>
    <p>[0023]	本发明与现有技术相比，其显著优点是：（1)本发明使用的超图模型，不仅可以利用图像与图像之间的关系、标签与标签之间的关系，同时还可以利用图像与标签之间的关系；（2)本发明提出的选取图像摘要和标签摘要的方法，同时考虑了语义和视觉的代表性，选取的图像摘要和标签摘要能够较好地代表所属分组。</p>
    <p>四附图说明</p>
    <p>[0024]	图1是本发明基于超图的图像混合摘要生成方法的流程图。</p>
    <p>[0025]	图2是本发明基于超图的图像混合摘要生成方法的超图示意图。</p>
    <p>[0026]	图3是本发明基于超图的图像混合摘要生成方法的效果实例图。</p>
    <p>五具体实施方式</p>
    <p>[0027]	下面结合附图对本发明作进一步详细描述。</p>
    <p>[0028]	结合图1、图2，本发明提出的基于超图的图像混合摘要生成方法，具体包括以下步骤：[0029]	步骤1、输入N张原始图像，每一张原始图像应满足以下条件：附带的标签个数不少于I个；</p>
    <p>[0030]	步骤2、输入标签列表：</p>
    <p>[0031]	2.1)输入原始图像的标签列表：每一张原始图像附带的标签的集合，称为该张原始图像的标签列表；</p>
    <p>[0032]	2. 2)从所有原始图像的标签列表中统计出独立标签集合</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00061.png"> <img id="idf0001" file="CN103020120AD00061.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00061.png" class="patent-full-image" alt="Figure CN103020120AD00061"> </a> </div>
    <p>1M}，其中独立标签集合L中任意两个标签I都不重复，M为独立标签的个数；</p>
    <p>[0033]	步骤3、抽取原始图像的视觉特征集合：抽取原始图像的视觉特征集合妒，</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00062.png"> <img id="idf0002" file="CN103020120AD00062.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00062.png" class="patent-full-image" alt="Figure CN103020120AD00062"> </a> </div>
    <p>其中1^表示1^种视觉特征，/；嚷示第1张图像的第j种视觉特征；所述的视觉特征包括颜色直方图、颜色相关图、边缘方向直方图、小波纹理和颜色矩。</p>
    <p>[0034]	步骤4、建立超图，如图2所示，超图是一种简单图的扩展，超图中的边可以连接任意个数的顶点。建立超图的过程包括以下两个步骤：</p>
    <p>[0035]	4.1)建立超图的顶点集合V，顶点集合V包括V1，V2 =V1表示第I种类型的顶点即原始图像的视觉特征集合《F; V2表示第2种类型的顶点即原始图像的独立标签集合L ；</p>
    <p>[0036]	4. 2)建立超图的超边集合E，超边集合E包括E1，E2, E3 =E1表示第I种超边，连接的是V1类型的顶点；E2表示第2种超边，连接的是V2类型的顶点；E3表示第3种超边，连接的是V1和V2类型的顶点；所述的E1，E2，E3分别定义如下：</p>
    <p>[0037]	(I)	，超边e;定义为：</p>
    <p>[0038]</p>
    <p>[0039]</p>
    <p>[0040]</p>
    <p>FjW2^TH1FjW1KTH1</p>
    <p>TH1表示阈值，I I &#183; I |2表示2-范数；e)的权重w丨定义为：</p>
    <p>(I)</p>
    <p>[0041 ] W1i =~^(4X_Fj Il2)</p>
    <p>[0042]	(2) E2 ={ef,e22,...,ef,...,e2M}，超边&lt;定义为</p>
    <p>21 V/ &#943; I, iL&gt;TH2</p>
    <p>[0043]	e；r,._,=</p>
    <p>(2)</p>
    <p>[0044]</p>
    <p>[0045]</p>
    <p>[0046]</p>
    <p>、0. du&lt;TH-	(3)</p>
    <p>TH2表示阈值，Clij表示标签Ii和标签L之间的关联程度，定义为,max(!og./.(/, )，log)) - log /(/:,. J1)</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00063.png"> <img id="idf0003" file="CN103020120AD00063.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00063.png" class="patent-full-image" alt="Figure CN103020120AD00063"> </a> </div>
    <p>(4)</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00064.png"> <img id="idf0004" file="CN103020120AD00064.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00064.png" class="patent-full-image" alt="Figure CN103020120AD00064"> </a> </div>
    <p>fdi)和f(Ij)表示N张原始图像中分别包含标签Ii和标签Ij的图像数量，f(IiIj)表示N张原始图像中同时包括标签Ii和标签Ij的图像数量；</p>
    <p>[0047] &lt;的权重W〗定义为：[0049]	(3) E3 =	...，4}，超边&lt;定义为：</p>
    <p>[0050]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00071.png"> <img id="idf0005" file="CN103020120AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00071.png" class="patent-full-image" alt="Figure CN103020120AD00071"> </a> </div>
    <p>[0051]	e丨的权重wf定义为：</p>
    <p>[0052]	wf=l 。	（7)</p>
    <p>[0053]	步骤5、使用超图谱分解技术对超图的顶点集合V进行分割，将超图的顶点集合V分割成若干组。超图分割问题通过解以下的优化问题：</p>
    <p>[0054]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00072.png"> <img id="idf0006" file="CN103020120AD00072.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00072.png" class="patent-full-image" alt="Figure CN103020120AD00072"> </a> </div>
    <p>[0056]	其中，函数f (U)表示将顶点u分到某类的概率。e是任意一条超边,U、V是e上任意两个超图顶点。d(v)表示顶点V的度，δ (e)表示超边e的度，w(e)表示超边e的权重。</p>
    <p>[0057]	定义两个矩阵，令&#169; = D^12HWDe1HrD;112 , Δ =1- O，其中I是单位矩阵，则</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00073.png"> <img id="idf0007" file="CN103020120AD00073.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00073.png" class="patent-full-image" alt="Figure CN103020120AD00073"> </a> </div>
    <p>[0059]	其中，Dv是顶点的度矩阵，De是超边的度矩阵，W是权重矩阵，H是顶点与超边的关联矩阵。Λ是半正定矩阵，Δ的最小特征值是0，对应的特征向量的是^。根据线性代数的知识，式（8)的优化问题的解就是矩阵△的最小非零特征值对应的特征向量Φ。因此，顶点集合可以分割为两个部分S= {v e VI Φ (ν)彡0}和S。= {v e VI Φ (v) &lt; 0}。</p>
    <p>[0060]</p>
    <p>另外，拉普拉斯算子△为</p>
    <p>[0061]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00074.png"> <img id="idf0008" file="CN103020120AD00074.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00074.png" class="patent-full-image" alt="Figure CN103020120AD00074"> </a> </div>
    <p>[0062]	使用（3)式，按简单图的谱分割方法分割超图。</p>
    <p>[0063]	步骤6、选取摘要，选取摘要的过程包括以下两个步骤：</p>
    <p>[0064]	6.1)在每一组中选取m个V2类型的顶点对应的独立标签，作为该组的标签摘要集合T，具体如下：</p>
    <p>[0065]令s(l，c)表示标签I描述第c类的代表性分数，则s (l，c)可定义为</p>
    <p>[0066]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00075.png"> <img id="idf0009" file="CN103020120AD00075.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00075.png" class="patent-full-image" alt="Figure CN103020120AD00075"> </a> </div>
    <p>[0069]K(I)表不与标签I共同出现的标签中，频次最1&#190;的k个标签的集合；|表不K(I)的所有标签中出现频次最高的标签；c(/，幻表示标签I和标签I?之间的共同出现的频次；Threshold表示共同出现频次的阈值；最后将每个标签的代表性分数s (1，c)由高到低来排序，则可以选择m个代表性分数最高的标签作为该组的标签摘要集合T。</p>
    <p>[0070]	6. 2)在每一组中选取η个V1类型的顶点对应的原始图像，作为该组的图像摘要集合I，具体如下：</p>
    <p>[0071]	令P (F，t，c)表示视觉特征F对应的原始图像描述第c类的代表性分数，t为视觉特征F对应的原始图像附带的标签集合，则P (F，t，c)可定义如下：</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00081.png"> <img id="idf0010" file="CN103020120AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN103020120A/CN103020120AD00081.png" class="patent-full-image" alt="Figure CN103020120AD00081"> </a> </div>
    <p>[0073]	其中，F表示第c类中的平均视觉特征，d(t，T)表示标签集合t与标签摘要集合T的关联程度，a表示视觉代表性和语义代表性的平衡因子；最后在该类中，将每张图像的代表性分数按由高到低的顺序进行排序，选择η个代表性分数最高的图像作为该类的图像范例I。 </p>
    <p>[0074]	实施例</p>
    <p>[0075]	如图3所示，使用本发明基于超图的图像混合摘要生成方法，对一批原始图像生成图像混合摘要，左边为原始图像，标签摘要包括“花朵、紫色”，“银莲花、白色”和“花丛、野地”，右边为混合摘要结果，可以看出选取的图像摘要和标签摘要能够较好地代表所属分组。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090290802">US20090290802</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年5月22日</td><td class="patent-data-table-td patent-date-value">2009年11月26日</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Concurrent multiple-instance learning for image categorization</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">非专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">参考文献</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">IAN SIMON等: "<a href='http://scholar.google.com/scholar?q="Scene+Summarization+for+Online+Image+Collections"'>Scene Summarization for Online Image Collections</a>", 《PROCEEDINGS OF THE IEEE 11TH INTERNATIONAL CONFERENCE ON COMPUTER VISION》, 31 December 2007 (2007-12-31), pages 1 - 8, XP031194352</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">M.REGE等: "<a href='http://scholar.google.com/scholar?q="CO-CLUSTERING+IMAGE+FEATURES+AND+SEMANTIC+CONCEPTS"'>CO-CLUSTERING IMAGE FEATURES AND SEMANTIC CONCEPTS</a>", 《PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING》, 31 December 2006 (2006-12-31), pages 137 - 140, XP031048592</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">李&#26107;先等: "<a href='http://scholar.google.com/scholar?q="%E5%9F%BA%E4%BA%8E%E8%A7%86%E8%A7%89%E6%98%BE%E8%91%97%E6%80%A7%E8%BF%91%E9%82%BB%E6%8A%95%E7%A5%A8%E7%9A%84%E6%A0%87%E7%AD%BE%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95"'>基于视觉显著性近邻投票的标签排序方法</a>", 《南京理工大学学报》, vol. 36, no. 4, 31 August 2012 (2012-08-31), pages 561 - 566</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=dgftBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0011600000">G06T11/60</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=dgftBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年5月1日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/1d5ebf932a1411878222/CN103020120A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E8%B6%85%E5%9B%BE%E7%9A%84%E5%9B%BE%E5%83%8F%E6%B7%B7%E5%90%88%E6%91%98%E8%A6%81.pdf?id=dgftBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U17wjmvqGrFrtjZ_i3u0v_6pJqhVA"},"sample_url":"https://www.google.com/patents/reader?id=dgftBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>