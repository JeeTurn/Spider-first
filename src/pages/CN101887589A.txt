<!DOCTYPE html><html><head><title>专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于立体视觉的实拍低纹理图像重建方法"><meta name="DC.contributor" content="达飞鹏" scheme="inventor"><meta name="DC.contributor" content="邵静" scheme="inventor"><meta name="DC.contributor" content="东南大学" scheme="assignee"><meta name="DC.date" content="2010-6-13" scheme="dateSubmitted"><meta name="DC.description" content="一种基于双目立体视觉的实拍低纹理图像重构方法，其实现步骤为：(1)使用两台摄像机从两个合适角度同时各拍摄一幅图像，取其中一幅为基准图像，另外一幅为配准图像；(2)分别对两台摄像机的内、外参数矩阵进行标定；(3)根据标定数据进行对极线校正、图像变换及高斯滤波；(4)为矫正后的两幅图像中每一点计算其自适应多边形支撑窗口，计算像素点的匹配度，得到视差空间图；(5)在全图范围内逐像素执行树形动态规划算法来完成稠密匹配；(6)采用左右一致性准则提取错误匹配点，并进行视差校正得到最终视差图；(7)根据标定数据和匹配关系计算图像上实际物点的三维坐标，从而重建出物体的三维点云。"><meta name="DC.date" content="2010-11-17"><meta name="DC.relation" content="CN:101101672:A" scheme="references"><meta name="DC.relation" content="CN:101398886:A" scheme="references"><meta name="DC.relation" content="CN:101625768:A" scheme="references"><meta name="DC.relation" content="JP:2002271818" scheme="references"><meta name="citation_patent_publication_number" content="CN:101887589:A"><meta name="citation_patent_application_number" content="CN:201010201709"><link rel="canonical" href="https://www.google.com/patents/CN101887589A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN101887589A?cl=zh"/><meta name="title" content="专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法"/><meta name="description" content="一种基于双目立体视觉的实拍低纹理图像重构方法，其实现步骤为：(1)使用两台摄像机从两个合适角度同时各拍摄一幅图像，取其中一幅为基准图像，另外一幅为配准图像；(2)分别对两台摄像机的内、外参数矩阵进行标定；(3)根据标定数据进行对极线校正、图像变换及高斯滤波；(4)为矫正后的两幅图像中每一点计算其自适应多边形支撑窗口，计算像素点的匹配度，得到视差空间图；(5)在全图范围内逐像素执行树形动态规划算法来完成稠密匹配；(6)采用左右一致性准则提取错误匹配点，并进行视差校正得到最终视差图；(7)根据标定数据和匹配关系计算图像上实际物点的三维坐标，从而重建出物体的三维点云。"/><meta property="og:title" content="专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN101887589A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN101887589A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN101887589A&amp;usg=AFQjCNGZFzYSHVfev1MPcm6APca0KFCnCg" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/d3a12f47fd7db63cf8c8/CN101887589A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/d3a12f47fd7db63cf8c8/CN101887589A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN101887589A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN101887589A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN101887589A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN101887589B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN101887589A?cl=zh" style="display:none"><span itemprop="description">一种基于双目立体视觉的实拍低纹理图像重构方法，其实现步骤为：(1)使用两台摄像机从两个合适角度同时各拍摄一幅图像，取其中一幅为基准图像，另外一幅为配准图像；(2)分别对两台摄像机的内、外参数矩阵进行标定；(3)根...</span><span itemprop="url">https://www.google.com/patents/CN101887589A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法" title="专利 CN101887589A - 一种基于立体视觉的实拍低纹理图像重建方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN101887589 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201010201709</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2010年11月17日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2010年6月13日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2010年6月13日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101887589B?hl=zh-CN&amp;cl=zh">CN101887589B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201010201709.X, </span><span class="patent-bibdata-value">CN 101887589 A, </span><span class="patent-bibdata-value">CN 101887589A, </span><span class="patent-bibdata-value">CN 201010201709, </span><span class="patent-bibdata-value">CN-A-101887589, </span><span class="patent-bibdata-value">CN101887589 A, </span><span class="patent-bibdata-value">CN101887589A, </span><span class="patent-bibdata-value">CN201010201709, </span><span class="patent-bibdata-value">CN201010201709.X</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%BE%BE%E9%A3%9E%E9%B9%8F%22">达飞鹏</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%82%B5%E9%9D%99%22">邵静</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E4%B8%9C%E5%8D%97%E5%A4%A7%E5%AD%A6%22">东南大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101887589A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101887589A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101887589A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (11),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201010201709&amp;usg=AFQjCNFATW9NEXq2Rx_WDf7hD7RORnZx1A"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D101887589A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNF5D0bufR0NJNT6FqMemtGFibX21Q"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT98856680" lang="ZH" load-source="patent-office">一种基于立体视觉的实拍低纹理图像重建方法</invention-title>
      </span><br><span class="patent-number">CN 101887589 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA81322885" lang="ZH" load-source="patent-office">
    <div class="abstract">一种基于双目立体视觉的实拍低纹理图像重构方法，其实现步骤为：(1)使用两台摄像机从两个合适角度同时各拍摄一幅图像，取其中一幅为基准图像，另外一幅为配准图像；(2)分别对两台摄像机的内、外参数矩阵进行标定；(3)根据标定数据进行对极线校正、图像变换及高斯滤波；(4)为矫正后的两幅图像中每一点计算其自适应多边形支撑窗口，计算像素点的匹配度，得到视差空间图；(5)在全图范围内逐像素执行树形动态规划算法来完成稠密匹配；(6)采用左右一致性准则提取错误匹配点，并进行视差校正得到最终视差图；(7)根据标定数据和匹配关系计算图像上实际物点的三维坐标，从而重建出物体的三维点云。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(3)</span></span></div><div class="patent-text"><ol mxw-id="PCLM33743928" lang="ZH" load-source="patent-office" class="claims">
    <li class="claim"> <div num="1" class="claim">
      <div class="claim-text">一种基于立体视觉的实拍低纹理图像重建方法，其特征在于该重建方法依次含有下列步骤：步骤1：图像获取使用双目摄像机获取图像，首先调整双目摄像机使其光轴基本平行并使左右镜角度处于合适的位置，然后同时各拍摄一幅图像，其中左镜头拍摄的为左图像，右镜头拍摄的为右图像；步骤2：摄像机标定分别对两台摄像机进行标定，获得各自的内参数矩阵AL、AR和外参数矩阵[RL&#160;tL]、[RR&#160;tR]；步骤3：对极线几何校正及图像变换采用步骤2得到的内外参运用极线校正方法对所拍摄的左右图像进行极线校正得到平行式双目视觉模型，使匹配像素对处于同一扫描线上；步骤4：计算视差空间图，步骤5：树形动态规划完成稠密匹配：以像素点px，y为基准，用箭头表示一种前驱后继关系，箭头从前驱节点出发指向后继节点；在以y为纵坐标的图像行上从最左边的像素点开始用箭头指向其右边像素点直到px，y，从最右边的像素点开始用箭头指向其左边的像素点直到px，y，然后在以x为横坐标的图像列上从最上边的像素点开始用箭头指向其下方的像素点直到px，y，从最下面的相似度开始用箭头指向其上方的像素点直到px，y；这样就构造了以像素点px，y为根节点的树，在根节点为px，y的树上用传统动态规划算法搜索匹配路径来最优化能量函数完成图像对像素点的稠密匹配；式中前一项m(px，y，d)表示px，y视差为d时的匹配度，由步骤4可得到；后一项是节点px，y所有子节点的数据累积代价，其中s(&#183;)代表相邻像素点之间的平滑代价，取为相邻像素点p和q的视差dp和dq之差的绝对值，即s(dp，dq)＝|dp-dq|；sub(px，y)表示px，y所有相邻子节点集合，即四个前驱节点px-1，y，px，y-1，px+1，y，px，y+1，则px，y的视差值为步骤6：视差校正步骤6.1：标记视差不可靠点分别以左右图像为基准图像计算视差图Dl，Dr，运用左右一致性准则，将满足|dl(p)-dr(q)|≤1的点标记为视差可靠点，并使Dispartiy(p)＝(dl(p)+dr(q))/2；否则标记为视差不可靠点，并标记Dispartiy(p)＝0；其中p点为基准图像中像素点，q点为配准图像中p点的匹配点，dl(p)∈Dl为像素点p点的视差值，dr(q)∈Dr为像素点q的视差，Dispartiy(p)为p点最终视差值；步骤6.2：填充视差不可靠点将基准图像中坐标(x，y)的视差不可靠像素点p八邻域的像素点标记为Npi，其中(xi，yi)为Npi的图像坐标，将Npi的灰度值与p点灰度值相减，得到灰度差值，并将灰度差值按从小到大的顺序排序；按照从灰度差值最小的像素到最大的像素的顺序来依次判断是否存在像素Npi满足如下三个条件(1)Npi为视差可靠点；(2)Npi∈Wp，其中Wp为步骤4.1中计算得到的p点自适应多边形支撑窗口；(3)|Il(xi，yi)-Ir(xi+d，yi)|≤s，其中Il(&#183;)，Ir(&#183;)表示基准图像和配准图像中像素点的灰度值，d＝dl(Npi)为Npi的视差值，s为设定的阈值；若存在Npi满足上面三个条件则将p点标记为可靠点，并使Dispartiy(p)＝dl(Npi)；否则将条件(3)替换为|Il(x，y)-Il(x+m，y+n)|≤s重新计算，如果存在Npi满足条件则将p标记为视差可靠点并使视差Dispartiy(p)＝dl(Npi)；其中m，n∈(-1，0，1)，m，n不同时为0；经过此步骤得到最终的视差图；步骤7：三维信息还原根据步骤2得到的摄像机内参数矩阵AL、AR和外参数矩阵[RL&#160;tL]、[RR&#160;tR]，以及步骤6得到的视差图中匹配点对关系，计算得到图像的三维点云坐标。FSA00000160363100011.tif,FSA00000160363100012.tif,FSA00000160363100021.tif</div>
    </div>
    </li> <li class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的基于立体视觉的实拍低纹理图像重建方法，其特征在于步骤 4的计算视差空间图包括以下四个子步骤，步骤4. 1 ：对两幅图像进行高斯滤波，消除噪声影响改善图像质量；步骤4.2 ：计算自适应多边形匹配窗口任取两幅图像中一幅为基准图像，另一幅为配准图像，对经过步骤3对极线校正变换 后得到的图像，通过公式&amp;zAQ^+ciAgy^计算基准图像中坐标为（x，y)的任意像素 点P的八邻域方向步长h( θ k)，其中为步长为hi的像素点与ρ点的灰度差，为 步长为Iii的像素点与ρ点的空间距离，当满足式、，;^ 时，h( θ k) = hi; a为常系数，τ 为阈值，{1,2,4,6,12,17}, θ k(k = 0，1，2.. 7)为八邻域方向；连接ρ点八邻域步长 h(ek)的顶点，形成ρ点的自适应多边形支撑窗口 Wp;4. 3 ：计算匹配度步骤4. 3包括以下三个子步骤：步骤4. 3. 1 ：经过步骤4. 2的计算，基准图像中坐标为（x，y)的任意像素点ρ的自适应 多边形支撑窗口为Wp，在配准图像中视差范围D内计算其在对应极线上可能的匹配点q的 自适应多边形支撑窗口为Wq ；如果基准图像为左图像配准图像为右图像，则q点坐标（x-d， y)，如果基准图像为右图像配准图像为左图像则q点坐标(x+d, y)，其中d e D，D = [dmin， dfflax]，dmin为最大视差值，dmax为最小视差值；步骤4. 3. 2 ：分别计算像素点p，q的特异性Q(P)，（^⑷，其中^…)=,=?^/…，"），、={(p+d) |dmin-dmax ( d ( dmax_dmin} ；f( &#183;)选取对光照及噪声鲁棒性较强的零均值归一化 算子（ZNCC)，支撑窗口的选取大小及形状与Wp和\的交集窗口 一致；步骤4. 3. 3 ：计算对应像素点ρ，ρ的匹配度，记为m(p，d)，m(P,d),湘&#183;’ f(p,q)步骤4. 4 ：在基准图像全图范围内重复计算步骤4. 1，步骤4. 2得到全图像素点在视差 范围内的匹配度，得到视差空间图。</div>
    </div>
    </li> <li class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1所述的基于立体视觉的实拍低纹理图像重建方法，其特征在于步骤 5的树形动态规划分为六个步骤来完成：步骤5. 1 ：构造水平树及垂直树动态规划路径以所有横坐标为χ的像素点为根节点构造树，则图像中所有像素点形成水平树动态规划路径；为所有纵坐标为y的像素点为根节点构造树，则图像中所有像素点形成了垂直树 动态规划路径；为简化E (px, y，d)的优化过程，将树形动态规划分为水平树动态规划路径和 垂直树动态规划路径；步骤5. 2 ：水平扫描线上动态规划最优能量函数首先使用传统动态规划算法在每一水平扫描线内单独进行优化得到当Px, y视差为 d时最优匹配能量值C(px,y，d)，水平扫描线上每一点的最优能量值C(px,y，d)即为分别 沿前向路径和后向路径的匹配度累计值，前向路径为图像中每一行从最左边像素延伸到 最右边像素的路径，后向路径为图像中每一行从最右边像素延伸到最左边像素的路径； = mip^ + ^ } ^iMd, O + 0) c (ρχ y，d) = ρ (Ρχ y，d) +B (Px,y，d) -m(px,y，d)其中:<span class="patent-image-not-available"> </span>m(px,y，d)表示px,y视差为d时的匹配度，由步骤4得到，Hsub(px,y)表示px,y点的水平 方向子节点Ρχ-μ，Ρχ+μ的集合；F(px,y，d)为前向路径最优匹配值，B(px,y，d)为后向路径最 优匹配值，λ为权重系数；步骤5. 3 ：水平树动态规划最优能量函数接下来运用所得到的每一行扫描线的最优能量值来优化水平树结构， H (ρ x, y，d)为以ρx, y为树的根节点，视差为d时的水平树的最优能量值；由公式<span class="patent-image-not-available"> </span>可推得：<span class="patent-image-not-available"> </span>由上式可看出水平树的能量优化实际上归结为当数据匹配度为步骤5. 2中所得水平 扫描线上匹配值C (px,y，d)时，在px,y所在位置的垂直扫描线上进行动态规划，其中Vsub (px, y)表示Px,y点的垂直方向子节点和Px,y+1的集合；在以y为纵坐标的垂直扫描线上执 行传统动态规划算法即得到水平树最优能量值H(px,y，d)； 步骤5. 4 ：垂直树动态规划最优能量函数同理根据步骤5. 2，步骤5. 3的方法先计算垂直扫描线上像素的匹配度再在水平扫描 线上进行动态规划得到可得到以Px, y为根节点的垂直树最优路径能量值V (px,y，d)； 步骤5. 5 ：得到像素点视差值首先计算垂直树结构最优路径能量值V(px, y，d)，然后将结果传递到水平树结构，使用式<span class="patent-image-not-available"> </span>y的匹配度m(px,y，d)更新为m' (px, y，d)来计算水平树上的最优路径能量H(px,y，d)，其中ξ为权重系数；如果视 差d不是垂直树上所得最优视差，则施加惩罚用以加大匹配代价，差值越大则惩罚越大， 这样就可以使水平树上的动态规划路径不经过视差为d的像素点，最终得到px,y的视差值 = ar^nNipxyiC/);步骤5. 6 ：重复计算全图匹配点对应关系，得到视差图。</div>
    </div>
  </li> </ol>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES38994678" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于立体视觉的实拍低纹理图像重建方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明属于双目立体视觉技术领域，涉及到基于立体视觉的实拍低纹理图像重建 问题，尤其涉及到一种在自适应多边形匹配窗口内引入基于基准图像中像素点的特异性和 基准图像和配准图像间像素点对的不相似性计算匹配度及基于树形动态规划算法来获得 实拍低纹理图像三维点云的方法。</p>
    <p>背景技术</p>
    <p>[0002]	双目立体视觉技术是一种被动式的三维测量技术，要解决的主要任务有：摄像机 标定，立体匹配和三维信息恢复。被动式立体视觉技术可以获得稠密的三维点云坐标，无需 向被测物体投影光栅等辅助信息，具有人机交互友好，硬件配置要求低，成本便宜及自动化 程度较高等优点，是三维重建领域中比较流行的一种技术。立体匹配是立体视觉中最重要 的步骤，主要任务是首先通过双目或多目图像匹配得到视差图，然后通过三角测量关系得 到物体的景深。根据约束方式的不同匹配方法分为两种：一种是对图像周围小区域进行约 束的局部匹配方法，另一种是对扫描线甚至整个图像进行约束的全局匹配方法。局部匹配 方法的优点是计算量小速度快，但由于局部寻优，对低纹理及遮挡区域比较敏感，全局匹配 方法的优点是正确匹配率较高，主要有动态规划，置信度传播和图割等，但计算量比局部匹 配方法大，耗时久，其中动态规划兼具计算效率高，匹配效果较好的特点，利于实拍图像的 重建。</p>
    <p>[0003]	现有的基于双目立体视觉的实拍低纹理图像重建算法存在以下几个方面的缺占.</p>
    <p>^ \\\ &#183;</p>
    <p>[0004]	(1)在匹配度的计算中单纯的在长方形或正方形支撑窗口中采用方差和、绝对差 和、零均值归一化及自适应权重测度等匹配测度函数计算；支撑窗口过小则不足以包含足 够的灰度变化来指导匹配而错误估计视差值；过大则会包含相同不同视差范围内的点使得 匹配度达到极值时并不代表正确匹配位置并模糊区域边界信息；而且上述测度函数在低纹 理区域可能不足以区分模糊像素点导致多对一的错误匹配；</p>
    <p>[0005]	(2)单纯的采用传统动态规划全局最优算法，传统的动态规划单独扫描每一条扫 描线，缺乏水平连续性约束和垂直连续性约束的融合，单点像素的误匹配时会影响同一条 扫描线上后续像素的匹配而产生明显的条纹效应；而且对于灰度实拍图像纹理不够充分而 且存在噪声的情况下，低纹理及视差跳变区域的待匹配点在正确的匹配上不能捕捉到足够 的正确纹理信息就容易导致错误的匹配结果，导致重建点云结果不平滑及边界不清晰。</p>
    <p>[0006]	由于存在以上缺点，现有的实拍低纹理图像重建算法在实际应用中并不能得到令 人满意的重建效果。</p>
    <p>发明内容</p>
    <p>[0007]	技术问题：本发明的目的是提供一种基于双目立体视觉的实拍低纹理图像重建方 法，能够准确快速自动重建低纹理实拍图像三维点云。[0008]	技术方案：首先进行摄像机标定，然后采集两幅图像，根据标定数据进行对极线校 正和图像变换，然后经过高斯滤波后在灰度信息和空间距离的约束下计算每个像素点的自 适应多边形支撑窗口，利用一种基于像素点自身的特异性和像素点间相似性的匹配测度函 数计算像素点间的匹配度，增大低纹理区域及重复纹理区域像素间匹配代价的区分度；根 据得到的匹配度分别以左右图像为匹配基元采用一种全局意义上的树形动态规划对图像 进行逐点匹配，之后在左右一致性准则的基础上运用一种简单有效的视差校正方法消除误 匹配得到最终视差图；最后利用标定数据和匹配结果重建出人脸三维点云坐标并显示。</p>
    <p>[0009]	该重建方法依次含有下列步骤：</p>
    <p>[0010]	步骤1:图像获取</p>
    <p>[0011]	使用双目摄像机获取图像，首先调整双目摄像机使其光轴基本平行并使左右镜角 度处于合适的位置，然后同时各拍摄一幅图像，其中左镜头拍摄的为左图像，右镜头拍摄的 为右图像；</p>
    <p>[0012]	步骤2:摄像机标定</p>
    <p>[0013]	分别对两台摄像机进行标定，获得各自的内参数矩阵~、AK和外参数矩阵风tj、 [Rr tJ ；</p>
    <p>[0014]	步骤3 ：对极线几何校正及图像变换</p>
    <p>[0015]	采用步骤2得到的内外参运用极线校正方法对所拍摄的左右图像进行极线校正 得到平行式双目视觉模型，使匹配像素对处于同一扫描线上；</p>
    <p>[0016]	步骤4:计算视差空间图，</p>
    <p>[0017]	步骤5 ：树形动态规划完成稠密匹配：</p>
    <p>[0018]	以像素点px, y为基准，用箭头表示一种前驱后继关系，箭头从前驱节点出发指 向后继节点；在以y为纵坐标的图像行上从最左边的像素点开始用箭头指向其右边像素 点直到Px,y，从最右边的像素点开始用箭头指向其左边的像素点直到Px,y，然后在以χ为 横坐标的图像列上从最上边的像素点开始用箭头指向其下方的像素点直到Px,y，从最下 面的相似度开始用箭头指向其上方的像素点直到Px, y ；这样就构造了以像素点Px, y为根 节点的树，在根节点为px,y的树上用传统动态规划算法搜索匹配路径来最优化能量函数</p>
    <span class="patent-image-not-available"> </span>
    <p>)完成图像对像素点的稠密匹配；式中前</p>
    <p>一项m(px,y，d)表示px, y视差为d时的匹配度，由步骤4可得到；后一项是节点Px, y所有子 节点的数据累积代价，其中s( &#183;)代表相邻像素点之间的平滑代价，取为相邻像素点ρ和q 的视差dp和d,之差的绝对值，即s (dp，dq) = I dp-dq I ；sub (px, y)表示px, y所有相邻子节点 集合，即四个前驱节点Pny，Pj^1，Px+1,y，Px,y+1，则Px,y的视差值为</p>
    <span class="patent-image-not-available"> </span>
    <p>[0019]	步骤6:视差校正</p>
    <p>[0020]	步骤6. 1 ：标记视差不可靠点</p>
    <p>[0021]	分别以左右图像为基准图像计算视差图D1, Dp运用左右一致性准则，将满足 d1(p)-dr(q) I ( 1的点标记为视差可靠点，并使DisPartiy(P) = ((I1 (p)+dr(q))/2 ；否则标</p>
    <p>记为视差不可靠点，并标记DisPartiy(P) = 0 ；其中ρ点为基准图像中像素点，q点为配准 图像中P点的匹配点，Cl1(P) e D1为像素点P点的视差值，dr(q) e Dr为像素点q的视差， Dispartiy (ρ)为ρ点最终视差值；[0022]	步骤6. 2 ：填充视差不可靠点</p>
    <p>[0023]	将基准图像中坐标（x，y)的视差不可靠像素点ρ八邻域的像素点标记为Npi，其中 (Xi, Yi)为Npi的图像坐标，ζ&#183; = 1,2...识(屮=8);将Npi的灰度值与P点灰度值相减，得到灰度 差值，并将灰度差值按从小到大的顺序排序；按照从灰度差值最小的像素到最大的像素的 顺序来依次判断是否存在像素Npi满足如下三个条件（I)Npi为视差可靠点；（2) Npi e Wp, 其中Wp为步骤4. 1中计算得到的ρ点自适应多边形支撑窗口 ；（3) Il1Ui, yi)-I,(Xi+d， Yi)彡s，其中IJ &#183;)，Ir( &#183;)表示基准图像和配准图像中像素点的灰度值，d = Cl1(Npi)为 Npi的视差值，s为设定的阈值；</p>
    <p>[0024]	若存在Npi满足上面三个条件则将ρ点标记为可靠点，并使DisPartiy(P)= Cl1(Npi)；否则将条件(3)替换为I1(x,y)-I1(x+m,y+n) |彡s重新计算，如果存在Npi满足 条件则将P标记为视差可靠点并使视差DiSpartiy(p) = (I1(Npi)；其中m，η e (-1,0,1), m, η不同时为0 ；经过此步骤得到最终的视差图；</p>
    <p>[0025]	步骤7 ：三维信息还原</p>
    <p>[0026]	根据步骤2得到的摄像机内参数矩阵~、ΑΚ和外参数矩阵风tj、[Re tE]，以及步 骤6得到的视差图中匹配点对关系，计算得到图像的三维点云坐标。</p>
    <p>[0027]	步骤4的计算视差空间图包括以下四个子步骤，</p>
    <p>[0028]	步骤4. 1 ：对两幅图像进行高斯滤波，消除噪声影响改善图像质量；</p>
    <p>[0029]	步骤4.2 ：计算自适应多边形匹配窗口</p>
    <p>[0030]	任取两幅图像中一幅为基准图像，另一幅为配准图像，对经过步骤3对极线校正 变换后得到的图像，通过公式</p>
    <span class="patent-image-not-available"> </span>
    <p>计算基准图像中坐标为（X，y)的任 意像素点P的八邻域方向步长h( θ k)，其中ACwwi,.为步长为比的像素点与ρ点的灰度差，</p>
    <p>为步长为Iii的像素点与ρ点的空间距离，当满足式^^ &amp;时，h( θ k) = hi; a为常 系数，τ为阈值，hi e {1,2,4,6,12,17}, θ k(k = 0，1，2.. 7)为八邻域方向；连接ρ点八邻 域步长h( θ k)的顶点，形成ρ点的自适应多边形支撑窗口 Wp ；</p>
    <p>[0031]	4. 3:计算匹配度</p>
    <p>[0032]	步骤4. 3包括以下三个子步骤：</p>
    <p>[0033]	步骤4. 3. 1 ：经过步骤4. 2的计算，基准图像中坐标为（x，y)的任意像素点ρ的 自适应多边形支撑窗口为Wp，在配准图像中视差范围D内计算其在对应极线上可能的匹配 点q的自适应多边形支撑窗口为Wq ；如果基准图像为左图像配准图像为右图像，则q点坐标 (x-d, y)，如果基准图像为右图像配准图像为左图像则q点坐标（x+d，y)，其中d e D，D = [dmin，dmax]，dmin为最大视差值，dmax为最小视差值；</p>
    <p>[0034]	步骤4. 3. 2 ：分别计算像素点p，q的特异性Q (P)，Q (q)，其中</p>
    <span class="patent-image-not-available"> </span>
    <p>Wp = {(p+d) |dmin-dmax彡d彡dmax-dmin} ；f ( &#183;)选取对光照及噪声鲁棒性较强的零均值归一 化算子（ZNCC)，支撑窗口的选取大小及形状与Wp和\的交集窗口 一致；</p>
    <p>[0035]	步骤4. 3. 3 ：计算对应像素点ρ，ρ的匹配度，记为m(p，d)，</p>
    <p>[0036]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0037]	步骤4. 4 ：在基准图像全图范围内重复计算步骤4. 1，步骤4. 2得到全图像素点在视差范围内的匹配度，得到视差空间图。</p>
    <p>[0038]	步骤5的树形动态规划分为六个步骤来完成：</p>
    <p>[0039]	步骤5. 1 ：构造水平树及垂直树动态规划路径</p>
    <p>[0040]	以所有横坐标为χ的像素点为根节点构造树，则图像中所有像素点形成水平树动态规划路径；为所有纵坐标为y的像素点为根节点构造树，则图像中所有像素点形成了垂 直树动态规划路径；为简化E(px,y，d)的优化过程，将树形动态规划分为水平树动态规划路 径和垂直树动态规划路径；</p>
    <p>[0041]	步骤5. 2 ：水平扫描线上动态规划最优能量函数</p>
    <p>[0042]	首先使用传统动态规划算法在每一水平扫描线内单独进行优化得到当px, y视差 为d时最优匹配能量值C(px,y，d)，水平扫描线上每一点的最优能量值C(px,y，d)即为分 别沿前向路径和后向路径的匹配度累计值，前向路径为图像中每一行从最左边像素延伸 到最右边像素的路径，后向路径为图像中每一行从最右边像素延伸到最左边像素的路径;</p>
    <span class="patent-image-not-available"> </span>
    <p>[0043]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0044]其中：F(px	y, d) = m(px y, d) + mm{Xs{d, &#970;) + F(px_l y, /))</p>
    <p>[0045]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0046]	m(px,y，d)表示px,y视差为d时的匹配度，由步骤4得到，Hsub(px,y)表示px,y点的 水平方向子节点ΡΧ-Μ，ΡΧ+Μ的集合；F(px,y，d)为前向路径最优匹配值，B(px,y，d)为后向路 径最优匹配值，λ为权重系数；</p>
    <p>[0047]	步骤5. 3 ：水平树动态规划最优能量函数</p>
    <p>[0048]	接下来运用所得到的每一行扫描线的最优能量值来优化水平树结构， H (ρx, y，d)为以ρx, y为树的根节点，视差为d时的水平树的最优能量值；由公式 H(px’y,d) = m(pxy,d)+ Yd min(A5(i/,i) + H(q,0)可推得：</p>
    <span class="patent-image-not-available"> </span>
    <p>[0049]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0050]	由上式可看出水平树的能量优化实际上归结为当数据匹配度为步骤5. 2中所得 水平扫描线上匹配值C(px,y，d)时，在px, y所在位置的垂直扫描线上进行动态规划，其中 Vsub (px, y)表示px, y点的垂直方向子节点px, 和px, y+1的集合；在以y为纵坐标的垂直扫 描线上执行传统动态规划算法即得到水平树最优能量值H (px, y，d)；</p>
    <p>[0051]	步骤5. 4 ：垂直树动态规划最优能量函数</p>
    <p>[0052]	同理根据步骤5. 2，步骤5. 3的方法先计算垂直扫描线上像素的匹配度再在水平 扫描线上进行动态规划得到可得到以Px,y为根节点的垂直树最优路径能量值V(px,y，d)；</p>
    <p>[0053]	步骤5. 5 ：得到像素点视差值</p>
    <p>[0054]	首先计算垂直树结构最优路径能量值V (px, y，d)，然后将结果传递到水平树结构，</p>
    <p>使用式=</p>
    <span class="patent-image-not-available"> </span>
    <p>将 px,y 的匹配度 rn(px,y，d)更</p>
    <p>新为m' (px, y，d)来计算水平树上的最优路径能量H(px,y，d)，其中ξ为权重系数；如果 视差d不是垂直树上所得最优视差，则施加惩罚用以加大匹配代价，差值越大则惩罚越大，这样就可以使水平树上的动态规划路径不经过视差为d的像素点，最终得到Px, y的视差值</p>
    <span class="patent-image-not-available"> </span>
    <p>[0055]	步骤5. 6 ：重复计算全图匹配点对应关系，得到视差图。</p>
    <p>[0056]	有益效果；与现有技术相比，本发明具有下面的优点：自适应多边形支撑窗口利 用灰度及空间距离同时约束窗口的大小及形状，包含足够的有效灰度信息的同时防止那些 在视差边界变化区域的点的支撑窗口覆盖不同视差范围内的像素，这样既使纹理稀疏的区 域能够获得准确的视差，点云更加平滑，又不至于使视差边界产生模糊的过平滑现象。为待 匹配点选取了合适的支撑窗口后，计算像素点之间的匹配度时，不仅将不同图中两像素间 的相似度作为匹配准则，同时也考虑了同一图像同一区域中像素点之间的差异，认为只有 既满足特异性高又满足相似度高的像素点才可能是匹配共轭点，两者的比值增大了匹配度 的变化梯度，明显增大了匹配度之间的差异，提高了匹配度的区别能力，结合自适应多边形 支撑窗口，对无纹理区域以及重复区域的像素有很好的辨识度，后续的树形动态规划充分 利用的整幅图像所有像素点的纹理信息来寻找最优解最小化能量函数，减小了由于匹配错 误及误差向后传播而导致条纹产生的可能，提高了低纹理图像的匹配精度。</p>
    <p>附图说明</p>
    <p>[0057]	图1本发明完整流程图。</p>
    <p>[0058]	图2系统模型及原理示意图。</p>
    <p>[0059]	图3像素点自适应多边形支撑窗口示意图。</p>
    <p>[0060]	图4对极线校正示意图。</p>
    <p>[0061 ] 图5树形动态规划候选路径示意图。</p>
    <p>[0062]	图6由匹配关系和标定数据计算图像上物点的空间三维坐标示意图。 具体实施方式</p>
    <p>[0063]	下面参照附图，对本发明具体实施方案做更为详细的描述。编程实现工具选用 VC++6.0，室内环境中拍摄了两幅低纹理建筑物图像作为待重构的图像。</p>
    <p>[0064]	图1为本发明的完整流程图。</p>
    <p>[0065]	图2为本发明的系统模型及原理示意图。使用两个CCD分别从两个不同角度同时 各拍摄一幅建筑物图像，C\、0K分别为两台摄像机的光心，Ip Ik分别为两台摄像机的成像平 面，P为待重构的建筑物上的一个空间物点，PL, Pe为物点P分别在两台摄像机成像平面上 所成的像点。这种由同一空间物点在不同摄像机成像平面上所成的像点为一对匹配点。任 取其中一幅为基准图像，另一幅为配准图像，为基准图像中的每个像素点在对准图像中搜 索对应的匹配点的过程称为立体匹配。得到像素点的匹配关系后，根据系统模型，结合标定 得到的摄像机内外参数，进行逆向运算，就可得到对应物点的空间三维坐标，从而实现图像 的三维重构。</p>
    <p>[0066]	图3是像素点自适应多边形支撑窗口示意图。如图3(a)所示，ρ为基准图像I1中 一个像素点，Wp为以P为中心像素点的一个自适应多边形窗口区域；图3(b)配准图像Ir中， 虚线框内区域为P对应的匹配点的视差范围即搜索范围，P的匹配点落在这个范围内，q为 搜索范围中的一个像素点，Wq为以q为中心点的自适应多边形匹配窗口区域，图3(c)中虚线区域&amp;为</p>
    <span class="patent-image-not-available"> </span>
    <p>中大小及形状与自适应多边形窗口 Wp和\的交集窗口一致的窗口 ；图3 (d) 中虚线区域K为仁中大小及形状与自适应多边形窗口&#12316;和Wq的交集窗口一致的窗口。I1 中像素点P和仁中与其视差距离为d的像素点q的匹配度m(p，d)定义为下面公式（1)： ,j、 Q(P)xQi^)</p>
    <p>[0067]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0068]其中：</p>
    <p>[0069]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0070]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0071]	本发明的方法具体包括以下几个步骤：</p>
    <p>[0072]	步骤1:图像获取</p>
    <p>[0073]	使用双目摄像机获取图像，首先调整双目摄像机使其光轴基本平行并使左右镜角 度处于合适的位置，然后同时各拍摄一幅图像，其中左镜头拍摄的为左图像，右镜头拍摄的 为右图像；</p>
    <p>[0074]	步骤2 ：摄像机标定</p>
    <p>[0075]本发明中采用参考文献"A	Flexible New Technique for CameraCalibration"(Zhang Z Y,IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 20 (11) =1330-1334)中提出的一种称为平面模板法的标定算法分别 对两台摄像机进行标定，获得各自的内参数矩阵4、AK和外参数矩阵tj、[RK tK]。</p>
    <p>[0076]	步骤3 ：对极几何校正及图像变换</p>
    <p>[0077]	如图4所示是对极线校正示意图。根据步骤2中标定过程得到的内外参数，采用 参考文献"A compact algorithm for rectification of stereo pairs. Machine Vision andApplications”（Fusiello A, Trucco E，Verri A. 2000,12(1) ： 16-22)中的提出的极线 校正方法对所拍摄的左右图像进行极线校正，如果变换后图像中的像素点坐标对应到原始 图像中的非整数坐标上时，则进行通过灰度双线性插值，最后得到平行式双目视觉模型，使 匹配像素对处于同一扫描线上，降低匹配的空间复杂度，经过校正后的图像无失真，校正准 确率高，误差小于一个像素。</p>
    <p>[0078]	步骤4 ：计算视差空间图，包括以下四个子步骤</p>
    <p>[0079]	步骤4. 1 ：对两幅图像进行高斯滤波，消除噪声影响改善图像质量。</p>
    <p>[0080]	步骤4.2 ：计算自适应多边形匹配窗口</p>
    <p>[0081]	任取两幅图像中一幅为基准图像，另一幅为配准图像，对经过步骤3对极线校正 变换后得到的图像，通过公式‘</p>
    <span class="patent-image-not-available"> </span>
    <p>,计算基准图像中坐标为（X，y)的任 意像素点P的八邻域方向步长h( θ k)，其中</p>
    <span class="patent-image-not-available"> </span>
    <p>为步长为hi的像素点与ρ点的灰度差， △gy&#20175;为步长为比的像素点与ρ点的空间距离，当满足式</p>
    <span class="patent-image-not-available"> </span>
    <p>时，h( θ k) = hi; a为常 系数，τ为阈值，hi e {1,2,4,6,12,17}, θ k(k = 0，1，2.. 7)为八邻域方向；连接ρ点八邻 域步长h( θ k)的顶点，形成ρ点的自适应多边形支撑窗口 Wp。[0082]	4. 3 ：计算匹配度</p>
    <p>[0083]	步骤4. 3包括以下三个子步骤：</p>
    <p>[0084]	步骤4. 3. 1 ：经过步骤4. 2的计算，基准图像中坐标为（x，y)的任意像素点ρ的 自适应多边形支撑窗口为Wp，在配准图像中视差范围D内计算其在对应极线上可能的匹配 点q的自适应多边形支撑窗口为Wq ；如果基准图像为左图像配准图像为右图像，则q点坐标 (x-d, y)，如果基准图像为右图像配准图像为左图像则q点坐标（x+d，y)，其中d e D，D = [dmin，dmax]，其中dmin为最大视差值，dmax为最小视差值。</p>
    <p>[0085]	步骤4. 3. 2 ：分别计算像素点p，q的特异性Q (P)，Q (q)，其中</p>
    <span class="patent-image-not-available"> </span>
    <p>；f ( &#183;)选取对光照及噪声鲁棒性较强的零均值归一 化算子（ZNCC)，支撑窗口的选取大小及形状与Wp和\的交集窗口 一致；</p>
    <p>,,、Q(p)xQi^)</p>
    <p>[0086]	步骤4. 3. 3 ：计算对应像素点ρ的匹配度，记为m(p，&#940;)	；</p>
    <span class="patent-image-not-available"> </span>
    <p>[0087]	步骤4. 4 ：在基准图像全图范围内重复计算步骤4. 1，步骤4. 2得到全图像素点在 视差范围内的匹配度，得到视差空间图。</p>
    <p>[0088]	步骤5 ：树形动态规划完成稠密匹配：</p>
    <p>[0089]	以像素点px, y为基准，用箭头表示一种前驱后继关系，箭头从前驱节点出发指 向后继节点；在以y为纵坐标的图像行上从最左边的像素点开始用箭头指向其右边像素 点直到Px,y，从最右边的像素点开始用箭头指向其左边的像素点直到Px,y，然后在以χ为 横坐标的图像列上从最上边的像素点开始用箭头指向其下方的像素点直到Px,y，从最下 面的相似度开始用箭头指向其上方的像素点直到px, y。这样就构造了以像素点Px,y为根 节点的树，在根节点为px,y的树上用传统动态规划算法搜索匹配路径来最优化能量函数</p>
    <span class="patent-image-not-available"> </span>
    <p>完成图像对像素点的稠密匹配；式中前</p>
    <p>一项m(px,y，d)表示px, y视差为d时的匹配度，由步骤4可得到；后一项是节点Px, y所有子 节点的数据累积代价，其中s( &#183;)代表相邻像素点之间的平滑代价，取为相邻像素点ρ和q 的视差dp和d,之差的绝对值，即s (dp，dq) = I dp-dq I ；sub (px, y)表示px, y所有相邻子节点 集合，即四个前驱节点Ρ^，Pj^1，Px+1,y，Px,y+1，则Px,y的视差值为=^rgrmnE(pxy,d)；</p>
    <p>[0090]	将步骤5的树形动态规划分为六个步骤来完成： [0091 ] 步骤5. 1 ：构造水平树及垂直树动态规划路径</p>
    <p>[0092]	以所有横坐标为χ的像素点为根节点构造树，则图像中所有像素点形成水平树动 态规划路径，如图5 (b)所示；为所有纵坐标为y的像素点为根节点构造树，则图像中所有像 素点形成了垂直树动态规划路径，如图5(c)所示；为简化E(px,y，d)的优化过程，将树形动 态规划分为水平树动态规划路径和垂直树动态规划路径。</p>
    <p>[0093]	步骤5. 2 ：水平扫描线上动态规划最优能量函数</p>
    <p>[0094]	首先使用传统动态规划算法在每一水平扫描线内单独进行优化得到当px,y视差为 d时最优匹配能量值C (px,y，d)，水平扫描线上每一点的最优能量值C (px,y，d)即为分别沿前 向路径和后向路径的匹配度累计值，如图5(a)所示前向路径为图像中每一行从最左边像 素延伸到最右边像素的路径，后向路径为图像中每一行从最右边像素延伸到最左边像素的</p>
    <span class="patent-image-not-available"> </span>
    <p>[0095]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0096]其中：</p>
    <span class="patent-image-not-available"> </span>
    <p>FOpyO)</p>
    <p>[0097]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0098]	m(px,y，d)表示px,y视差为d时的匹配度，由步骤4得到，Hsub(px,y)表示px,y点的 水平方向子节点ΡΧ-Μ，ΡΧ+Μ的集合；F(px,y，d)为前向路径最优匹配值，B(px,y，d)为后向路 径最优匹配值，λ为权重系数；</p>
    <p>[0099]	步骤5. 3 ：水平树动态规划最优能量函数</p>
    <p>[0100]	接下来运用所得到的每一行扫描线的最优能量值来优化水平树结构，H(px,y，d)为 以Px,y为树的根节点，视差为d时的水平树的最优能量值；</p>
    <p>[0101]由公式丑</p>
    <span class="patent-image-not-available"> </span>
    <p>可推得：</p>
    <span class="patent-image-not-available"> </span>
    <p>[0103]	由上式可看出水平树的能量优化实际上归结为当数据匹配度为步骤5. 2中所得 水平扫描线上匹配值C(px,y，d)时，在px, y所在位置的垂直扫描线上进行动态规划，其中 Vsub (px, y)表示px, y点的垂直方向子节点px, 和px, y+1的集合；在以y为纵坐标的垂直扫 描线上执行传统动态规划算法即得到水平树最优能量值H (px, y，d)；</p>
    <p>[0104]	步骤5. 4 ：垂直树动态规划最优能量函数</p>
    <p>[0105]	同理根据步骤5. 2，步骤5. 3的方法先计算垂直扫描线上像素的匹配度再在水平 扫描线上进行动态规划得到可得到以Px,y为根节点的垂直树最优路径能量值V(px,y，d)；</p>
    <p>[0106]	步骤5. 5 ：得到像素点视差值</p>
    <p>[0107]	首先计算垂直树结构最优路径能量值V (px, y，d)，然后将结果传递到水平树结构， 使用式历</p>
    <span class="patent-image-not-available"> </span>
    <p>d)更</p>
    <p>新为m' (px, y，d)来计算水平树上的最优路径能量H(px,y，d)，其中ξ为权重系数；如果 视差d不是垂直树上所得最优视差，则施加惩罚用以加大匹配代价，差值越大则惩罚越大， 这样就可以使水平树上的动态规划路径不经过视差为d的像素点，最终得到px,y的视差值 =argl^mpVdh</p>
    <p>[0108]	步骤5. 6 ：重复计算全图匹配点对应关系，得到视差图；</p>
    <p>[0109]	步骤6 ：视差校正，分为以下两个子步骤：</p>
    <p>[0110]	步骤6. 1 ：标记视差不可靠点</p>
    <p>[0111]	分别以左右图像为基准图像计算视差图D1, Dp运用左右一致性准则，将满足 d1(p)-dr(q) I ( 1的点标记为视差可靠点，并使DisPartiy(P) = ((I1 (p)+dr(q))/2 ；否则标</p>
    <p>记为视差不可靠点，并标记DisPartiy(P) = O ；其中ρ点为基准图像中像素点，q点为配准 图像中P点的匹配点，Cl1(P) e D1为像素点P点的视差值，dr(q) e Dr为像素点q的视差， Dispartiy (ρ)为ρ点最终视差值；</p>
    <p>[0112]	步骤6. 2 ：填充视差不可靠点</p>
    <p>[0113]	将基准图像中坐标（x，y)的视差不可靠像素点ρ八邻域的像素点标记为Npi，其中(Xi, Yi)为Npi的图像坐标，</p>
    <span class="patent-image-not-available"> </span>
    <p>将Npi的灰度值与ρ点灰度值相减，得到灰度 差值，并将灰度差值按从小到大的顺序排序；按照从灰度差值最小的像素到最大的像素的 顺序来依次判断是否存在像素Npi满足如下三个条件（I)Npi为视差可靠点；（2) Npi e Wp, 其中Wp为步骤4. 1中计算得到的ρ点自适应多边形支撑窗口 ；（3) Il1Ui, yi)-I,(Xi+d， Yi)≤s，其中IJ &#183;)，Ir( &#183;)表示基准图像和配准图像中像素点的灰度值，d = Cl1(Npi)为 Npi的视差值，s为设定的阈值；</p>
    <p>[0114]	若存在Npi满足上面三个条件则将ρ点标记为可靠点，并使DisPartiy(P)= Cl1(Npi)；否则将条件(3)替换为I1(x,y)-I1(x+m,y+n) |≤s重新计算，如果存在Npi满足 条件则将P标记为视差可靠点并使视差DiSpartiy(p) = (I1(Npi)；其中m，η e (-1,0,1), m, η不同时为0 ；经过此步骤得到最终的视差图；</p>
    <p>[0115]	步骤7:三维信息还原</p>
    <p>[0116]	根据步骤2得到的摄像机内参数矩阵~、Ak和外参数矩阵风tJ、[RK tj，以及步 骤4和步骤5得到的匹配点对应关系，通过空间交汇法就可以计算出图像上物点的三维坐 标。 </p>
    <p>[0117]</p>
    <span class="patent-image-not-available"> </span>
    <p>[0118]	图6是空间交汇法的示意图，空间中任意一点？礼&#20034;义，1)分别与双目摄像机两 光心(\、0Κ的所成的直线(\P，OeP在成像平面上的交点就是其在图像平面的投影PJu1, ν)， Pe(u2, ν)，通过空间点的左右图像坐标根据公式（3)可得到直线(\P，OkP的方程，其交点就 是P点世界坐标，两条直线如果异面不相交，则取其公垂线的中点。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101101672A?cl=zh">CN101101672A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年7月13日</td><td class="patent-data-table-td patent-date-value">2008年1月9日</td><td class="patent-data-table-td ">中国科学技术大学</td><td class="patent-data-table-td ">基于虚拟图像对应的立体视觉三维人脸建模方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101398886A?cl=zh">CN101398886A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年3月17日</td><td class="patent-data-table-td patent-date-value">2009年4月1日</td><td class="patent-data-table-td ">杭州大清智能技术开发有限公司</td><td class="patent-data-table-td ">一种基于双目被动立体视觉的快速三维人脸识别方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101625768A?cl=zh">CN101625768A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年7月23日</td><td class="patent-data-table-td patent-date-value">2010年1月13日</td><td class="patent-data-table-td ">东南大学</td><td class="patent-data-table-td ">一种基于立体视觉的三维人脸重建方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3D2002271818A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGz-sQFRUW8UMQ1Nt4Zh6o9OBolWw">JP2002271818A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">没有名称</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102810205B?cl=zh">CN102810205B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年7月9日</td><td class="patent-data-table-td patent-date-value">2015年8月5日</td><td class="patent-data-table-td ">深圳泰山在线科技有限公司</td><td class="patent-data-table-td ">一种摄像或照相装置的标定方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103054522A?cl=zh">CN103054522A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月31日</td><td class="patent-data-table-td patent-date-value">2013年4月24日</td><td class="patent-data-table-td ">河海大学</td><td class="patent-data-table-td ">基于视觉测量的清洁机器人系统及其测控方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103054522B?cl=zh">CN103054522B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月31日</td><td class="patent-data-table-td patent-date-value">2015年7月29日</td><td class="patent-data-table-td ">河海大学</td><td class="patent-data-table-td ">一种清洁机器人系统及其测控方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103136750A?cl=zh">CN103136750A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月30日</td><td class="patent-data-table-td patent-date-value">2013年6月5日</td><td class="patent-data-table-td ">广西工学院</td><td class="patent-data-table-td ">双目视觉系统的立体匹配优化方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103136750B?cl=zh">CN103136750B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月30日</td><td class="patent-data-table-td patent-date-value">2015年8月19日</td><td class="patent-data-table-td ">广西科技大学</td><td class="patent-data-table-td ">双目视觉系统的立体匹配优化方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103236081A?cl=zh">CN103236081A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年4月25日</td><td class="patent-data-table-td patent-date-value">2013年8月7日</td><td class="patent-data-table-td ">四川九洲电器集团有限责任公司</td><td class="patent-data-table-td ">一种彩色点云的配准方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103236081B?cl=zh">CN103236081B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年4月25日</td><td class="patent-data-table-td patent-date-value">2016年4月27日</td><td class="patent-data-table-td ">四川九洲电器集团有限责任公司</td><td class="patent-data-table-td ">一种彩色点云的配准方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103868460A?cl=zh">CN103868460A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2014年3月13日</td><td class="patent-data-table-td patent-date-value">2014年6月18日</td><td class="patent-data-table-td ">桂林电子科技大学</td><td class="patent-data-table-td ">基于视差优化算法的双目立体视觉自动测量方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103986923A?cl=zh">CN103986923A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年2月7日</td><td class="patent-data-table-td patent-date-value">2014年8月13日</td><td class="patent-data-table-td ">财团法人成大研究发展基金会</td><td class="patent-data-table-td ">影像立体匹配系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103986923B?cl=zh">CN103986923B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年2月7日</td><td class="patent-data-table-td patent-date-value">2016年5月4日</td><td class="patent-data-table-td ">财团法人成大研究发展基金会</td><td class="patent-data-table-td ">影像立体匹配系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2015139454A1?cl=zh">WO2015139454A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2014年10月21日</td><td class="patent-data-table-td patent-date-value">2015年9月24日</td><td class="patent-data-table-td ">华为技术有限公司</td><td class="patent-data-table-td ">一种高动态范围图像合成的方法及装置</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0007000000">G06T7/00</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=A_KABwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0011000000">G06T11/00</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2010年11月17日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2010年12月29日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年5月2日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Granted</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年11月6日</td><td class="patent-data-table-td ">C41</td><td class="patent-data-table-td ">Transfer of the right of patent application or the patent right</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年11月6日</td><td class="patent-data-table-td ">ASS</td><td class="patent-data-table-td ">Succession or assignment of patent right</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">FORMER OWNER: SOWTHEAST UNIV.</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20131018</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NANTONG RUIYIN CLOTHING CO., LTD.</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOWTHEAST UNIV.</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20131018</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年11月6日</td><td class="patent-data-table-td ">COR</td><td class="patent-data-table-td ">Bibliographic change or correction in the description</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CORRECT: ADDRESS; FROM: 210009 NANJING, JIANGSU PROVINCE TO: 226600 NANTONG, JIANGSU PROVINCE</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/d3a12f47fd7db63cf8c8/CN101887589A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E7%9A%84%E5%AE%9E%E6%8B%8D%E4%BD%8E%E7%BA%B9.pdf?id=A_KABwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U0RmKR9pOUMT9PBEJblMLKSzdSDGA"},"sample_url":"https://www.google.com/patents/reader?id=A_KABwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>