<!DOCTYPE html><html><head><title>专利 CN102842135A - 一种商品图像主体区域检测方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种商品图像主体区域检测方法"><meta name="DC.contributor" content="王海洋" scheme="inventor"><meta name="DC.contributor" content="黄琦" scheme="inventor"><meta name="DC.contributor" content="林建聪" scheme="inventor"><meta name="DC.contributor" content="薛琴" scheme="inventor"><meta name="DC.contributor" content="曾凡涛" scheme="inventor"><meta name="DC.contributor" content="孙凯" scheme="inventor"><meta name="DC.contributor" content="杭州淘淘搜科技有限公司" scheme="assignee"><meta name="DC.date" content="2012-7-17" scheme="dateSubmitted"><meta name="DC.description" content="本发明公开了一种商品图像跨类目检索方法，本发明针对服饰类商品，基于人类视觉注意的模型提出了一种商品图像主体区域检测方法，而不依赖人工交互也不再只突出主体的边缘。通过有效地结合GMM方法、商品图像内容的复杂度以及商品图像中主体颜色的空间信息，自适应地计算图像中每一个像素点的显著性，达到自动检测服饰类商品图像的商品显著区域的目的。方法对非主体区域的背景部分有更好的抑制，对主体区域有更好的突显，对主体本身有更好的强调，并满足图像检索的实时性要求，从而解决了商品图像主体区域检测的问题，能够准确地定位用户感兴趣的部分，帮助用户快速准确地搜索到目标商品。"><meta name="DC.date" content="2012-12-26"><meta name="DC.relation" content="CN:101853299:A" scheme="references"><meta name="DC.relation" content="CN:102567997:A" scheme="references"><meta name="DC.relation" content="JP:2009140058" scheme="references"><meta name="citation_reference" content="SEUNGJI YANG等: &quot;Semantic Home Photo Categorization&quot;, 《IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY》, vol. 17, no. 3, 31 March 2007 (2007-03-31), pages 324 - 335, XP011172323, DOI: doi:10.1109/TCSVT.2007.890829"><meta name="citation_reference" content="SUKHWINDER BIR等: &quot;Color Image Segmentation in CIELab Space Using Hill Climbing Algorithm&quot;, 《INTERNATIONAL JOURNAL OF COMPUTER APPLICATIONS》, vol. 7, no. 3, 30 September 2010 (2010-09-30), pages 48 - 53"><meta name="citation_reference" content="TIE LIU等: &quot;Learning to Detect a Salient Object&quot;, 《IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE》, vol. 33, no. 2, 28 February 2011 (2011-02-28), pages 353 - 367, XP031411999"><meta name="citation_reference" content="闵华清等: &quot;基于时空分析的视频前景提取&quot;, 《模式识别与人工智能》, vol. 24, no. 4, 31 August 2011 (2011-08-31), pages 582 - 590"><meta name="citation_patent_publication_number" content="CN:102842135:A"><meta name="citation_patent_application_number" content="CN:201210249142"><link rel="canonical" href="https://www.google.com/patents/CN102842135A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102842135A?cl=zh"/><meta name="title" content="专利 CN102842135A - 一种商品图像主体区域检测方法"/><meta name="description" content="本发明公开了一种商品图像跨类目检索方法，本发明针对服饰类商品，基于人类视觉注意的模型提出了一种商品图像主体区域检测方法，而不依赖人工交互也不再只突出主体的边缘。通过有效地结合GMM方法、商品图像内容的复杂度以及商品图像中主体颜色的空间信息，自适应地计算图像中每一个像素点的显著性，达到自动检测服饰类商品图像的商品显著区域的目的。方法对非主体区域的背景部分有更好的抑制，对主体区域有更好的突显，对主体本身有更好的强调，并满足图像检索的实时性要求，从而解决了商品图像主体区域检测的问题，能够准确地定位用户感兴趣的部分，帮助用户快速准确地搜索到目标商品。"/><meta property="og:title" content="专利 CN102842135A - 一种商品图像主体区域检测方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102842135A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102842135A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102842135A&amp;usg=AFQjCNEzNFY3ntbSmjD7XsAWwd_rrJxcFA" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/2fc86a010f8ee6ca6377/CN102842135A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/2fc86a010f8ee6ca6377/CN102842135A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102842135A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102842135A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102842135A?cl=zh" style="display:none"><span itemprop="description">本发明公开了一种商品图像跨类目检索方法，本发明针对服饰类商品，基于人类视觉注意的模型提出了一种商品图像主体区域检测方法，而不依赖人工交互也不再只突出主体的边缘。通过有效地结合GMM方法、商品图像内容的复杂...</span><span itemprop="url">https://www.google.com/patents/CN102842135A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102842135A - 一种商品图像主体区域检测方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102842135A - 一种商品图像主体区域检测方法" title="专利 CN102842135A - 一种商品图像主体区域检测方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102842135 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210249142</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年12月26日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年7月17日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年7月17日</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210249142.2, </span><span class="patent-bibdata-value">CN 102842135 A, </span><span class="patent-bibdata-value">CN 102842135A, </span><span class="patent-bibdata-value">CN 201210249142, </span><span class="patent-bibdata-value">CN-A-102842135, </span><span class="patent-bibdata-value">CN102842135 A, </span><span class="patent-bibdata-value">CN102842135A, </span><span class="patent-bibdata-value">CN201210249142, </span><span class="patent-bibdata-value">CN201210249142.2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E6%B5%B7%E6%B4%8B%22">王海洋</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%BB%84%E7%90%A6%22">黄琦</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9E%97%E5%BB%BA%E8%81%AA%22">林建聪</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%96%9B%E7%90%B4%22">薛琴</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9B%BE%E5%87%A1%E6%B6%9B%22">曾凡涛</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%AD%99%E5%87%AF%22">孙凯</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E6%9D%AD%E5%B7%9E%E6%B7%98%E6%B7%98%E6%90%9C%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">杭州淘淘搜科技有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102842135A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102842135A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102842135A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#npl-citations">非专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (1),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210249142&amp;usg=AFQjCNEnjQu6Uje7KbzEPkGtnPybT7W_Xw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102842135A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHsJJ7CQ0M9M1eOqYHqr9HIbMtsOQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT118333743" lang="ZH" load-source="patent-office">一种商品图像主体区域检测方法</invention-title>
      </span><br><span class="patent-number">CN 102842135 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA103754444" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明公开了一种商品图像跨类目检索方法，本发明针对服饰类商品，基于人类视觉注意的模型提出了一种商品图像主体区域检测方法，而不依赖人工交互也不再只突出主体的边缘。通过有效地结合GMM方法、商品图像内容的复杂度以及商品图像中主体颜色的空间信息，自适应地计算图像中每一个像素点的显著性，达到自动检测服饰类商品图像的商品显著区域的目的。方法对非主体区域的背景部分有更好的抑制，对主体区域有更好的突显，对主体本身有更好的强调，并满足图像检索的实时性要求，从而解决了商品图像主体区域检测的问题，能够准确地定位用户感兴趣的部分，帮助用户快速准确地搜索到目标商品。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(1)</span></span></div><div class="patent-text"><div mxw-id="PCLM47847687" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	�种商品图像主体区域检测方法，其特征在于，该方法包括如下步骤：  (1)对商品图像进行图像尺寸缩放、过滤肤色像素以及去除&#38996;色聚类生成的具有较小聚类数目的像素等预处理，以排除与商品主体颜色较不相关像素的干扰；  (2)对步骤（I)得到的商品图像进行&#38996;色分割，得到初始聚类数目和初始聚类中心；利用颜色聚类计算新聚类中心、每类颜色权重以及每类颜色协方差；  (3)将步骤（2)得到的聚类中心、颜色权重、颜色协方差以及步骤（I)得到的商品图像结合，计算主体区域；包括以下子步骤： &#183; 3.	I)对步骤（2)得到的聚类中心、颜色协方差以及步骤（I)得到的商品图像的像素样本值计算指定每类颜色类别下观察像素样本的类条件概率密度；利用指定每类颜色类别下观察像素样本的类条件概率密度和步骤（2)得到的颜色权重计算每类颜色的后验概率，得到颜色概率映射图； &#183; 3.	2)对步骤3. I)得到的颜色概率映射图和步骤（I)得到的商品图像像素样本的空间位置，分别计算每类颜色空间位置的水平方差和垂直方差；&#183;  3.	3)对步骤（I)得到的商品图像分别计算五个区域的&#38996;色直方图并比较峰值个数，当峰值个数满足某条件时，该图就是复杂背景图； &#183; 3.	4)只对背景简单的商品图，利用步骤3. I)得到的颜色概率映射图和步骤（I)得到的商品图像像素样本的空间位置与图像中心位置的距离，计算每类颜色空间位置的中心权重；得到每类颜色的空间分布权重；&#183;  3.	5)将步骤3. I)得到的颜色概率映射图和步骤3. 4)得到的每类颜色的空间分布权重结合，计算图像每个像素样本的概率加权和，得到商品图像中的主体区域并进行显示。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES54895518" lang="ZH" load-source="patent-office" class="description">
    <p>&#8212;种商品图像主体区域检测方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及图像搜索技术领域和人类视觉心理学领域，尤其涉及一种基于人类视觉特性的商品图像主体区域检测方法。</p>
    <p>背景技术</p>
    <p>[0002]	在图像搜索技术中，当用户上传一幅商品图像并期望搜寻与该图相同或相近的商品时，用户更关注商品本身，如T恤、风衣、靴子、卫衣、半身裙、裤子、包包、连衣裙等，且该商品区域最能表现用户的视觉注意程度，从人类视觉感知模型的角度看，用户感兴趣的区域一定是图像或视频中的显著性区域。为了改善用户的视觉 搜索体验，使用户购物体验更友好，对用户重点关注的“感兴趣区域”进行重点分析，通过自动定位、准确捕捉输入商品图中的显著性的主体区域，有效地提高“以图搜图”的检索精度和效率。</p>
    <p>发明内容</p>
    <p>[0003]	本发明的目的在于针对现有技术的不足，提供一种商品图像跨类目检索方法。</p>
    <p>[0004]	本发明的目的是通过以下技术方案来实现的：一种商品图像主体区域检测方法，其特征在于，该方法包括如下步骤：</p>
    <p>[0005]	(I)对商品图像进行图像尺寸缩放、过滤肤色像素以及去除颜色聚类生成的具有较小聚类数目的像素等预处理，以排除与商品主体颜色较不相关像素的干扰；</p>
    <p>[0006]	(2)对步骤（I)得到的商品图像进行颜色分割，得到初始聚类数目和初始聚类中心；利用颜色聚类计算新聚类中心、每类颜色权重以及每类颜色协方差；</p>
    <p>[0007]	(3)将步骤（2)得到的聚类中心、颜色权重、颜色协方差以及步骤（I)得到的商品图像结合，计算主体区域；包括以下子步骤：</p>
    <p>[0008]	3. I)对步骤（2)得到的聚类中心、颜色协方差以及步骤（I)得到的商品图像的像素样本值计算指定每类颜色类别下观察像素样本的类条件概率密度；利用指定每类颜色类别下观察像素样本的类条件概率密度和步骤（2)得到的颜色权重计算每类颜色的后验概率，得到颜色概率映射图；</p>
    <p>[0009]	3. 2)对步骤3. I)得到的颜色概率映射图和步骤（I)得到的商品图像像素样本的空间位置，分别计算每类颜色空间位置的水平方差和垂直方差；</p>
    <p>[0010]	3. 3)对步骤（I)得到的商品图像分别计算五个区域的颜色直方图并比较峰值个数，当峰值个数满足某条件时，该图就是复杂背景图；</p>
    <p>[0011]	3. 4)只对背景简单的商品图，利用步骤3. I)得到的颜色概率映射图和步骤（I)得到的商品图像像素样本的空间位置与图像中心位置的距离，计算每类颜色空间位置的中心权重；得到每类颜色的空间分布权重；</p>
    <p>[0012]	3. 5)将步骤3. I)得到的颜色概率映射图和步骤3. 4)得到的每类颜色的空间分布权重结合，计算图像每个像素样本的概率加权和，得到商品图像中的主体区域并进行显示。</p>
    <p>[0013]	本发明的有益效果是，本发明针对服饰类商品，基于人类视觉注意的模型提出了一种商品图像主体区域检测方法，而不依赖人工交互也不再只突出主体的边缘。通过有效地结合Gaussian Mixture Model (GMM)方法、商品图像内容的复杂度以及商品图像中主体颜色的空间信息，自适应地计算图像中每一个像素点的显著性，达到自动检测服饰类商品图像的商品显著区域的目的。方法对非主体区域的背景部分有更好的抑制，对主体区域有更好的突显,对主体本身有更好的强调，并满足图像检索的实时性要求,从而解决了商品图像主体区域检测的问题，能够准 确地定位用户感兴趣的部分，帮助用户快速准确地搜索到目标商品。</p>
    <p>附图说明</p>
    <p>[0014]	图I是商品图像主体区域检测流程图；</p>
    <p>[0015]	图2是商品图像预处理流程图；</p>
    <p>[0016]	图3是肤色过滤流程图；</p>
    <p>[0017]	图4是Hill-climbing方法流程图；</p>
    <p>[0018]	图5是颜色聚类流程图；</p>
    <p>[0019]	图6是颜色概率映射图生成流程图；</p>
    <p>[0020]	图7是颜色空间分布权重计算流程图；</p>
    <p>[0021]	图8是商品主体区域显示方式示意图。</p>
    <p>具体实施方式</p>
    <p>[0022]	下面以服饰类图像的主体区域检测和显示为例，结合附图详细描述本发明，本发明的目的和效果将变得更加明显。</p>
    <p>[0023]	如图I所示，本发明一种商品图像主体区域检测方法包括如下步骤：</p>
    <p>[0024]	步骤I :对商品图像进行图像尺寸缩放、过滤肤色像素以及去除颜色聚类生成的具有较小聚类数目的像素等预处理，以排除与商品主体颜色较不相关像素的干扰，如图2。</p>
    <p>[0025]	在保持原始商品图像的高宽比不变的前提下，将输入图像缩放到图像最大尺寸(图像高度或图像宽度）为128，该尺寸下得到的主体区域与输入原图尺寸得到的主体区域视觉差异不明显。</p>
    <p>[0026]	在处理包含人体肤色的服饰类商品图时，用户并非重点关注人体肤色，且该类图中的肤色将会干扰颜色的聚类效果，需要去除肤色对商品主体颜色的影响。</p>
    <p>[0027]在本发明中，米用	V. A. Oliveira, A. Conci. Skin Detection using HSV colorspace[J]. Computation Institute-Universidade Federal Fluminense-UFF-Niteroi,Brazil.所述方法，滤除商品图像中的肤色像素。如图3，首先对图像进行肤色检测，由于HSV颜色空间更接近人类的颜色感知，所以将图像从RGB转换到HSV空间，在亚洲人和白种人中肤色的特点是Hue色度通道范围为[0，50]且Saturation饱和度通道范围为[0. 23，0. 68]。仅使用Hue色度通道范围在[6，38]，认为在该范围内的像素值是肤色，设置为255，不在该范围内的像素值是非肤色，设置为O。其次对图像中的肤色与非肤色进行分类，初步肤色检测后，图像中仍存在一些噪声等干扰因素，因此，使用5X5结构元进行形态滤波处理。首先利用该结构元进行膨胀滤波，扩展肤色区域；然后利用该结构元腐蚀图像，消除膨胀效应；再使用3X3中值滤波进行平滑；最后寻找轮廓，计算每个轮廓区域的面积，小于设定阈值250的面积不属于肤色。</p>
    <p>[0028]	在服饰类商品中，商品主体的颜色均较集中或有特定的规律性且在图像中所占比例较大，而在非主体区域中常有一些不同于主体颜色且在图像中所占比例较少的颜色，如：头发、背景装饰品等，需要去除数目较少的颜色类型，以消除这些颜色对占比重较大的颜色的影响。</p>
    <p>[0029]	对排除肤色像素的图像像素样本，在RGB颜色空间，采用指定聚类数目为7的K-means颜色聚类，得到7类颜色聚类结果，在7类颜色聚类结果中，去除聚类个数小于给定阈值50的聚类，保留其它聚类。</p>
    <p>[0030]	步骤2 :对步骤I得到的商品图像进行颜色分割，得到初始聚类数目和初始聚类中心；利用颜色聚类计算新聚类中心、每类颜色权重以及每类颜色协方差。 [0031]	为了排除干扰颜色的影响，对保留下来的图像像素样本的颜色进行聚类，强化图像中的主要颜色,初步地区分出主要主体和背景。</p>
    <p>[0032]在本发明中，米用	S. Bir, A. Kaur. Color Image Segmentation in CIELabSpace Using Hill Climbing Algorithm[J]. International Journal of ComputerApplications (0975-8887), Volume 7-No.3, September 2010.和 R.Achanta,F.Estrada, P. ffils, and S.Susstrunk. Salient Region Detection and Segmentation[J].International Conference on Computer Vision Systems, 2008.所述 Hill-climbing 方法，计算初始聚类数目和初始聚类中心，如图4，在RGB颜色空间下，计算步骤I得到的商品图像的3D彩色直方图，利用3X3X3的搜索窗口寻找10X 10X IObins的直方图中的局部最大值，得到的峰值个数即为初始聚类数目，与峰值对应的bins值即为初始聚类中心。利用非参数方法得到的初始聚类数目和初始聚类中心，采用K-means算法对RGB像素样本值进行颜色聚类，生成新颜色聚类中心和新颜色聚类数目，然后计算每一类的颜色权重，并结合RGB像素样本值计算每类的颜色协方差，如图5。</p>
    <p>[0033]	步骤3 :将步骤2得到的聚类中心、颜色权重、颜色协方差以及步骤I得到的商品图像结合，计算主体区域。</p>
    <p>[0034]	步骤3. I :对步骤（2)得到的聚类中心、颜色协方差以及步骤（I)得到的商品图像的像素样本值计算指定每类颜色类别下观察像素样本的类条件概率密度；利用指定每类颜色类别下观察像素样本的类条件概率密度和步骤（2)得到的颜色权重计算每类颜色的后验概率，得到颜色概率映射图。</p>
    <p>[0035]利用	Roger Jang (张智星）&#183; Data Clustering and Pattern Recognition [Μ] &#183;Chapter 7 ：GMM:高斯混合模型&#183; CS Dept. , Tsing Hua University, Tai wan.所述混合高斯模型，平滑地近似图像样本颜色的密度分布，估计图像中每个像素样本属于某类颜色的概率，进一步区分主要的主体颜色和背景颜色。</p>
    <p>[0036]	根据图像的RGB像素样本值、颜色聚类中心以及颜色协方差，利用多变量的高斯概率密度函数公式（1)，计算指定颜色类别j下观察像素样本χ的类条件概率密度（即g(y,MP I ))。若要得到更准确的颜色聚类中心、每类颜色的权重和每类颜色的协方差，可以米用 Roger Jang (张智星）&#183; Data Clustering and Pattern Recognition [Μ]. Chapter7 :GMM :高斯混合模型&#183; CS Dept. , Tsing Hua University, Taiwan.所述 ExpectationMaximization (EM)方法,对其三个参数进行迭代更新,计算最佳的颜色聚类中心、每类颜色的权重和每类颜色的协方差。</p>
    <p>[0037]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00061.png"> <img id="idf0001" file="CN102842135AD00061.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00061.png" class="patent-full-image" alt="Figure CN102842135AD00061"> </a> </div>
    <p>[0038]	其中，V为像素样本χ的RGB值的向量;M为颜色的聚类中心；Σ为颜色的协方差；exp ( &#183;)为指数函数操作。</p>
    <p>[0039]	根据指定颜色类别j下观察像素样本χ的类条件概率密度8(ν;Μ」，Σ )和每一</p>
    <p>类颜色j的先验概率，即权重α」，利用贝叶斯公式（2)、全概率公式（3)和Roger Jang (张智星）&#183; Data Clustering and Pattern Recognition [Μ]. Chapter 7 ：GMM:高斯混合模型.CS Dept. , Tsing Hua University, Taiwan.所述方法,计算每一类颜色j出现的后验概率P (j Ix)，即给定观察样本X，某类颜色j发生的概率，得到颜色概率映射图，如图6。</p>
    <p>[0040]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00062.png"> <img id="idf0002" file="CN102842135AD00062.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00062.png" class="patent-full-image" alt="Figure CN102842135AD00062"> </a> </div>
    <p>[0041] </p>
    <p>[0042]	 </p>
    <p>[0043]	 [0044]	式中，β」(χ)或p(j |χ)为每一类颜色j出现的后验概率，即给定观察样本X，某类颜色j发生的概率；P (j)为每一类颜色j的先验概率，即每一类颜色的权重；P (x I j)为指定每一类颜色j下观察样本X的类条件概率密度；a j为每一类颜色j的权重；Μ]为每一类颜色j的聚类中心；Σ ^为每一类颜色j的协方差4(ν;Μ,,Σ”为给定每一类颜色j下观察每个像素样本χ的类条件概率密度，即高斯密度函数；ρ (χ)为全概率公式，即样本χ出现的全概率；ρ(χ| I)为指定颜色类别I下观察样本X的类条件概率密度。</p>
    <p>[0045]	可以采用公式（4)的EM算法寻找最佳的颜色聚类中心Μ」、每类颜色的权重a j和每类颜色的协方差&#962; O</p>
    <p>[0046]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00063.png"> <img id="idf0003" file="CN102842135AD00063.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00063.png" class="patent-full-image" alt="Figure CN102842135AD00063"> </a> </div>
    <p>[0049]	式中，为更新后每一类颜色的先验概率，即每一类颜色的权重；β j(Xi)为更新前每一类颜色j出现的后验概率Wi为像素样本Xi的RGB值的向量;Μ」为更新后的每一类颜色的聚类中心\力更新后的每一类颜色的协方差。</p>
    <p>[0050]	步骤3. 2 :对步骤3. I得到的颜色概率映射图和步骤I得到的商品图像像素样本的空间位置，分别计算每类颜色空间位置的水平方差和垂直方差。[0051]	利用步骤3. I得到的颜色概率映射图、商品图像像素样本的空间位置和Liu，T.，Sun，J.，Zheng, N.，Tang, X.，Shumj H. : Learning to detect a salient object [J].In: IEEE CVPR, pp. 1-8 (2007) &#183; /T. Liu, Z. Yuan，J. Sun，J. D. Wang, N. N. Zheng, X. 0. Tang,H. Y. Shum. Learning to Detect a Salient object[J]. IEEE Transaction on PatternAnalysis and Machine Intelligence. Volume 33-No.2, pages 353-367，Feb.201L /Jian Sun，Beijing(CN);Tie Liu, Shaanxi (CN);Xiaoou Tang, Beijing(CN);Heung-YeungShum, Beijing(CN). SALIENT OBJECT DETECTION[P]. Patent No. :US 7940985B2，Date ofPatent:May 10,2011.所述方法，分别按照公式（5)和公式（6)计算每类颜色j空间位置的水平方差Vh和垂直方差Vv，并&#37318;用公式（7)分别将水平方差和垂直方差归一化到[0，1]之间，&#37318;用公式（8)计算每类颜色j的空间总方差V(j)，得到每类颜色空间信息。</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00071.png"> <img id="idf0004" file="CN102842135AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102842135A/CN102842135AD00071.png" class="patent-full-image" alt="Figure CN102842135AD00071"> </a> </div>
    <p>[0059]	式中，Vh和Vv分别为每类颜色j空间位置的水平方差和垂直方差；|X|」为每一类颜色的出现的总概率；P(j Ix)为每一类颜色j出现的后验概率，即给定观察样本X，某类颜色j发生的概率；Xh和yv*别为像素样本X的横坐标和纵坐标;Mh(j)和10)分别为每一类颜色j空间位置的的水平均值和垂直均值-X h,v(j)为每一类颜色j的归一化后的水平或垂直方向的方差；\v(j)为每一类颜色j的归一化前的水平或垂直方向的方差；mir^ Vh,v(j)为每一类颜色j的归一化前的水平或垂直方向的方差的最小值；ma\_ Vh,v(j)为每一类颜色j的归一化前的水平或垂直方向的方差的最大值；V(j)为每一类颜色j的空间总方差。</p>
    <p>[0060]	步骤3. 3 :对步骤I得到的商品图像分别计算五个区域的颜色直方图并比较峰值个数，当峰值个数满足某条件时，该图就是复杂背景图。</p>
    <p>[0061]	米用 Yang, S. , Kim, S. , Ro, Y. Μ. : Semantic Home Photo Categorization. IEEETrans. Circuits Sys. Video Tech. 17, 324 - 335 (2007).所述 PRT (Photographic RegionTemplates)方法，得到图像的四个角的矩形区域块和中心区域块，分别对该五个区域块采用 S. Bir, A.Kaur. Color Image Segmentation in CIELab Space Using Hill ClimbingAlgorithm[J]. International Journal of Computer Applications (0975-8887),Volume7-No. 3, September 2010.和 R. Achanta, F. Estrada, P. ffils, and S.Susstrunk. SalientRegion Detection and Segmentation[J]. International Conference on ComputerVision Systems, 2008.所述Hill-climbing方法,统计五个区域块的颜色直方图的峰值个数，当图像四个角的矩形区域块的颜色直方图的峰值个数的均值大于中心区域块的颜色直方图的峰值个数时，标记该图为复杂背景图。</p>
    <p>[0062]	步骤3. 4 :只对背景简单的商品图，利用步骤3. I得到的颜色概率映射图和步骤I得到的商品图像像素样本的空间位置与图像中心位置的距离，计算每类颜色空间位置的中心权重；得到每类颜色的空间分布权重，如图7。</p>
    <p>[0063]	图像中的主体应尽量分布在图像中部周围，应弱化分布在图像边界周围的颜色，以明确区分出哪些图像像素样本是主体，哪些图像像素样本是背景。</p>
    <p>[0064]	利用公式（9)计算商品图像像素样本χ的空间位置与图像中心位置间的欧氏距离Dx,利用公式（10)，结合颜色概率映射图P (j I χ)，计算，突出在商品图像边界周围的颜色。根据商品图像背景的复杂性，只对背景简单的商品图计算中心权重。 </p>
    <p>[0065]	Dx= I I X-Xcenter I I，X= [xr°W, Χ&#8482;1] T, Xcenter 二 [X:,.，KeLr Y ( 9 )</p>
    <p>[0066]	w(j)= Σ xp(j |x)Dx	(10)</p>
    <p>[0067]	式中，Dx为像素样本χ的空间位置与图像中心位置间的欧氏距离为像素样本X的空间位置坐标=[d,f为图像中心像素样本的空间位置坐标;W(j)为每类颜色j空间位置的中心权重；p(j|x)为给定像素样本X下，某类颜色发生的概率，即颜色概率映射。</p>
    <p>[0068]	步骤3. 5 :将步骤3. I得到的颜色概率映射图和步骤3. 4得到的每类颜色的空间分布权重结合，计算图像每个像素样本的概率加权和，得到商品图像中的主体区域并进行显不O</p>
    <p>[0069]	利用步骤3. I区分出主要的主体颜色和背景颜色以及步骤3. 4强调分布在图像中部周围的颜色的重要性，联合二者，以削弱背景颜色，增强主体颜色。</p>
    <p>[0070]	某类颜色的空间分布的方差值V(j)越大，表明该类颜色成为主体颜色的可能性越小，反之亦然。某类颜色的空间位置中心权重值w(j)越大，表明该类颜色分布得越广、更靠近图像的边界周围区域，这样，该类颜色成为主体颜色的可能性就越小，反之亦然。应该减弱在商品图像边界周围的颜色的重要性，结合给定像素样本X，不同颜色类别j发生的概率P (j IX)(颜色概率映射图)，利用公式（11)计算商品图像F中给定每个像素样本X，在各个颜色类别j下发生的概率加权和，从而抑制背景区域的颜色，突显主体区域的颜色，得到主体区域。</p>
    <p>[0071 ] F⑴=Σ /H./ 卜&#183;）χ U &#8212;「(./)) χ (I- M./))	(11)</p>
    <p>[0072]	式中，p(j|x)为给定像素样本χ下，某类颜色j发生的概率，即颜色概率映射；V(j)为某类颜色j的空间分布的方差值;W(j)为某类颜色j的空间位置中心权重值。</p>
    <p>[0073]	主体区域显示时，有三种方式，如图8，第一、初始的商品主体区域图，商品主体区域以亮度值不等的灰度值[0，255]显示，值越大表明主体越亮，主体越突出，非主体区域为黑色；第二、采用Otsu阈值分割算法，将初始的商品主体区域图二值化，得到新的商品主体区域，商品的主体区域以高亮255显示，非主体区域为黑色；第三、将Otsu算法得到的新主体区域图像与原商品图像进行“&amp;”运算，得到实物图像的主体区域，商品的主体区域以原图像的RGB颜色值显示，非主体区 域为黑色。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102567997A?cl=zh">CN102567997A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年1月4日</td><td class="patent-data-table-td patent-date-value">2012年7月11日</td><td class="patent-data-table-td ">西安电子科技大学</td><td class="patent-data-table-td ">基于稀疏表示和视皮层注意机制的目标检测方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3D2009140058A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHFXxBfr0vsVEzsx3OYGTj6k4j3NQ">JP2009140058A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">没有名称</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">非专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">参考文献</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">SEUNGJI YANG等: "<a href='http://scholar.google.com/scholar?q="Semantic+Home+Photo+Categorization"'>Semantic Home Photo Categorization</a>", 《IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY》, vol. 17, no. 3, 31 March 2007 (2007-03-31), pages 324 - 335, XP011172323, DOI: doi:10.1109/TCSVT.2007.890829</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">SUKHWINDER BIR等: "<a href='http://scholar.google.com/scholar?q="Color+Image+Segmentation+in+CIELab+Space+Using+Hill+Climbing+Algorithm"'>Color Image Segmentation in CIELab Space Using Hill Climbing Algorithm</a>", 《INTERNATIONAL JOURNAL OF COMPUTER APPLICATIONS》, vol. 7, no. 3, 30 September 2010 (2010-09-30), pages 48 - 53</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">TIE LIU等: "<a href='http://scholar.google.com/scholar?q="Learning+to+Detect+a+Salient+Object"'>Learning to Detect a Salient Object</a>", 《IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE》, vol. 33, no. 2, 28 February 2011 (2011-02-28), pages 353 - 367, XP031411999</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td ">闵华清等: "<a href='http://scholar.google.com/scholar?q="%E5%9F%BA%E4%BA%8E%E6%97%B6%E7%A9%BA%E5%88%86%E6%9E%90%E7%9A%84%E8%A7%86%E9%A2%91%E5%89%8D%E6%99%AF%E6%8F%90%E5%8F%96"'>基于时空分析的视频前景提取</a>", 《模式识别与人工智能》, vol. 24, no. 4, 31 August 2011 (2011-08-31), pages 582 - 590</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2015120772A1?cl=zh">WO2015120772A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2015年2月2日</td><td class="patent-data-table-td patent-date-value">2015年8月20日</td><td class="patent-data-table-td ">阿里巴巴集团控股有限公司</td><td class="patent-data-table-td ">一种计算商品图像牛皮癣分值的方法和装置</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=Lm22BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0007000000">G06T7/00</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年12月26日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年2月13日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年9月2日</td><td class="patent-data-table-td ">C05</td><td class="patent-data-table-td ">Deemed withdrawal (patent law before 1993)</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/2fc86a010f8ee6ca6377/CN102842135A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%95%86%E5%93%81%E5%9B%BE%E5%83%8F%E4%B8%BB%E4%BD%93%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B%E6%96%B9.pdf?id=Lm22BwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U0yisobTMp7iPq_T5vUwfFnZgI_sw"},"sample_url":"https://www.google.com/patents/reader?id=Lm22BwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>