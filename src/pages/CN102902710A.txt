<!DOCTYPE html><html><head><title>专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_50a6672b5f82ffbd39b7a9e87fd4594c/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_50a6672b5f82ffbd39b7a9e87fd4594c__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="基于条形码的增强现实方法、系统及移动终端"><meta name="DC.contributor" content="柳寅秋" scheme="inventor"><meta name="DC.contributor" content="李薪宇" scheme="inventor"><meta name="DC.contributor" content="宋海涛" scheme="inventor"><meta name="DC.contributor" content="成都理想境界科技有限公司" scheme="assignee"><meta name="DC.date" content="2012-8-8" scheme="dateSubmitted"><meta name="DC.description" content="本发明公开了一种基于条形码的增强现实方法，将条形码技术与增强现实技术相结合，用条形码做索引进行样本图像及虚拟信息资源搜索，将复杂的图像搜索通过条形码、二维码及其衍生的编解码方式，简化为整形数据搜索或者字符串匹配，降低了系统是实现的难度，相应的，本发明还公开了一种基于条形码的增强现实系统及移动终端，极大提高了增强现实在海量样本图像数据下的应用的可行性。"><meta name="DC.date" content="2013-1-30"><meta name="DC.relation" content="CN:102196245:A" scheme="references"><meta name="DC.relation" content="CN:102411854:A" scheme="references"><meta name="DC.relation" content="CN:102497331:A" scheme="references"><meta name="DC.relation" content="CN:102523758:A" scheme="references"><meta name="DC.relation" content="EP:2299726:A1" scheme="references"><meta name="DC.relation" content="WO:2008109567:A2" scheme="references"><meta name="citation_patent_publication_number" content="CN:102902710:A"><meta name="citation_patent_application_number" content="CN:201210280141"><link rel="canonical" href="https://www.google.com/patents/CN102902710A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102902710A?cl=zh"/><meta name="title" content="专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端"/><meta name="description" content="本发明公开了一种基于条形码的增强现实方法，将条形码技术与增强现实技术相结合，用条形码做索引进行样本图像及虚拟信息资源搜索，将复杂的图像搜索通过条形码、二维码及其衍生的编解码方式，简化为整形数据搜索或者字符串匹配，降低了系统是实现的难度，相应的，本发明还公开了一种基于条形码的增强现实系统及移动终端，极大提高了增强现实在海量样本图像数据下的应用的可行性。"/><meta property="og:title" content="专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102902710A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102902710A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102902710A&amp;usg=AFQjCNG6m7kh3CLLJCCj4IPEcWZ_EaHwMA" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/fe3605f1446da62939cd/CN102902710A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/fe3605f1446da62939cd/CN102902710A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102902710A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102902710A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102902710A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102902710B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102902710A?cl=zh" style="display:none"><span itemprop="description">本发明公开了一种基于条形码的增强现实方法，将条形码技术与增强现实技术相结合，用条形码做索引进行样本图像及虚拟信息资源搜索，将复杂的图像搜索通过条形码、二维码及其衍生的编解码方式，简化为整形数据搜索或者...</span><span itemprop="url">https://www.google.com/patents/CN102902710A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端" title="专利 CN102902710A - 基于条形码的增强现实方法、系统及移动终端"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102902710 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210280141</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年1月30日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年8月8日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年8月8日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102902710B?hl=zh-CN&amp;cl=zh">CN102902710B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210280141.4, </span><span class="patent-bibdata-value">CN 102902710 A, </span><span class="patent-bibdata-value">CN 102902710A, </span><span class="patent-bibdata-value">CN 201210280141, </span><span class="patent-bibdata-value">CN-A-102902710, </span><span class="patent-bibdata-value">CN102902710 A, </span><span class="patent-bibdata-value">CN102902710A, </span><span class="patent-bibdata-value">CN201210280141, </span><span class="patent-bibdata-value">CN201210280141.4</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9F%B3%E5%AF%85%E7%A7%8B%22">柳寅秋</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9D%8E%E8%96%AA%E5%AE%87%22">李薪宇</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%AE%8B%E6%B5%B7%E6%B6%9B%22">宋海涛</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E6%88%90%E9%83%BD%E7%90%86%E6%83%B3%E5%A2%83%E7%95%8C%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">成都理想境界科技有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102902710A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102902710A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102902710A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (6),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (3),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210280141&amp;usg=AFQjCNFOdulF_uyaHnLWy3jcRgKBNXjkVw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102902710A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHanwhkTkKOEqdR8EXmGIJoMRKfVQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT120567568" lang="ZH" load-source="patent-office">基于条形码的增强现实方法、系统及移动终端</invention-title>
      </span><br><span class="patent-number">CN 102902710 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA106311707" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明公开了一种基于条形码的增强现实方法，将条形码技术与增强现实技术相结合，用条形码做索引进行样本图像及虚拟信息资源搜索，将复杂的图像搜索通过条形码、二维码及其衍生的编解码方式，简化为整形数据搜索或者字符串匹配，降低了系统是实现的难度，相应的，本发明还公开了一种基于条形码的增强现实系统及移动终端，极大提高了增强现实在海量样本图像数据下的应用的可行性。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(8)</span></span></div><div class="patent-text"><div mxw-id="PCLM50457087" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种基于条形码的增强现实方法，其特征在于，包括：  用摄像机捕获当前场景图像，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值；  解析所述条形码，并根据解析出的条形码内容在图像数据库中搜索与当前场景图像中的目标图像匹配的样本图像及虚拟信息资源URI ；  实时连续地对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵；  根据所述单应性矩阵及虚拟信息资源URI，在摄像机捕获的当前场景图像中的目标图像位置上，渲染并输出显示所述与目标图像匹配的虚拟信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.如权利要求I所述的方法，其特征在于，所述条形码包括一维条形码或二维条形码或彩色条形码。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.如权利要求2所述的方法，其特征在于，所述对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵，包括：  对摄像机捕获的当前场景图像进行特征检测，提取出图像特征点，并对图像特征点进行特征描述，得到当前场景图像整幅图像的特征描述数据；  根据当前场景图像的特征描述数据及样本图像的特征描述数据，对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.如权利要求I至3中任一项所述的方法，其特征在于，当所述与目标图像匹配的虚拟信息资源为视频资源时，还包括：对所述视频资源进行音视频分离和解码，得到视频各帧图像序列和音频数据；  而所述根据所述单应性矩阵及虚拟信息资源URI，在摄像机捕获的当前场景图像中的目标图像位置上，渲染并输出显示所述与目标图像匹配的虚拟信息步骤，进一步包括：  根据所述单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型；  将所述视频各帧图像序列中的视频帧图像作为纹理逐帧映射到所述3D模型上，进行图形渲染；  将摄像机捕获的当前场景图像与渲染的3D模型融合输出显示，并同步输出所述音频数据。</div>
    </div>
    </div> <div class="claim"> <div num="5" class="claim">
      <div class="claim-text">5.	一种基于条形码的增强现实系统，其特征在于，包括摄像模块、图像特征提取模块、条形码解码模块、搜索模块、图像跟踪配准模块、图像渲染模块、虚实融合显示模块，其中：  摄像模块，用于捕获当前场景图像，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值；  条形码解析模块，解析摄像模块捕获的条形码，得到摄像模块捕获图像中的目标图像的索引值；  搜索模块，根据条形码解析模块解析出的索引值，在图像数据库中进行搜索，将匹配成功的样本图像的特征描述数据传递给图像跟踪配准模块，同时将匹配成功的虚拟信息资源URI传递给图形渲染模块；  图像特征提取模块，对摄像模块捕获的当前场景图像进行特征点检测及提取，并生成特征点描述，得到当前场景图像的特征描述数据；图像跟踪配准模块，根据搜索模块返回的样本图像的特征描述数据及图像特征提取模块产生的当前场景图像的特征描述数据，实时连续地对摄像模块捕获的当前场景图像和样本图像进行跟踪配准，得到单应性矩阵；  图形渲染模块，根据图像跟踪配准模块计算出的单应性矩阵及虚拟信息资源URI，在摄像模块捕获的当前场景图像中的目标图像位置上，渲染与目标图像匹配的虚拟信息；  虚实融合显示模块，将摄像模块捕获的当前场景图像与图形渲染模块渲染的虚拟信息融合输出显不。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.如权利要求5所述的系统，其特征在于，所述系统还包括音视频分离及解码模块，当所述搜索模块搜索得到的与目标图像匹配的虚拟信息为视频资源时：  所述搜索模块将匹配成功的视频资源URI传递给音视频分离及解码模块；   所述音视频分离及解码模块，根据搜索模块得到的视频资源的URI，找到对应的视频文件，并对所述视频文件进行音视频分离和解码，得到视频各帧图像序列和音频数据；并将所述视频各帧图像序列传递给图形渲染模块，将所述音频数据传递给虚实融合显示模块；所述图形渲染模块，根据图像跟踪配准模块计算出的单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型，并将音视频分离及解码模块提取出来的视频各帧图像作为纹理逐帧映射到所述3D模型上，完成图形渲染；  所述虚实融合显示模块，将摄像机模块捕获的当前场景图像与图形渲染模块渲染的虚拟信息融合输出显示，并同步输出音频数据。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.如权利要求5或6所述的系统，其特征在于，所述条形码为一维条形码或二维条形码或彩色条形码。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.	一种移动终端，其特征在于，所述移动终端包括权利要求5至7中任一项所述的基于条形码的增强现实系统。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES57516426" lang="ZH" load-source="patent-office" class="description">
    <p>基于条形码的增强现实方法、系统及移动终端</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及移动增强现实领域，尤其涉及一种基于条形码的增强现实方法、系统及移动终端。</p>
    <p>背景技术</p>
    <p>[0002]	近年来，条形码技术（包括一维条形码、二维条形码等）得到了广泛应用，同时增强现实（AR, Augumented Reality)技术也在一些应用中开始出现,增强现实是在真实环境呈现的场景中，借助与计算机图形技术和可视化技术，将虚拟信息实时“无缝”与之融合，利用虚拟信息对真实场景进行补充、增强，能可给予用户完全区别于传统PC平台的全新的感知体验和交互模式。</p>
    <p>[0003]	现有条形码技术和增效现实技术一般都是单独使用的，单独使用条形码只能得到 条形码对应的网址，使用起来不够炫；而单独使用现有增强现实技术，系统的响应时间长，海量数据应用可行性低。</p>
    <p>[0004]	当然也有少数公开方案将条形码与增强现实技术结合，如中国发明专利CN200910224378. 9(公布号：CN102087743A)中公开了一种条形码扩充实境系统与方法，其技术方案中，条形码扩充实境方法主要是根据取得的条形码标志的空间信息，将虚拟信息堆栈到实境影像或另一背景图像中（参见权利要求16及说明书[0038]、[0041]段），该方法虚拟信息的堆栈位置，主要取决于条形码标志的影像信息，如果在虚拟信息堆栈过程中，将该条形码部分或全部遮挡，该方法将无法找到虚拟信息堆栈位置，造成堆栈位置偏差，影响用户体验。</p>
    <p>发明内容</p>
    <p>[0005]	本发明的目的是提供一种基于条形码的增强现实方法、系统及移动终端，用完全不同于上述专利公开的技术，将人们开始广泛使用的二维码与增强现实技术相结合，既解决了单独使用条形码不够炫，又能解决单独使用现有增强现实技术响应时间长的问题，能给予用户完全区别于传统拍照应用的全新感知体验和交互模式。</p>
    <p>[0006]	为了实现上述发明目的，本发明提供了一种基于条形码的增强现实方法，包括：</p>
    <p>[0007]	用摄像机捕获当前场景图像，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值；</p>
    <p>[0008]	解析所述条形码，并根据解析出的条形码内容在图像数据库中搜索与当前场景图像中的目标图像匹配的样本图像及虚拟信息资源URI ；</p>
    <p>[0009]	实时连续地对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵；</p>
    <p>[0010]	根据所述单应性矩阵及虚拟信息资源URI，在摄像机捕获的当前场景图像中的目标图像位置上，渲染并输出显示所述与目标图像匹配的虚拟信息。</p>
    <p>[0011]	优选的，所述条形码为一维条形码或二维条形码或彩色条形码。[0012]	优选的，所述对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵，包括：对摄像机捕获的当前场景图像进行特征检测，提取出图像特征点，并对图像特征点进行特征描述，得到当前场景图像整幅图像的特征描述数据；根据当前场景图像的特征描述数据及样本图像的特征描述数据，对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵。</p>
    <p>[0013]	优选的，当所述与目标图像匹配的虚拟信息资源为视频资源时，还包括：对所述视频资源进行音视频分离和解码，得到视频各帧图像序列和音频数据；</p>
    <p>[0014]	而所述根据所述单应性矩阵及虚拟信息资源URI，在摄像机捕获的当前场景图像中的目标图像位置上，渲染并输出显示所述与目标图像匹配的虚拟信息步骤，进一步包括：</p>
    <p>[0015]	根据所述单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型； </p>
    <p>[0016]	将所述视频各帧图像序列中的视频帧图像作为纹理逐帧映射到所述3D模型上，进行图形渲染；</p>
    <p>[0017]	将摄像机捕获的当前场景图像与渲染的3D模型融合输出显示，并同步输出所述音频数据。</p>
    <p>[0018]	相应的，本发明还提供了一种基于条形码的增强现实系统，包括摄像模块、图像特征提取模块、条形码解码模块、搜索模块、图像跟踪配准模块、图像渲染模块、虚实融合显示模块，其中：</p>
    <p>[0019]	摄像模块，用于捕获当前场景图像，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值，所述条形码可以为一维条形码，也可以为二维条形码、彩色条形码及其衍生的编解码，还可以为自定义编解码方式编出的条码图案；</p>
    <p>[0020]	条形码解析模块，解析摄像模块捕获的条形码，得到摄像模块捕获图像中的目标图像的索引值；</p>
    <p>[0021]	搜索模块，根据条形码解析模块解析出的索引值，在图像数据库中进行搜索，将匹配成功的样本图像的特征描述数据传递给图像跟踪配准模块，同时将匹配成功的虚拟信息资源URI传递给图形渲染模块；</p>
    <p>[0022]图像特征提取模块，对摄像模块捕获的当前场景图像进行特征点检测及提取，并生成特征点描述，得到当前场景图像的特征描述数据；</p>
    <p>[0023]图像跟踪配准模块，根据搜索模块返回的样本图像的特征描述数据及图像特征提取模块产生的当前场景图像的特征描述数据，实时连续地对摄像模块捕获的当前场景图像和样本图像进行跟踪配准，得到单应性矩阵；</p>
    <p>[0024]图形渲染模块，根据图像跟踪配准模块计算出的单应性矩阵及虚拟信息资源URI，在摄像模块捕获的当前场景图像中的目标图像位置上，渲染与目标图像匹配的虚拟信息；</p>
    <p>[0025]	虚实融合显示模块，将摄像模块捕获的当前场景图像与图形渲染模块渲染的虚拟信息融合输出显不。</p>
    <p>[0026]	优选的，所述系统还包括音视频分离及解码模块，当所述搜索模块搜索得到的与目标图像匹配的虚拟信息为视频资源时：</p>
    <p>[0027]	所述搜索模块将匹配成功的视频资源URI传递给音视频分离及解码模块；[0028]	所述音视频分离及解码模块，根据搜索模块得到的视频资源的URI，找到对应的视频文件，并对所述视频文件进行音视频分离和解码，得到视频各帧图像序列和音频数据；并将所述视频各帧图像序列传递给图形渲染模块，将所述音频数据传递给虚实融合显示模块；</p>
    <p>[0029]	所述图形渲染模块，根据图像跟踪配准模块计算出的单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型，并将音视频分离及解码模块提取出来的视频各帧图像作为纹理逐帧映射到所述3D模型上，完成图形渲染；</p>
    <p>[0030]	所述虚实融合显示模块，将摄像机模块捕获的当前场景图像与图形渲染模块渲染的虚拟信息融合输出显示,并同步输出音频数据。</p>
    <p>[0031]	相应的，本发明还提供了一种移动终端，所述移动终端包括上述的基于条形码的增强现实系统。</p>
    <p>&#183;[0032]	本发明具有如下有意效果：</p>
    <p>[0033]	I、通过条形码来进行图像搜索，缩短图像配准时间，在海量样本图像条件下，仍能到达系统实时性要求，极大提高了增强现实在海量样本图像数据下的应用的可行性；</p>
    <p>[0034]	2、将复杂的图像搜索通过条形码、二维码及其衍生的编解码方式，简化为整形数据搜索或者字符串匹配，降低了系统是实现的难度；</p>
    <p>[0035]	3、由于条形码仅用于搜索，在搜索完成后，无论条形码是否遮挡，均可以准确地在目标图像上进行虚拟信息渲染。</p>
    <p>附图说明</p>
    <p>[0036]	为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动性的前提下，还可以根据这些附图获得其他的附图：</p>
    <p>[0037]	图I为本发明实施例中基于条形码的增强现实系统结构示意图一；</p>
    <p>[0038]	图2为图I中系统进行虚拟信息叠加的过程及效果示意图；</p>
    <p>[0039]	图3为本发明实施例中基于条形码的增强现实系统结构示意图二 ；</p>
    <p>[0040]	图4为图2中系统进行虚拟信息叠加的过程及效果示意图；</p>
    <p>[0041]	图5为本发明实施例中基于条形码的增强现实方法的流程示意图；</p>
    <p>[0042]	图6为自定义条形码标志中的一种形式示意图。</p>
    <p>具体实施方式</p>
    <p>[0043]	下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有作出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。</p>
    <p>[0044]	参见图1，为本发明实施例中基于条形码的增强现实系统结构示意图一，该系统包括摄像模块I (摄像模块包括移动终端中的摄像机）、条形码解码模块2、搜索模块3、图像特征提取模块4、图像跟踪配准模块5、图像渲染模块6、虚实融合显示模块7，其中：[0045]	所述摄像模块1，用于捕获当前场景图像，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值，所述条形码可以为一维条形码，也可以为二维条形码、彩色条形码及其衍生的编解码，还可以为自定义编解码方式编出的条码图案(参见图6)。所述摄像模块I分别与条形码解码模块2、图像特征提取模块4和虚实融合显示模块7相连，摄像模块I将捕获的条形码图像传递给条形码解码模块2进行解码，将捕获的当前场景图像传给图像特征提取模块4用于特征提取，同时将捕获的图像传递给虚实融合显不模块7用于融合输出显不。</p>
    <p>[0046]	所述条形码解析模块2，用于解析摄像模块I捕获的条形码，得到摄像模块I所捕获的图像中的目标图像的索引值；</p>
    <p>[0047]	所述搜索模块3，用于根据条形码解析模块2解析出的索引值，在服务器端的图像数据库中进行搜索，将匹配成功的样本图像的特征描述数据传递给图像跟踪配准模块5，同时将匹配成功的虚拟信息资源URI传递给图形渲染模块6。本实施例中条形码与样本图像是一一对应的，因此系统能够通过对条形码的解码搜索到相匹配的样本图像及其相关资 源。</p>
    <p>[0048]	所述图像特征提取模块4，用于对摄像模块捕获的当前场景图像进行特征点检测及提取，并生成特征点描述，得到当前场景图像的特征描述数据，并将其传递给图像跟踪配准模块5。</p>
    <p>[0049]	所述图像跟踪配准模块5，根据搜索模块3返回的样本图像的特征描述数据及图像特征提取模块4产生的当前场景图像的特征描述数据，实时连续地对摄像模块I捕获的当前场景图像和样本图像进行跟踪配准，计算摄像模块的姿态，得到单应性矩阵，并将单应性矩阵传递给图形渲染模块6。</p>
    <p>[0050]	所述图形渲染模块6，根据图像跟踪配准模块5计算出的单应性矩阵及搜索模块3返回的虚拟信息资源URI，在摄像模块I捕获的当前场景图像中的目标图像位置上，渲染与目标图像匹配的虚拟信息；</p>
    <p>[0051]	虚实融合显示模块7，将摄像模块I捕获的当前场景图像与图形渲染模块6渲染的虚拟信息融合输出显示。</p>
    <p>[0052]	在本实施例中，由于搜索是按照条形码代表的索引值进行的，摄像模块I可以一直同时拍摄条形码与目标图像，也可以先拍摄条形码，再拍摄目标图像，即摄像模块先拍摄条形码实现搜索，再通过拍摄目标图像实现图像跟踪配准，以叠加虚拟信息。在搜索过程完成后，无论条形码是否处于遮挡状态或是否处于摄像模块拍摄范围内，虚拟信息的叠加均不受影响。</p>
    <p>[0053]	参见图2，为图I中系统进行虚拟信息叠加的过程及效果示意图，摄像模块捕获的当前场景图像即图2中的真实场景图像，如图2示例：真实场景图像中包含有目标图像和位于目标图像附近的条形码，该条形码解码后得到目标图像的索引值（如图2中的110101101001100011)，根据该索引值，在服务器端的图像数据库中进行搜索，得到匹配的样本图像和虚拟信息资源（或虚拟信息资源的URI)，所述虚拟信息资源的内容可以是3D模型或者视频，如图2以3D模型资源为例。在通过条形码搜索到样本图像以后，对摄像机捕获的真是场景图像和样本图像进行配准并跟踪，计算摄像机的姿态，得到单应性矩阵，并根据单应性矩阵，在摄像机捕获图像中的目标图像位置上渲染所述3D模型。[0054]	本发明实施例中，根据索引的不同，样本图像的存储结构与搜索方式也有差异：若索引为无符号整形，则使用B树，B+树，B*树，Hash表等方法进行索引；若索引为字符串，则使用字典树。除了使用标准的一维条形码和二维码编解码来实现图像搜索功能，也可自定义编解码方式和条码图案，来满足美观上的需求。例如图6所示，将条形码用黑白方块代替，其中黑色代表1，白色代表0，反之亦可，如此与产品Logo融为一体（图6中的Idealsee为Logo)，既不会给用户突兀的感觉，又能用简单的编解码方式得到目标图像的索引，需要说明的是，图6中的自定义编码方式仅仅为一种示例，并不代表仅能用这种自定义编码形式，不能作为对本发明的限定。</p>
    <p>[0055]	参见图3，为本发明实施例中基于条形码的增强现实系统结构示意图二，本实施例增强现实系统在图I的基础上增加了一个音视频分离及解码模块8，当所述搜索模块3搜索得到的与目标图像匹配的虚拟信息为视频资源时：</p>
    <p>[0056]	所述搜索模块3将匹配成功的视频资源URI传递给音视频分离及解码模块；</p>
    <p>[0057]	所述音视频分离及解码模块8，根据来自搜索模块3的视频资源的URI，找到对应 的视频文件，并对所述视频文件进行音视频分离和解码，得到视频各帧图像序列和音频数据；并将所述视频各帧图像序列传递给图形渲染模块6，将所述音频数据传递给虚实融合显示模块7 ;</p>
    <p>[0058]	所述图形渲染模块6，根据图像跟踪配准模块5计算出的单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型，并将音视频分离及解码模块提取出来的视频各帧图像作为纹理逐帧映射到所述3D模型上，并实时更新，完成图形渲染；所述与目标图像完全覆盖包括与目标图像重合，例如：当目标图像为长方形时，可绘制出完全与目标图像重合的长方形平面3D模型，而当目标图像为非矩形形状时，则绘制出完全覆盖该目标图像的矩形平面3D模型。</p>
    <p>[0059]	所述虚实融合显示模块7，将摄像机模块I捕获的当前场景图像与图形渲染模块6渲染的虚拟信息融合输出显示，并同步输出音频数据。</p>
    <p>[0060]	音频数据和视频帧图像能够实现同步，需要在视频文件分离为视频流和音频流之后，对两者进行数据包分割。其中，视频数据包根据其时间戳按照先后顺序组成链表，形成一个有序队列，分别对每个数据包进行解码并提取其中的图像，则得到视频每一帧的图像序列，用时间戳控制图像序列的输出。图形渲染模块则将视频帧图像作为纹理映射到3D模型上时，由于输出图像随时间有序变化,3D模型的纹理也随之变化，完成视频的播放。此外，音频流也被分为数据包，以视频数据包的时间戳为基准，调整音频数据包的输出，使音视频同步输出。</p>
    <p>[0061]	本发明实施例能通过将虚拟视频文件分离解码，将视频叠加到真实场景中，如：能将报刊杂志等平面媒体上的图片信息，通过本系统转化为视频信息，视频与图片在空间位置上完全贴合，使用户获得“寓情于景”的全新视听体验。</p>
    <p>[0062]	参见图4，为图3系统中虚拟信息叠加的过程及效果示意图，同样是通过条形码解码出来的索引值，去服务器端的数据库中搜索匹配的样本图像和虚拟信息资源。当搜索出的虚拟信息资源为视频资源时，首选绘制出与目标图像位置重合的3D模型，同时将视频文件分离解码，从分离出来视频帧图像序列中提取出视频各帧图像，并将其作为纹理逐帧映射到长方形平面3D模型上，实时更新，完成图形渲染，实现将视频叠加到真实场景中的目标图片上，同时根据视频图像的渲染进度同步输出从视频文件中分离出来的音频数据。</p>
    <p>[0063]	参见图5，为本发明实施例中基于条形码的增强现实方法的流程示意图，包括如下步骤：</p>
    <p>[0064]	SlOl :用摄像机捕获当前场景图像 ，所述当前场景图像包括目标图像和条形码，所述条形码标志的内容为目标图像的索引值，所述条形码可以为常规使用的一维条形码或二维条形码或彩色条形码，也可以为自定义的各种编码。</p>
    <p>[0065]	S102 :解析所述条形码，并根据解析出的条形码内容（即索引值）在服务器端的图像数据库中搜索与当前场景图像中的目标图像匹配的样本图像及虚拟信息资源URI ；</p>
    <p>[0066]	S103:实时连续地对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵；</p>
    <p>[0067]	S104:根据所述单应性矩阵及虚拟信息资源URI，在摄像机捕获的当前场景图像中的目标图像位置上，渲染并输出显示所述与目标图像匹配的虚拟信息。</p>
    <p>[0068]	其中，步骤S103中，得到单应性矩阵的过程具体为：对摄像机捕获的当前场景图像进行特征检测，提取出图像特征点，并对图像特征点进行特征描述，得到当前场景图像整幅图像的特征描述数据；根据当前场景图像的特征描述数据及样本图像的特征描述数据，对摄像机捕获的当前场景图像和样本图像进行跟踪配准，计算摄像机的姿态，得到单应性矩阵。</p>
    <p>[0069]	优选的，当步骤S102中搜索得到的与目标图像匹配的虚拟信息资源为视频资源时，本发明实施例中，增强现实的方法还包括：对所述视频信息进行音视频分离和解码，得到视频各帧图像序列和音频数据；在这种情况下，步骤S104进一步包括：根据所述单应性矩阵，绘制出能将当前场景图像中的目标图像完全覆盖的矩形平面3D模型；将所述视频各帧图像序列中的视频帧图像作为纹理逐帧映射到所述3D模型上，进行图形渲染；将摄像机捕获的当前场景图像与渲染的3D模型融合输出显示，并同步输出所述音频数据。</p>
    <p>[0070]	在本实施例方法中，由于搜索是按照条形码代表的索引值进行的，摄像机可以一直同时拍摄条形码与目标图像，也可以先拍摄条形码，再拍摄目标图像，即摄像机先拍摄条形码实现搜索，再通过拍摄目标图像实现图像跟踪配准，以叠加虚拟信息。在搜索过程完成后，无论条形码是否处于遮挡状态或是否处于摄像模块拍摄范围内，虚拟信息的叠加均不受影响。本发明实施例基于条形码的增强现实方法的过程及效果，同样可参见图2或图4。</p>
    <p>[0071]	本发明除了上述的增强现实系统和方法外，还提供了一种移动终端，所述移动终端包括上述的基于条形码的增强现实系统。</p>
    <p>[0072]	本发明通过条形码或二维码来进行图像搜索，缩短图像配准时间，能在海量样本图像条件下，达到系统实时性要求。</p>
    <p>[0073]	本说明书中公开的所有特征，或公开的所有方法或过程中的步骤，除了互相排斥的特征和/或步骤以外，均可以以任何方式组合。</p>
    <p>[0074]	本说明书（包括任何附加权利要求、摘要和附图）中公开的任一特征，除非特别叙述，均可被其他等效或具有类似目的的替代特征加以替换。即，除非特别叙述，每个特征只是一系列等效或类似特征中的一个例子而已。</p>
    <p>[0075]	本发明并不局限于前述的具体实施方式。本发明扩展到任何在本说明书中披露的新特征或任何新的组合，以及披露的任一新的方法或过程的步骤或任何新的组合。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102196245A?cl=zh">CN102196245A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年4月7日</td><td class="patent-data-table-td patent-date-value">2011年9月21日</td><td class="patent-data-table-td ">北京中星微电子有限公司</td><td class="patent-data-table-td ">一种角色互动的视频播放方法和视频播放装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102411854A?cl=zh">CN102411854A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年9月1日</td><td class="patent-data-table-td patent-date-value">2012年4月11日</td><td class="patent-data-table-td ">北京传动新媒体技术有限公司</td><td class="patent-data-table-td ">基于增强现实的课堂教学混合技术应用系统及方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102497331A?cl=zh">CN102497331A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年12月16日</td><td class="patent-data-table-td patent-date-value">2012年6月13日</td><td class="patent-data-table-td ">于凯</td><td class="patent-data-table-td ">一种信息提供方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102523758A?cl=zh">CN102523758A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年8月18日</td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td ">新日铁系统集成株式会社</td><td class="patent-data-table-td ">增强现实提供系统、信息处理终端、信息处理装置、增强现实提供方法、信息处理方法以及程序</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2299726A1?cl=zh">EP2299726A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年6月17日</td><td class="patent-data-table-td patent-date-value">2011年3月23日</td><td class="patent-data-table-td ">Huawei Device Co., Ltd.</td><td class="patent-data-table-td ">Video communication method, apparatus and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008109567A2?cl=zh">WO2008109567A2</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年3月3日</td><td class="patent-data-table-td patent-date-value">2008年9月12日</td><td class="patent-data-table-td ">Organic Motion</td><td class="patent-data-table-td ">System and method for tracking three dimensional objects</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103246742A?cl=zh">CN103246742A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年5月20日</td><td class="patent-data-table-td patent-date-value">2013年8月14日</td><td class="patent-data-table-td ">成都理想境界科技有限公司</td><td class="patent-data-table-td ">图像检索触发方法及增强现实方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103398717A?cl=zh">CN103398717A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年8月22日</td><td class="patent-data-table-td patent-date-value">2013年11月20日</td><td class="patent-data-table-td ">成都理想境界科技有限公司</td><td class="patent-data-table-td ">全景地图数据库采集系统及基于视觉的定位、导航方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103398717B?cl=zh">CN103398717B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年8月22日</td><td class="patent-data-table-td patent-date-value">2016年4月20日</td><td class="patent-data-table-td ">成都理想境界科技有限公司</td><td class="patent-data-table-td ">全景地图数据库采集系统及基于视觉的定位、导航方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103853819A?cl=zh">CN103853819A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2014年2月13日</td><td class="patent-data-table-td patent-date-value">2014年6月11日</td><td class="patent-data-table-td ">腾讯科技（深圳）有限公司</td><td class="patent-data-table-td ">一种信息获取的方法、终端及系统</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0017000000">G06T17/00</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=KRm8BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0007000000">G06T7/00</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年1月30日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年3月13日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年8月26日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/fe3605f1446da62939cd/CN102902710A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_50a6672b5f82ffbd39b7a9e87fd4594c.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E5%9F%BA%E4%BA%8E%E6%9D%A1%E5%BD%A2%E7%A0%81%E7%9A%84%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E6%96%B9%E6%B3%95_%E7%B3%BB.pdf?id=KRm8BwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U2y-X1nC2dxhve0jG8cwD3IKedzWg"},"sample_url":"https://www.google.com/patents/reader?id=KRm8BwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>