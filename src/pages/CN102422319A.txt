<!DOCTYPE html><html><head><title>专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="图像检索方法、图像检索程序和图像登记方法"><meta name="DC.contributor" content="古桥幸人" scheme="inventor"><meta name="DC.contributor" content="岩村雅一" scheme="inventor"><meta name="DC.contributor" content="&#23791;泰治" scheme="inventor"><meta name="DC.contributor" content="野口和人" scheme="inventor"><meta name="DC.contributor" content="黄濑浩一" scheme="inventor"><meta name="DC.contributor" content="公立大学法人大阪府立大学" scheme="assignee"><meta name="DC.contributor" content="奥林巴斯株式会社" scheme="assignee"><meta name="DC.date" content="2010-3-3" scheme="dateSubmitted"><meta name="DC.description" content="一种图像检索方法，包括以下步骤：从拍摄有待检索对象的查询图像提取表示查询图像的局部特征的至少一个查询特征向量；访问图像数据库，多个参考图像、从各参考图像生成的学习图像以及表示参考图像和学习图像的局部特征的多个参考特征向量彼此关联地预先存储在该图像数据库中；使用近似最近邻搜索匹配查询特征向量和与各参考图像关联的参考特征向量，并找到近似最接近查询特征向量的参考特征向量；以及选择与所找到的各参考特征向量关联的参考图像作为检索结果，其中：通过对各参考图像进行施加在拍摄待检索对象的图像时可能出现的模糊和/或变化的图像处理来生成学习图像；使用尺度空间技术分别从参考图像和对应于参考图像的学习图像提取各参考特征向量；使用尺度空间技术从查询图像提取查询特征向量；以及用计算机执行上述各步骤。"><meta name="DC.date" content="2012-4-18"><meta name="DC.relation" content="CN:101140624:A" scheme="references"><meta name="DC.relation" content="CN:101216841:A" scheme="references"><meta name="DC.relation" content="WO:2008026414:A1" scheme="references"><meta name="citation_patent_publication_number" content="CN:102422319:A"><meta name="citation_patent_application_number" content="CN:201080019955"><link rel="canonical" href="https://www.google.com/patents/CN102422319A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102422319A?cl=zh"/><meta name="title" content="专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法"/><meta name="description" content="一种图像检索方法，包括以下步骤：从拍摄有待检索对象的查询图像提取表示查询图像的局部特征的至少一个查询特征向量；访问图像数据库，多个参考图像、从各参考图像生成的学习图像以及表示参考图像和学习图像的局部特征的多个参考特征向量彼此关联地预先存储在该图像数据库中；使用近似最近邻搜索匹配查询特征向量和与各参考图像关联的参考特征向量，并找到近似最接近查询特征向量的参考特征向量；以及选择与所找到的各参考特征向量关联的参考图像作为检索结果，其中：通过对各参考图像进行施加在拍摄待检索对象的图像时可能出现的模糊和/或变化的图像处理来生成学习图像；使用尺度空间技术分别从参考图像和对应于参考图像的学习图像提取各参考特征向量；使用尺度空间技术从查询图像提取查询特征向量；以及用计算机执行上述各步骤。"/><meta property="og:title" content="专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102422319A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102422319A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102422319A&amp;usg=AFQjCNGxyLlRTQI5QA6z0JhvW9KGvFm81Q" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/8bece3a1dba5f583c7a9/CN102422319A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/8bece3a1dba5f583c7a9/CN102422319A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102422319A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102422319A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102422319A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102422319B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102422319A?cl=zh" style="display:none"><span itemprop="description">一种图像检索方法，包括以下步骤：从拍摄有待检索对象的查询图像提取表示查询图像的局部特征的至少一个查询特征向量；访问图像数据库，多个参考图像、从各参考图像生成的学习图像以及表示参考图像和学习图像的局部特...</span><span itemprop="url">https://www.google.com/patents/CN102422319A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法" title="专利 CN102422319A - 图像检索方法、图像检索程序和图像登记方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102422319 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201080019955</td></tr><tr><td class="patent-bibdata-heading"> 专利合作条约 (PCT) 编号</td><td class="single-patent-bibdata">PCT/JP2010/053446</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年4月18日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2010年3月3日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2009年3月4日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102422319B?hl=zh-CN&amp;cl=zh">CN102422319B</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2405391A1?hl=zh-CN&amp;cl=zh">EP2405391A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2405391A4?hl=zh-CN&amp;cl=zh">EP2405391A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8818103?hl=zh-CN&amp;cl=zh">US8818103</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20120051628?hl=zh-CN&amp;cl=zh">US20120051628</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2010101186A1?hl=zh-CN&amp;cl=zh">WO2010101186A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201080019955.1, </span><span class="patent-bibdata-value">CN 102422319 A, </span><span class="patent-bibdata-value">CN 102422319A, </span><span class="patent-bibdata-value">CN 201080019955, </span><span class="patent-bibdata-value">CN-A-102422319, </span><span class="patent-bibdata-value">CN102422319 A, </span><span class="patent-bibdata-value">CN102422319A, </span><span class="patent-bibdata-value">CN201080019955, </span><span class="patent-bibdata-value">CN201080019955.1, </span><span class="patent-bibdata-value">PCT/2010/53446, </span><span class="patent-bibdata-value">PCT/JP/10/053446, </span><span class="patent-bibdata-value">PCT/JP/10/53446, </span><span class="patent-bibdata-value">PCT/JP/2010/053446, </span><span class="patent-bibdata-value">PCT/JP/2010/53446, </span><span class="patent-bibdata-value">PCT/JP10/053446, </span><span class="patent-bibdata-value">PCT/JP10/53446, </span><span class="patent-bibdata-value">PCT/JP10053446, </span><span class="patent-bibdata-value">PCT/JP1053446, </span><span class="patent-bibdata-value">PCT/JP2010/053446, </span><span class="patent-bibdata-value">PCT/JP2010/53446, </span><span class="patent-bibdata-value">PCT/JP2010053446, </span><span class="patent-bibdata-value">PCT/JP201053446</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%8F%A4%E6%A1%A5%E5%B9%B8%E4%BA%BA%22">古桥幸人</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%B2%A9%E6%9D%91%E9%9B%85%E4%B8%80%22">岩村雅一</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%B3%AF%E6%B3%B0%E6%B2%BB%22">&#23791;泰治</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%87%8E%E5%8F%A3%E5%92%8C%E4%BA%BA%22">野口和人</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%BB%84%E6%BF%91%E6%B5%A9%E4%B8%80%22">黄濑浩一</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%85%AC%E7%AB%8B%E5%A4%A7%E5%AD%A6%E6%B3%95%E4%BA%BA%E5%A4%A7%E9%98%AA%E5%BA%9C%E7%AB%8B%E5%A4%A7%E5%AD%A6%22">公立大学法人大阪府立大学</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%A5%A5%E6%9E%97%E5%B7%B4%E6%96%AF%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%22">奥林巴斯株式会社</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102422319A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102422319A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102422319A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (5),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201080019955&amp;usg=AFQjCNHcpWK9_kLRGpMQsDPohi6bUKIxYA"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102422319A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGzC7qBK5MOgLvUSAX8LjnSGOszIQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT112144513" lang="ZH" load-source="patent-office">图像检索方法、图像检索程序和图像登记方法</invention-title>
      </span><br><span class="patent-number">CN 102422319 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA95612525" lang="ZH" load-source="patent-office">
    <div class="abstract">一种图像检索方法，包括以下步骤：从拍摄有待检索对象的查询图像提取表示查询图像的局部特征的至少一个查询特征向量；访问图像数据库，多个参考图像、从各参考图像生成的学习图像以及表示参考图像和学习图像的局部特征的多个参考特征向量彼此关联地预先存储在该图像数据库中；使用近似最近邻搜索匹配查询特征向量和与各参考图像关联的参考特征向量，并找到近似最接近查询特征向量的参考特征向量；以及选择与所找到的各参考特征向量关联的参考图像作为检索结果，其中：通过对各参考图像进行施加在拍摄待检索对象的图像时可能出现的模糊和/或变化的图像处理来生成学习图像；使用尺度空间技术分别从参考图像和对应于参考图像的学习图像提取各参考特征向量；使用尺度空间技术从查询图像提取查询特征向量；以及用计算机执行上述各步骤。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(7)</span></span></div><div class="patent-text"><div mxw-id="PCLM40936254" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种图像检索方法，包括以下步骤：从拍摄有图像检索对象的查询图像提取至少一个查询特征向量，所述查询特征向量表示所述查询图像的局部特征；访问预先存储了多个参考图像的图像数据库，各参考图像与从该参考图像生成的学习图像以及表示该参考图像和该学习图像的局部特征的参考特征向量相关联地存储；比较步骤，使用近似最近邻搜索来比较所述查询特征向量和与各参考图像相关联地存储的参考特征向量，以找到近似最接近所述查询特征向量的参考特征向量；以及选择步骤，从所述多个参考图像中选择与所找到的参考特征向量相关联地存储的参考图像作为检索结果，其中，通过向各参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成各学习图像，使用尺度空间方法来分别从各参考图像和与该参考图像相对应的各学习图像提取参考特征向量，使用所述尺度空间方法来从所述查询图像提取所述查询特征向量，以及利用计算机执行上述各步骤。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的图像检索方法，其特征在于，所述比较步骤分别使用与顺序从轻到重的处理负荷相对应的顺序从粗略到精细的多个近似程度，重复所述查询特征向量与各参考特征向量的比较，以及所述选择步骤使所述比较步骤重复，以执行比较直到提供了确定检索结果的基础为止。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1或2所述的图像检索方法，其特征在于，还包括标量量化步骤，所述标量量化步骤用于将各参考特征向量的向量维度量化成预定的位数。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求3所述的图像检索方法，其特征在于，所述标量量化步骤将各向量维度量化成2位或更少位数。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求1至4中任一项所述的图像检索方法，其特征在于，通过在水平方向和/或垂直方向上施加所述运动模糊效果来生成各学习图像。</div>
    </div>
    </div> <div class="claim"> <div num="6" class="claim">
      <div class="claim-text">6.	一种图像存储方法，用于将参考图像存储到能够存储所述参考图像的图像数据库中，所述图像数据库用于检索与拍摄有图像检索对象的查询图像相匹配的特定参考图像的图像检索，所述图像存储方法包括如下步骤：通过向要存储的参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成学习图像；分别从所述参考图像和各学习图像提取至少一个参考特征向量；以及将所述参考特征向量和所述学习图像与相应的参考图像相关联地存储在所述图像数据库中，其中，使用尺度空间方法来分别从所述参考图像和各学习图像提取所述参考特征向量，通过以下步骤来执行所述图像检索：以与提取所述参考特征向量的方式相同的方式从所述查询图像提取至少一个查询特征向量；使用近似最近邻搜索来比较所述查询特征向量和从所述参考图像及所述学习图像提取的各参考特征向量，以找到近似最接近所述查询特征向量的特定参考特征向量；以及从所述参考图像中选择与所找到的参考特征向量相关联地存储的参考图像，以及利用计算机执行上述各步骤。</div>
    </div>
    </div> <div class="claim"> <div num="7" class="claim">
      <div class="claim-text">7. 一种用于使计算机执行如下步骤的图像检索程序，所述步骤包括： 从拍摄有图像检索对象的查询图像提取至少一个查询特征向量，所述查询特征向量表示所述查询图像的局部特征；访问预先存储了多个参考图像的图像数据库，各参考图像与从该参考图像生成的学习图像以及表示该参考图像和该学习图像的局部特征的参考特征向量相关联地存储；使用近似最近邻搜索来比较所述查询特征向量和与各参考图像相关联地存储的参考特征向量，以找到近似最接近所述查询特征向量的参考特征向量；以及从所述多个参考图像中选择与所找到的参考特征向量相关联地存储的参考图像作为检索结果，其中，通过向各参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成各学习图像，使用尺度空间方法来分别从各参考图像和与该参考图像相对应的各学习图像提取参考特征向量，以及使用所述尺度空间方法来从所述查询图像提取所述查询特征向量。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES46398648" lang="ZH" load-source="patent-office" class="description">
    <p>图像检索方法、图像检索程序和图像登记方法</p>
    <p>技术领域 </p>
    <p>[0001]	本发明涉及图像检索方法、图像检索程序和图像登记方法，特别地，涉及使用诸如配置有照相机的移动电话等的常见的图像输入装置拍摄的图像作为查询图像的图像检索方法、图像检索程序和图像登记方法。</p>
    <p>背景技术</p>
    <p>[0002]	随着配置有照相机的移动电话的普及，需要一种不仅把照相机用作为简单拍摄图像的设备而且还用作为输入设备的服务。例如，已知通过拍摄杂志、目录等所示的产品而订购产品或者呈现相关信息的服务。为此，需要高速且高精度地识别图像中的物体。</p>
    <p>[0003]	发明人提出了利用大型图像数据库（例如参见专利文献1、非专利文献2和3)高速且高精度地识别图像的方法。在本方法中，针对表示图像的局部特性的局部特征应用 PCA-SIFT (例如参见非专利文献1)，使用近似最近邻搜索来比较查询图像的局部特征和图像数据库中的局部特征，并将最接近查询图像的图像确定为识别结果。注意，通常，局部特征表示成向量，PCA-SIFT就是如此。专利文献1和非专利文献2的方法的特征为级联处理， 其中，根据查询图像对近似程度进行适当调整，以提高处理速度。非专利文献3的方法的特征为通过使用标量量化来减少所使用的存储量。</p>
    <p>[0004]	引文列表</p>
    <p>[0005]	专利文献</p>
    <p>[0006]	专利文献1 ：国际公开W02008/026414</p>
    <p>[0007]	非专利文献</p>
    <p>[0008]非专利文献	1 :Y. Ke and R. Sukthankar, PCA-SIFT :A More Distinctive Representation For Local Image Descriptors,Proc. Of CVPR 2004,vol. 2,pp. 506-516, 2004。</p>
    <p>[0009]	非专利文献2:野口和人，黄&#28716;浩一，岩村雅一，“近似最近傍探索O多段&#38542;化(二 J:易物体Θ高速&#35469;&#35672;，‘‘画像Θ&#35469;&#35672;&#8226;理解* &gt; *。”勺Λ (MIRU2007)&#35542;文集，pp. 111-118， July,2007ο</p>
    <p>[0010]	非专利文献3 ：野口和人，黄&#28716;浩一，岩村雅一，“局所&#35352;述子t二基^ &lt;物体&#35469;&#35672; Θ f^ Θ J ^ 'J削&#28187;O&#23455;&#39443;的&#26908;&#35342;，‘‘画像&#9702;&#35469;&#35672;&#183;理解* &gt;水。”勺K (MIRU2008)&#35542;文集，pp. 251-258，July, 2008ο</p>
    <p>发明内容</p>
    <p>[0011]	然而，如果将上述方法应用于由配置有照相机的移动电话所拍摄的查询图像，存在不能获得足够的识别率的问题。认为这是因为，由于诸如在配置有照相机的移动电话拍摄目录等的特写图像时出现的模糊或失焦等的劣化，从拍摄到的图像提取出的局部特征改变了。这里，模糊或失焦均使得拍摄对象（被摄体）的轮廓不清楚，虽然它们的起因不同。 模糊是由于拍摄瞬间照相机不静止引起的。失焦是由于被摄体在焦点之外或者照相机分辨率不够引起的。</p>
    <p>[0012]	作为改进 上述劣化图像的识别率的方法，有三种可能的方法：（1)修复查询图像的劣化；(2)使用相对于图像的劣化稳健的局部特征，以及（3)通过对原始图像施加劣化来生成学习图像。在修复图像的劣化的方法（1)中，需要在识别时执行修复处理，因而处理时间成为问题。至于使用不随劣化改变的局部特征的方法（2)，根据一些技术文献， PCA-SIFT 对于失焦相对稳健（例如参见 K. Mikolajazyk and C. Schmid,"A Performance Evaluation of Local Descriptors"IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, vol. 27，no. 10，pp. 1615-1630，2005)，其中，在后面所述的针对适用于本发明的方法的局部特征的实验也用到了 PCA-SIFT。因而，进一步改进方法（2)是困难的。也就是说，PCA-SIFT和作为PCA-SIFT的基础的SIFT是用于提取表示图像的局部特性的特征点以及与特征点相对应的局部特征的方法。提取过程包括如下过程：在以某个速率增大高斯滤波器的规模的情况下将高斯滤波器顺序应用于原始图像以生成平滑的图像（尺度空间），然后，根据相邻的尺度图像之间的差分图像检测极值，以确定特征点和尺度。因而，认为在提取局部特征的过程中已考虑了对失焦的处理。此外，由于使用尺度空间，PCA-SIFT和 SIFT对于图像尺度的改变是稳健的。</p>
    <p>[0013]	考虑到上述情况，发明人致力于用于学习根据原始图像生成的劣化图像的方法。 该方法称为生成型学习，并用于如字符识别等领域（例如，参见石田皓之，高&#27211;友和，井手一郎，目加田&#24950;人，村&#28716;洋，“手y H青&#22577;&amp;利用L&#183; t力j，人力型低品&#36074;文字O&#35469;&#35672;法，“画像&#9702;&#35469;&#35672;&#183;理解* &gt; 求”々 A (MIRU2006)&#35542;文集，pp. 180-186，July, 2006)。在利用局部特征的图像识别的领域中，也提出了一种方法，其中，局部区域经过几千种随机转换，学习所获得的局部区域，从而实现高精度（例如，M. Ozuysal, M. Calonder, V. Lepetit, and P. Fua, "Fast Keypoint Recognition using Random Ferns，，，IEEE Transactions on Pattern Analysis and Machine Intelligence.)。然而，在这些文献中，只针对仅包括几千个图像的图像数据库验证了有效性，而没有针对本发明考虑的大型图像数据库验证其有效性。</p>
    <p>[0014]	首先，已知使用局部特征的图像识别对部分遮挡和由几何变换引起的变形是稳健的，而且，在各种局部特征中，通过使用尺度空间所提取出的局部特征对失焦和照明条件变化尤其稳健。然而，在使用局部特征的图像识别中，需要存储和处理大量局部特征。因而， 进行的研究主要旨在减少数据量和处理时间。另一方面，在生成型学习中，由于学习模式是从原始图像生成的，要登记到图像数据库中的数据量增加了。也就是说，使用局部特征的图像识别技术的方向和生成型学习的方向不兼容。然而，发明人大胆地尝试结合这两种方法， 并且发现通过容忍处理时间一定程度上的增加，识别率得到极大改进，这是始料未及的。</p>
    <p>[0015]	此外，发现如果把用于减少处理时间的非专利文献2的方法和用于减少存储量的非专利文献3的方法相结合，则可以实现高识别率，同时抑制了生成型学习的缺点。</p>
    <p>[0016]	有鉴于此，本发明提供了一种图像检索方法，该图像检索方法可以实现高精度的图像检索、即图像识别，特别是在使用由诸如配置有照相机的移动电话等的常见的图像输入设备拍摄的图像作为查询图像的情况下。此外，本发明提供一种用于生成图像检索用的图像数据库的图像登记方法。此外，本发明提供用于通过计算机处理来实现图像检索方法的图像检索程序。[0017]	问题解 决方案</p>
    <p>[0018]	本发明提供一种通过利用生成型学习来解决如模糊和失焦等问题的方法。具体而言，从要登记的图像生成多个具有模糊或失焦的图像，从生成的图像获得局部特征，从而增加了登记的局部特征存在于查询图像的每个局部特征的附近的概率。这时发生的存储量增加和处理时间增加的问题可以通过组合非专利文献2的级联方法和使用标量量化的非专利文献3的方法而解决。</p>
    <p>[0019]	也就是说，本发明提供一种图像检索方法，其包括：从拍摄有图像检索对象的查询图像提取至少一个查询特征向量，所述查询特征向量表示所述查询图像的局部特征；访问预先存储了多个参考图像的图像数据库，各参考图像与从该参考图像生成的学习图像以及表示该参考图像和该学习图像的局部特征的参考特征向量相关联地存储；比较步骤，使用近似最近邻搜索来比较所述查询特征向量和与各参考图像相关联地存储的参考特征向量， 以找到近似最接近所述查询特征向量的参考特征向量；以及选择步骤，从所述多个参考图像中选择与所找到的参考特征向量相关联地存储的参考图像作为检索结果，其中，通过向各参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成各学习图像，使用尺度空间方法来分别从各参考图像和与该参考图像相对应的各学习图像提取参考特征向量，使用所述尺度空间方法来从所述查询图像提取所述查询特征向量，以及利用计算机执行上述各步骤。</p>
    <p>[0020]	另一方面，本发明提供一种用于使计算机执行如下步骤的图像检索程序，所述步骤包括：从拍摄有图像检索对象的查询图像提取至少一个查询特征向量，所述查询特征向量表示所述查询图像的局部特征；访问预先存储了多个参考图像的图像数据库，各参考图像与从该参考图像生成的学习图像以及表示该参考图像和该学习图像的局部特征的参考特征向量相关联地存储；使用近似最近邻搜索来比较所述查询特征向量和与各参考图像相关联地存储的参考特征向量，以找到近似最接近所述查询特征向量的参考特征向量；以及从所述多个参考图像中选择与所找到的参考特征向量相关联地存储的参考图像作为检索结果，其中，通过向各参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成各学习图像，使用尺度空间方法来分别从各参考图像和与该参考图像相对应的各学习图像提取参考特征向量，以及使用所述尺度空间方法来从所述查询图像提取所述查询特征向量。</p>
    <p>[0021]	另一方面，本发明提供一种图像存储方法，用于将参考图像存储到能够存储所述参考图像的图像数据库中，所述图像数据库用于检索与拍摄有图像检索对象的查询图像相匹配的特定参考图像的图像检索，所述图像存储方法包括如下步骤：通过向要存储的参考图像施加在拍摄所述图像检索对象时可能出现的失焦和/或运动模糊效果来生成学习图像；分别从所述参考图像和各学习图像提取至少一个参考特征向量；以及将所述参考特征向量和所述学习图像与相应的参考图像相关联地存储在所述图像数据库中，其中，使用尺度空间方法分别从所述参考图像和各学习图像提取所述参考特征向量，通过以下的处理来执行所述图像检索：以与提取所述参考特征向量的方式相同的方式从所述查询图像提取至少一个查询特征向量；使用近似最近邻搜索来比较所述查询特征向量和从所述参考图像及所述学习图像提取的各参考特征向量，以找到近似最接近所述查询特征向量的特定参考特征向量；以及从所述多个参考图像中选择与所找到的参考特征向量相关联地存储的参考图像，以及利用计算机执行上述各步骤。</p>
    <p>[0022]	发明效果</p>
    <p> [0023]	在本发明的图像检索方法中，通过向每个参考图像施加失焦和/或运动模糊效果来生成学习图像，通过使用尺度空间方法从每个参考图像和对应于参考图像的学习图像提取参考特征向量，而且，通过使用尺度空间方法从查询图像提取查询特征向量。因而，可以实现高精度图像识别，特别是在使用如配置有照相机的移动电话这样的尺寸小重量轻的常见的图像输入设备拍摄的图像作为查询图像的情况下。</p>
    <p>[0024]	根据本发明的图像检索程序具有和上述图像检索方法相同的优点。此外，可以利用根据本发明的图像登记方法生成图像检索方法所使用的图像数据库。</p>
    <p>[0025]	根据本发明的图像检索方法和图像登记方法的步骤利用例如个人计算机或服务器的CPU来执行。除此之外，还可用例如移动终端或移动电话这样的设备的CPU或微型计算机来执行这些步骤。根据本发明的图像检索程序是本发明的一方面，是使这种CPU或微型计算机执行上述处理的程序。</p>
    <p>[0026]	作为从图像提取局部特征的算法，已知有诸如SIFT和PCA-SIFT的一些方法。在下文所述的实施例中使用PCA-SIFT。然而，本发明不限于PCA-SIFT。本发明中，查询图像的特征由从查询图像提取的多个查询特征向量表示。参考图像的特征由从参考图像提取的多个参考特征向量表示。图像检索基于如下处理：将每个查询特征向量与参考特征向量进行比较，并找到近似最接近查询特征向量的参考特征向量。</p>
    <p>[0027]	预先将参考特征向量和每个参考图像相关联地登记在图像数据库中。本发明的特征方面在于，不仅使用直接从每个参考图像提取的参考特征向量，还使用从通过对每个参考图像施加失焦和/或运动模糊效果并应用尺度空间方法所生成的每个学习图像提取的局部特征，来进行比较。</p>
    <p>[0028]	注意，如果简单增加从学习图像提取的参考特征向量，由于用来进行比较的参考特征向量的数量增加，所以图像数据库所需的存储量增加，并且处理时间增加。因而，在优选实施例中，本发明可以与标量量化方法结合，以节约存储量，而且可以与级联方法结合， 以节约处理时间。</p>
    <p>附图说明</p>
    <p>[0029]	图1是示出根据本发明的级联标识单元的结构的图。</p>
    <p>[0030]	图2是示出根据本发明的原始图像和从原始图像生成的学习图像的例子的图。</p>
    <p>[0031]	图3是示出与由移动电话的照相机拍摄的图像（左）的特征点相对应的每个学习集（右）中的特征点的数量以及对应关系作为表示根据本发明的生成型学习的效果的例子的第一图。</p>
    <p>[0032]	图4是示出与由移动电话的照相机拍摄的图像（左）的特征点相对应的每个学习集（右）中的特征点的数量以及对应关系作为表示根据本发明的生成型学习的效果的例子的第二图。</p>
    <p>[0033]	图5是示出与由移动电话的照相机拍摄的图像（左）的特征点相对应的每个学习集（右）中的特征点的数量以及对应关系作为表示根据本发明的生成型学习的效果的例子的第三图。[0034]	图6是示出根据本发明的登记图像的例子的图。</p>
    <p>[0035]	图7是示出在本发明实验中使用学习集C成功识别的查询图像的例子的图。</p>
    <p>[0036]	图8是示出本发明在使用级联标识单元的情况下以及未使用级联标识单元的情况下的实验中获得的登记图像的数量和识别率之间的关系的图。</p>
    <p>[0037]	图9是示出本发明在进行了标量量化的情况下以及未进行标量量化的情况下的实验中获得的登记图像的数量和识别率之间的关系的图。</p>
    <p>具体实施方式</p>
    <p> [0038]	下面描述本发明的优选实施例。</p>
    <p>[0039]	在根据本发明的图像检索方法中，比较步骤可以分别使用与顺序从轻到重的处理负荷相对应的顺序从粗略到精细的多个近似程度，重复查询特征向量和每个参考特征向量的比较；选择步骤可以使比较步骤重复进行比较，直到提供了确定检索结果的基础为止。也就是说，图像检索方法可以与级联方法结合。这样，比较步骤从需要小的处理量的粗略近似程度的级开始进行比较，逐步进行到精细近似程度的级，并在获得确定检索结果的有效基础时的级处结束检索。因而，容易识别的查询图像的比较处理在开始级结束，即，在进行粗略近似的早的级。另一方面，在难以识别的查询图像的比较处理中，与针对容易的查询图像的比较处理相比，该比较处理重复较多的次数之后确定检索结果。因而，在比较重复了与查询图像的识别难度相对应的次数之后结束比较，即，处理时间与难度相对应。</p>
    <p>[0040]	这种情况下，与利用针对任何查询图像均&#21243;设置的近似程度所进行的比较的情况相比，大大减小了各种查询图像的比较的平均处理时间。这是由于，如果均&#21243;设置近似程度以获得等同于级联方法的识别性能的识别性能，需要基于最难识别的查询图像来设置近似程度。结果，容易识别的查询图像的近似程度比所需的更精细，针对查询图像进行的比较耗费长的时间。这里，在使用级联方法进行比较的本发明中，如果随着近似级的推进通过使用在前一级获得的近似结果来进行累积比较，则与均&#21243;设置近似程度的情况相比，即使对于最难以识别的查询图像，处理时间也不会大大增加。这种累积比较是可能的（参考非专利文献2)。因而，可以通过引入生成型学习来解决处理时间增加的问题。</p>
    <p>[0041]	此外，图像检索方法可进一步包括将每个参考特征向量的向量维度量化成预定位数的标量量化步骤。即，图像检索方法可以和标量量化方法结合。这样，可以减少登记图像所需的存储量，同时与不使用标量量化方法的情况相比几乎维持识别率。因而，可以通过引入生成型学习来解决存储量的增加的问题。</p>
    <p>[0042]	此外，标量量化步骤可以将每个向量维度量化成两位或更少位。这是由于下文所述实验证明，即使通过将每个向量维度量化成两位或更少位而大大减少了存储量，识别率也不会大幅度降低。</p>
    <p>[0043]	可通过在水平方向和/或垂直方向施加运动模糊效果来生成学习图像。S卩，如果通过使用仅在水平方向和/或垂直方向施加运动模糊效果的图像来生成学习集，则可以获得最大效果，同时与学习图像还包括在倾斜方向上施加了运动模糊效果的图像的情况相比，使由于引入生成型学习而增加的处理时间和存储量保持为最小。</p>
    <p>[0044]	上述优选模式可以相互结合。</p>
    <p>[0045]	以下参考附图详细描述本发明。注意，以下说明从各方面来讲都是说明性的。不应将以下说明理解为对本发明的限定。</p>
    <p>[0046]	首先，描述作为本发明的前提的使用近似最近邻搜索（Approximate Nearest Neighbor Search)的物体识别方法的基本过程。然后描述通过生成型学习生成学习数据。</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00091.png"> <img id="idf0001" file="CN102422319AD00091.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00091.png" class="patent-full-image" alt="Figure CN102422319AD00091"> </a> </div>
    <p>[0048]	适用于本发明的物体识别方法使用包括利用级联方法的高速识别处理和利用标量量化的存储量减小处理的近似最近邻搜索。将描述通常用于至图像数据库的登记的哈希函数和基于近似最近邻搜索的图像检索。之后，描述登记，然后描述检索。</p>
    <p>[0049]	1. 1哈希函数和特征向量的提取</p>
    <p>[0050]	首先，描述作为近似最近邻搜索的基本元素的哈希函数。哈希表和哈希函数是检索领域所已知的。此处，将集中根据本发明的用于将局部特征登记至哈希表的具体过程来描述哈希函数。注意，局部特征是使用PCA-SIFT提取的。即，使用PCA-SIFT执行提取查询特征向量的过程和提取参考特征向量的过程。使用PCA-SIFT获得的36维特征向量χ是主成分分析的结果。因而，较低维度的特征向量χ的特征值较大。然后，由表达式1表示χ的第1到第d维（d &lt; 36)。</p>
    <p>[0051]	Γ表汰式11</p>
    <p> [0054]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00092.png"> <img id="idf0002" file="CN102422319AD00092.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00092.png" class="patent-full-image" alt="Figure CN102422319AD00092"> </a> </div>
    <p>[0055]	接下来，通过使用表达式2将各维度转换成二进制值，从而生成由表达式3表示的位向量。</p>
    <p>[0056]	Γ表汰式31</p>
    <p>[0057]	U= (U1,&#8212;&#8212;，Ud)</p>
    <p>[0058]	此处，Uj是所有对象图像的向量Xj的平均值。</p>
    <p>[0059]	Γ表汰式41</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00093.png"> <img id="idf0003" file="CN102422319AD00093.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00093.png" class="patent-full-image" alt="Figure CN102422319AD00093"> </a> </div>
    <p>[0061]	然后，通过使用表达式4获得哈希值。此处，mod表示求余数运算，Hsize是哈希表的大小。</p>
    <p>[0062]	ι. 2 m^mma^m^mmmimim^m</p>
    <p>[0063]	利用前述哈希函数将参考特征向量登记到哈希表。在哈希表中，通过标量量化减少了数据量的特征向量与参考图像的ID&#8212;起登记。当登记中出现冲突时，将多个特征向量登记成链表（下文简称为列表）。这时，如果列表过长，则会出现检索中的距离计算的处理量过度增加的问题。因而，本实施例中，为列表长度η设置上限值C。如果满足11&gt;(3，则从哈希表中删除整个列表。如果大量的参考特征向量具有同一哈希值，则参考特征向量对图像识别的贡献不大。因而，认为删除整个列表的影响相对较小。</p>
    <p>[0064]	对要登记到图像数据库的所有特征向量执行上述处理，从而完成将参考图像的数据登记到图像数据库。</p>
    <p>[0065]	ι. 3 mmm^mnmmmmmnm ( WM ) [0066]	在检索(比较）中，利用图ι所示的级联方法实现高速处理。</p>
    <p>[0067]	图1是示出根据本发明的级联方法的结构的图。在图1中，编号为0到b的方块表示用于在各级执行检索处理的标识单元。编号较小的标识单元利用较粗略的近似执行检索。每个标识单元通过近似最近邻搜索将表示查询特征的每个查询特征向量与表示图像数据库11中包括的特征点的参考特征向量进行比较，获得近似最接近每个查询特征向量的参考特征向量，然后获得从中提取了参考特征向量的参考图像的ID。对与各个查询特征向量相对应的参考图像的ID中的每个ID执行投票处理，将与最大数量的查询特征向量相对应的参考图像确定为检索答案。</p>
    <p>[0068]	首先，第一级的标识单元对从查询图像获得的一组查询特征向量执行检索处理， 从而执行查询图像识别。在该级，如果获得下文所述的有效基础，则识别处理结束，并将识别结果确定为答案。另一方面，如果没有获得有效基础，则执行更精细近似的下一级标识单元对该组查询特征向量执行检索处理，从而再次执行查询图像识别。</p>
    <p>[0069]	粗略近似的比较处理的负荷较小，这是因为要比较的数据量小，精细近似的比较处理的负荷较重。在图1所示的级联方法中，近似程度逐渐从粗略近似变成精细近似。精细近似的检索的一部分处理（一些检索对象）与在前一级执行的粗略近似的检索的处理重叠。考虑到上述方面，每个标识单元可以被配置为对于与在前一级执行的处理重叠的处理部分使用在前一级获得的结果，并且新执行检索处理的其他部分。</p>
    <p>[0070]	如果即使在每一级执行了检索直到最后一级（第b+Ι级）也没有获得有效基础， 则可以把获得最多投票的参考图像确定为答案，或者可以拒绝该查询图像。利用上述处理， 在较早的级处结束处理的图像的识别的效率大幅度提高，而且可以根据需要进行较长时间的识别。</p>
    <p>[0071]	识别时会引起错误的图像的特征在于，首先，图像获得很少投票，而且，即使图像获得一定数量的投票，该图像的投票数量大致等于第二位候选的投票数量。鉴于此，可以如下确定基础是否有效。如果同时满足V1 &gt; t和!"V1 &gt; V2，其中，V1是第一位候选的投票数量，V2是第二位候选的投票数量，结束处理以将第一位候选确定为答案。注意，t是投票数量的阈值，r是第一位候选的投票数量和第二位候选投票数量之比的阈值。</p>
    <p>[0072]	1. 4各标识单元的处理（各级的比较和选择）</p>
    <p>[0073]	下面描述各级的标识单元执行的处理。每个标识单元从哈希表检索要确定为从查询图像获得的每个查询特征向量q的最近邻的参考特征向量。然而，由于结果包括近似，所以通过近似最近邻搜索所获得的结果可以为近似最近邻，但是不一定是最近邻。这是由于， 考虑到处理时间而对检索精度做了折衷。然而，通过下文所述的投票处理来补偿经过折衷的检索精度。此处，将获得的一组参考特征向量表示为X。接下来，计算通过对每个查询特征向量q进行标量量化所获得的向量和X中包括的每个向量之间的欧氏（Euclidian)距离，从而获得被确定为最近邻的参考特征向量X*。</p>
    <p>[0074]	然后，向对应于X*的参考图像的ID投票。如果存在多个被确定为最近邻的参考特征向量，则执行如下投票处理：向对应于多个参考特征向量的参考图像的每个ID投票， 然后确定获得最多投票的参考图像。对查询图像的所有查询特征向量执行投票处理，选择最终获得最多投票的参考图像作为答案（检索结果）。</p>
    <p>[0075]	在各标识单元的处理中，通过用于检索与各查询特征向量q相对应的特征向量组 X的步骤来确定近似度。在执行最粗略近似的情况下（第一级），如登记时那样，从每个查询特征向量q获得位向量，通过使用哈希函数获得具有相同哈希值的特征向量。</p>
    <p>[0076]	在执行粗略近似的情况下，考虑到特征向量的每个维度的值可能随拍摄条件而变化，执行检索来处理该变化。具体而言，通过使用每个维度的值的变化范围e作为参数，来以下述方式处理该变化。</p>
    <p>[0077]	Γ表汰式51</p>
    <p>[0078]	q = (q1? . . . , qd)</p>
    <p>[0079]	由表达式5表示q。</p>
    <p>[0080]	Γ表汰式61</p>
    <p>[0081]	Iqj-UjISe</p>
    <p>[0082]	如果维度j满足表达式6，则不仅使用~而且还使用表达式7来执行参考特征向量的检索。</p>
    <p>[0083]	Γ表汰式71</p>
    <p>[0084]	u，j = (Uj+l)mod2 (如果Uj为0则U，」为1，如果Uj为1则U，」为0)</p>
    <p>[0085]	从最高维度开始依次对每个特征向量执行上述处理。随着检索处理进行到级联方法的较后级，即，随着近似度变得更精细，每个查询特征向量以及要执行处理的参考特征向量的维数增加。如果处理到达最后一级（第b级），要处理&#20034;位向量。这里，可以通过使用除了在前一级已执行了处理的位向量之外的位向量来容易地执行检索处理（参见非专利文献2)。因此，可将由于级联方法而引起的处理量增加抑制到最少。</p>
    <p>[0086]	2. #用尺It空丨旬方法的学3图彳象牛成（牛成型学3)</p>
    <p>[0087]	接下来，描述学习图像的生成。对于在拍摄如书或图片这样的平面物体时出现的劣化，存在几个可能原因。本发明假定失焦和模糊是最大的原因，并对此采取一些措施。</p>
    <p>[0088]	应对失焦和模糊的最简单有效的方法是使用生成型学习的方法。在该方法中，通过向原始图像应用各种失焦和模糊来生成学习图像，而且还从学习图像提取识别要使用的特征向量。在每个标识单元的处理中，使用所提取的特征向量，连同从原始图像中提取的特征向量一起，作为学习数据。</p>
    <p>[0089]	本发明中，由于通过对局部特征的最近邻搜索来进行识别，所以学习数据的增加导致所需存储量的增加，而且增加了处理时间。因而，为了尽可能减少学习数据，对模糊设置下述假定。作为拍摄对象的书或图片不会移动，并且在拍摄图像时按下拍摄按钮之际出现模糊。因而，假定仅在垂直方向或水平方向上出现模糊。</p>
    <p>[0090]	本实施例中，从原始图像以及由原始图像生成的一组学习图像（学习集）中提取局部特征。</p>
    <p>[0091]	图2是示出包括原始图像以及通过向原始图像施加失焦或模糊而获得的学习图像的示例的图。通过在改变核（kernel)大小的情况下使用高斯滤波器模糊图像数据库11 中的图像来生成学习图像。图2中框线表示的图像集合是学习集A到D，它们是根据失焦和模糊的程度和类型生成的。将从1、7、13和19[像素]选出的组合用于垂直核大小和水平核大小。垂直核大小和水平核大小不同的图像具有模糊。注意，实验所使用的图像数据库11中的图像的大小基本等于QVGA大小（320X240)。</p>
    <p>[0092]	此处，在PCA-SIFT中，利用失焦程度不同的多个图像生成尺度空间，并获得特征点，从而实现尺度不变。因而，有可能从垂直核大小和水平核大小彼此不同的、具有模糊的 相应图像中提取出与原始图像的特征不同的特征。另一方面，从垂直核大小和水平核大小彼此相同的、具有失焦的相应图像中仅提取出与原始图像的特征相同的特征，因此，认为在学习集中增加这样的图像不会大幅度提高识别率。</p>
    <p>[0093]	然而，与预期不同的是，发现如果在学习集中增加垂直核大小和水平核大小彼此相同的这种失焦图像，识别率会增加，从而验证了本发明的有效性。认为其原因和以下事实有关：利用PCA-SIFT生成尺度空间时，高斯滤波器的尺度离散扩展了。S卩，认为原因在于，如果查询图像的失焦度在离散尺度之间，则可以从相应图像提取不同于原始图像的特征点。为了验证这一点，通过从学习集D中包括的图像提取垂直核大小和水平核大小彼此相同的图像（图2中的对角成分）来生成学习集Ddiag。</p>
    <p>[0094]	图3到图5是示出从根据本发明的学习集A、B、C、D和Ddiag中的每个图像获得的局部特征与通过利用移动电话拍摄原始图像所获得的拍摄图像的局部特征之间的一一对应关系的图。左侧图像（拍摄图像）的特征点（获得了局部特征的部分）的数量是134。 注意，为了便于说明，右侧图像显示为相同图像（学习集A中的图像），但是特征点表示从每个学习集获得局部特征的部分。</p>
    <p>[0095]	具体而言，从每个学习集发现与拍摄图像的每个局部特征最相似的局部特征（到拍摄图像的每个局部特征距离最小的局部特征），如果距离等于或小于阈值，则判断为所发现的局部特征与拍摄图像的局部特征具有对应关系。如图3的（a)所示，在学习集A中，9 个局部特征与拍摄图像的局部特征具有一一对应关系。如图的3(b)所示，在学习集B中， 31个局部特征与拍摄图像的局部特征具有一一对应关系。学习集B中的31个局部特征包括学习集A中的9个局部特征。因而，在学习集B中，与学习集A相比，新获得了 22个局部特征。如图4的（a)所示，在学习集C中，比学习集B中的多了 5个的36个局部特征与拍摄图像的局部特征具有一一对应关系，如图4的（b)所示，在学习集D中，与学习集C中的一样多的36个局部特征与拍摄图像的局部特征具有一一对应关系。</p>
    <p>[0096]	在图3到图5中，点表示特征点，线表示对应关系。发现学习集中的模糊图像的数量越大，获得的对应关系的数量越多。同样，在图5所示的学习集Ddiag中，获得的对应关系的数量比学习集A的多，因而，发现这是有一些作用的。</p>
    <p>[0097]	基于以上，发现通过利用生成型学习，学习集中与使用移动电话拍摄的图像的特征点具有对应关系的特征点的数量增加了，即，找到了离从查询图像提取的每个查询特征向量更近的参考特征向量。这表示查询图像与学习集中的图像类似，即，查询图像具有模糊和失焦。注意，上述“对应关系”是仅利用对应于同一个原始图像的图像所获得的处理结果。 因而，实验中示出了在登记了多个图像的情况下所获得的识别结果。</p>
    <p>[0098]	3.实验</p>
    <p>[0099]	3. 1.实验条件</p>
    <p>[0100]	为了验证本发明的有效性，进行下述实验。在实验中，使用下述图像数据库和查询图像。通过使用 PCA-SIFT (获取自 http://www. cs. emu. edu/yke/pcasift/)提取局部特征。 使用以下表达式表示的哈希表大小。[0101]	r表汰式81</p>
    <p>[0102]	Hsize = 2d</p>
    <p>[0103]	以下说明中的处理时间是识别每个查询图像所需的时间。</p>
    <p>[0104]	注意，处理时间不包括提取特征向量所耗费的时间。使用的CPU为2. 8GHz的AMD 皓龙（注册商标）、64GB内存的计算机。</p>
    <p>[0105]	3. 1. 1图像数据库</p>
    <p>[0106]	下面描述实验所使用的图像。图像数据库包括使用“动物”、“生日”、“食物”、“日本”等标签的从照片共享站点flickr收集到的10万个图像。如图6所示，图像数据库主要包括物品、自然和人物等的照片。注意，缩小了图像大小，使得图像的长边等于或小于320像素。</p>
    <p>[0107]	表1示出从第2节描述的每个学习集获得的特征点的数量。图像越模糊，从图像获得的PCA-SIFT特征点的数量越少。因而，与图像的数量相比，特征点的数量没有增加。</p>
    <p>[0108]	「表 Il</p>
    <p>[0109]	学习集类型</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00131.png"> <img id="idf0004" file="CN102422319AD00131.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00131.png" class="patent-full-image" alt="Figure CN102422319AD00131"> </a> </div>
    <p>[0111]	在实验中，将从1万个图像的图像数据库获得的标量量化的阈值用于任意数量的图像的图像数据库。</p>
    <p>[0112]	3. 1.2杳询图像</p>
    <p>[0113]	生成两类查询图像，S卩，识别用的查询图像和拒绝用的查询图像。识别用的查询图像是通过拍摄从图像数据库随机选取的1000个图像所获得的。具体生成过程如下。首先， 将排列有四个或十六个图像的每组图像打印到一张A4纸上。使用彩色激光打印机打印。拍摄者使用不同的各移动电话拍摄纸张，以包括每个图像的全部。这时，如表2所示，图像大小设置为QVGA (320 X 240)，并且设置微距模式。拒绝用的查询图像是通过打印图像数据库中不包括的1000个图像所获得的，并且以与上述方式类似的方式拍摄所打印的图像。表2 示出每个图像的平均特征点数量。</p>
    <p>[0114]	「表 21</p>
    <p>[0115]	杳询图像类型</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00141.png"> <img id="idf0005" file="CN102422319AD00141.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00141.png" class="patent-full-image" alt="Figure CN102422319AD00141"> </a> </div>
    <p>[0117]	3. 2没有柜绝的实验</p>
    <p>[0118]	3. 2. 1 学习集</p>
    <p>[0119]	首先，调查生成型学习对识别率的影响。登记图像的数量为1万。不进行可能对识别率产生负面影响的级联方法和量化（即，特征向量由每维16位的向量表示）。处理参数设为b = 10,c = 100,d = 28以及e = 400。表3示出结果。八位拍摄者拍摄图像。随着学习集如图2按A、B、C和D的顺序改变，识别率增加。在学习集D中，识别率是93. 3%， 比学习集A(原始图像）的81%增加了 12.3%。在拍摄者之间相比，拍摄者4表示最有效， 其中，识别率从57.0%增加到88.7%，增加了 31.7%。拍摄者3表示第二最有效。如表2 所示，拍摄者3和4在未使用微距模式的情况下拍摄图像，使用的每张A4纸上打印了 16个图像。因而，认为所拍图像的失焦程度比其它查询图像的大，这样，本发明的方法提供了良好效果。在学习集Ddiag中，虽然识别率比学习集D的小，但是比学习集A的增加了 10%。未对学习集Ddiag进行施加模糊的处理。因而，发现甚至仅执行施加失焦的处理也可以获得一定程度的效果。</p>
    <p>[0120]	另一方面，有关处理时间，发现学习集中特征点的数量越多，处理时间越长。例如， 学习集C的特征点的数量是学习集A的约5倍，学习集C的处理时间是学习集A的约2倍。 学习集A、B、C和D实际使用的存储量包括了除了用于特征向量的存储量以外的存储量以及占用的存储量，并且分别为2. 5GB、3. 5GB、4. 3GB和4. 5GB,即，存储量按A、B、C和D的顺序增加。这样，以处理时间和存储量增加为代价，实现了通过增加学习集中图像的数量而改善识别率。</p>
    <p>[0121]	图7示出学习集A中未能识别的图像以及学习集C中成功识别的图像。从图7发现均&#21243;模糊和失焦严重的图像也可识别。</p>
    <p>[0122]	「表 31</p>
    <p>[0123]	每类学习集和每类杳询图像的识别率「％ 1和处理时间「msl</p>
    <p>[0124]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00151.png"> <img id="idf0006" file="CN102422319AD00151.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00151.png" class="patent-full-image" alt="Figure CN102422319AD00151"> </a> </div>
    <p>[0125]	3. 2. 2可扩展件</p>
    <p>[0126]	根据上述实验，发现可以通过增加学习集中的特征点的数量提高识别率。然而，增加要登记至哈希表的特征点可能会对本发明方法的可扩展性产生负面影响。因而，本发明中，通过在将登记图像的数量增加到10万的情况下使用学习集C来调查识别率、处理时间和存储量三个要素。参数设置为b = 400、C = 100、d = 28、t = 4以及r = 0. 4。在使用级联方法和不使用级联方法两种情况下进行了实验。在标量量化中，将每维的位数设为2  和16 (未量化）。</p>
    <p>[0127]	图8示出识别率。在未进行量化的情况下，即使登记图像的数量增加到10万，识别率也基本恒定为92.5%。在将每维量化为2位的情况下，识别率下降约1%。另一方面， 几乎未出现由于级联方法引起的识别率下降。</p>
    <p>[0128]	图9示出处理时间。发现使用级联方法使处理时间减少了约1/10。在未进行量化的情况下，需要额外的处理时间来处理查询图像的特征向量量化。</p>
    <p>[0129]	在登记图像的数量为10万的情况下，如果不进行量化，存储量是22. 6GB，如果将每维量化成2位，存储量是6. 7GB，这大约是22. 6GB的1/3。在登记图像的数量为1万的情况下，如果将每维量化成2位，识别率是92. 4%，处理时间是1. 7ms，存储量是2. 7GB。在第 1节中，在使用学习集A时，即不执行生成型学习时，识别率是81.0%，处理时间是7. 7ms，存储量是2. 5GB。因而，可以说通过使用级联方法和量化的组合，可以利用几乎相同的存储量实现高速和高精度识别。</p>
    <p>[0130]	3. 3柜绝的实验</p>
    <p>[0131]	最后，描述拒绝的实验的结果，其中登记图像的数量为1万并使用学习集C。如下定义结果的评价标准。首先，在存在与查询图像相对应的图像的情况下，使用识别率C1、错误识别率E1和拒绝率R1 (C1, E1和R1满足Ci+Ei+Ri = 1)。在不存在与查询图像相对应的图像的情况下，使用错误识别率E2和拒绝率R2 (E2和R2满足E2+R2 = 1)。在实验中，使用了十重交叉验证（10-fold cross validation)。通过使用表4所示的三类标准从学习样本获得参数，并将参数应用于测试样本。使用从b = 5、10和15，c = 2、5、10和100，d = 20、24和 28，e = 200,400和600，r = 0. 2,0. 4和0. 6，t = 4、8和12中选择的每种参数组合。表4 示出结果。拒绝用的查询图像的处理时间比识别用的查询图像的处理时间长。这是由于处理需要进行到最后一级以拒绝查询图像。如果允许约IOms的查询时间和约10%的拒绝率 R1，则可以认为实现了错误识别率为或更低（相当于几乎不出现错误识别）的识别。</p>
    <p>[0132]	「表 41</p>
    <p>[0133]	在讲行柜绝情况下的结果</p>
    <p>[0134]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00161.png"> <img id="idf0007" file="CN102422319AD00161.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102422319A/CN102422319AD00161.png" class="patent-full-image" alt="Figure CN102422319AD00161"> </a> </div>
    <p>[0135]	4.结论</p>
    <p>[0136]	本发明通过使用生成型学习来解决在使用配置有照相机的移动电话作为图像输入设备的情况下对局部特征使用最近邻搜索的图像识别方法中出现的诸如模糊和失焦等的问题。根据使用1万个图像的实验结果，发现识别率提高了 12.3%。</p>
    <p>[0137]	除上述实施例之外，还可以实现本发明的各种变形。这些变形不应当认为是在本发明的范围以外。本发明包括在权利要求及其等同的范围内以及在上述范围内的所有变形。</p>
    <p>[0138]	工业应用件</p>
    <p>[0139]	根据本发明，在使用配置有照相机的移动电话等作为查询图像的输入设备的情况下，可以抑制由在用输入设备拍摄图像时产生的失焦和模糊引起的识别精度下降，并执行高精度图像识别。</p>
    <p>[0140]	此处引入的生成型学习所带来的一个问题是最近邻搜索所需的存储量和处理时间随着学习数据的增加而增加。该问题在使用大型图像数据库的情况下尤其突出。然而， 发现可以通过使用发明人提出的级联方法和标量量化来有效解决该问题。</p>
    <p>[0141]	附图标记说明</p>
    <p>[0142]	11:图像数据库</p>
    <p>[0143]	A、B、C 和 D:学习集</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101140624A?cl=zh">CN101140624A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年10月18日</td><td class="patent-data-table-td patent-date-value">2008年3月12日</td><td class="patent-data-table-td ">清华大学</td><td class="patent-data-table-td ">图像匹配方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101216841A?cl=zh">CN101216841A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年1月14日</td><td class="patent-data-table-td patent-date-value">2008年7月9日</td><td class="patent-data-table-td ">南京搜拍信息技术有限公司</td><td class="patent-data-table-td ">交互式图像搜索系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008026414A1?cl=zh">WO2008026414A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年8月1日</td><td class="patent-data-table-td patent-date-value">2008年3月6日</td><td class="patent-data-table-td ">Osaka Prefecture University Public Corporation</td><td class="patent-data-table-td ">Image recognition method, image recognition device, and image recognition program</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103020321A?cl=zh">CN103020321A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月11日</td><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">广东图图搜网络科技有限公司</td><td class="patent-data-table-td ">近邻搜索方法与系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103020321B?cl=zh">CN103020321B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月11日</td><td class="patent-data-table-td patent-date-value">2015年8月19日</td><td class="patent-data-table-td ">广东图图搜网络科技有限公司</td><td class="patent-data-table-td ">近邻搜索方法与系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103488701A?cl=zh">CN103488701A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年9月5日</td><td class="patent-data-table-td patent-date-value">2014年1月1日</td><td class="patent-data-table-td ">成都理想境界科技有限公司</td><td class="patent-data-table-td ">图像样本训练方法及图像检索系统</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06T0001000000">G06T1/00</a></span></td></tr><tr><td class="patent-data-table-td "> 合作分类</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6211">G06K9/6211</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=JjRuBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30259">G06F17/30259</a></span></td></tr><tr><td class="patent-data-table-td "> 欧洲专利分类号</td><td class="patent-data-table-td "><span class="nested-value">G06K9/62A1A3</span>, <span class="nested-value">G06F17/30M1S</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年4月18日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年5月30日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年9月21日</td><td class="patent-data-table-td ">REG</td><td class="patent-data-table-td ">Reference to a national code</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Ref country code: </span><span class="nested-value">HK</span></div><div class="nested-key-value"><span class="nested-key">Ref legal event code: </span><span class="nested-value">DE</span></div><div class="nested-key-value"><span class="nested-key">Ref document number: </span><span class="nested-value">1164516</span></div><div class="nested-key-value"><span class="nested-key">Country of ref document: </span><span class="nested-value">HK</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">2014年4月30日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2014年12月5日</td><td class="patent-data-table-td ">REG</td><td class="patent-data-table-td ">Reference to a national code</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Ref country code: </span><span class="nested-value">HK</span></div><div class="nested-key-value"><span class="nested-key">Ref legal event code: </span><span class="nested-value">GR</span></div><div class="nested-key-value"><span class="nested-key">Ref document number: </span><span class="nested-value">1164516</span></div><div class="nested-key-value"><span class="nested-key">Country of ref document: </span><span class="nested-value">HK</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年10月28日</td><td class="patent-data-table-td ">C41</td><td class="patent-data-table-td ">Transfer of patent application or patent right or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/8bece3a1dba5f583c7a9/CN102422319A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95_%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2%E7%A8%8B%E5%BA%8F%E5%92%8C.pdf?id=JjRuBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U1PTP2TeEYP1YEemBf04m1rNgTbyQ"},"sample_url":"https://www.google.com/patents/reader?id=JjRuBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>