<!DOCTYPE html><html><head><title>专利 CN102799635A - 一种用户驱动的图像集合排序方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种用户驱动的图像集合排序方法"><meta name="DC.contributor" content="张怡" scheme="inventor"><meta name="DC.contributor" content="赵天昊" scheme="inventor"><meta name="DC.contributor" content="李文博" scheme="inventor"><meta name="DC.contributor" content="莫志鹏" scheme="inventor"><meta name="DC.contributor" content="天津大学" scheme="assignee"><meta name="DC.date" content="2012-6-27" scheme="dateSubmitted"><meta name="DC.description" content="本发明属于网络图像搜索技术领域，涉及一种根据用户反馈筛选、排序图片的方法，包括以下步骤：统计用户在每张图像的浏览信息，包括浏览时间、下载次数和用户评分；利用以上信息加权后为已浏览图像评分并分类；提取出用户的偏好图像特征集进行训练，将利用不同特征组合的训练出的测试图片集标定结果与人工标定的结果进行比对，选定最适合此类图片集的特征集合，使图片集的展示更加符合用户偏好。本发明具有兼顾图像普适性和用户个人偏好的特点，既能够在图像搜索中提供总体的图像排序，又能为个体用户提供个性化搜索结果，具有良好的学习能力，能够根据反馈信息不断调整返回给用户的结果。"><meta name="DC.date" content="2012-11-28"><meta name="DC.relation" content="CN:101833565:A" scheme="references"><meta name="DC.relation" content="CN:101853299:A" scheme="references"><meta name="DC.relation" content="CN:102024049:A" scheme="references"><meta name="DC.relation" content="US:20030163467:A1" scheme="references"><meta name="citation_patent_publication_number" content="CN:102799635:A"><meta name="citation_patent_application_number" content="CN:201210215796"><link rel="canonical" href="https://www.google.com/patents/CN102799635A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102799635A?cl=zh"/><meta name="title" content="专利 CN102799635A - 一种用户驱动的图像集合排序方法"/><meta name="description" content="本发明属于网络图像搜索技术领域，涉及一种根据用户反馈筛选、排序图片的方法，包括以下步骤：统计用户在每张图像的浏览信息，包括浏览时间、下载次数和用户评分；利用以上信息加权后为已浏览图像评分并分类；提取出用户的偏好图像特征集进行训练，将利用不同特征组合的训练出的测试图片集标定结果与人工标定的结果进行比对，选定最适合此类图片集的特征集合，使图片集的展示更加符合用户偏好。本发明具有兼顾图像普适性和用户个人偏好的特点，既能够在图像搜索中提供总体的图像排序，又能为个体用户提供个性化搜索结果，具有良好的学习能力，能够根据反馈信息不断调整返回给用户的结果。"/><meta property="og:title" content="专利 CN102799635A - 一种用户驱动的图像集合排序方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102799635A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102799635A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=I7SwBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102799635A&amp;usg=AFQjCNFxZWVMPGgpGbeDSKXYmQnyqB-1kQ" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/8aab3cf0be2a9f4b864c/CN102799635A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/8aab3cf0be2a9f4b864c/CN102799635A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102799635A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102799635A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102799635A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102799635B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102799635A?cl=zh" style="display:none"><span itemprop="description">本发明属于网络图像搜索技术领域，涉及一种根据用户反馈筛选、排序图片的方法，包括以下步骤：统计用户在每张图像的浏览信息，包括浏览时间、下载次数和用户评分；利用以上信息加权后为已浏览图像评分并分类；提取出...</span><span itemprop="url">https://www.google.com/patents/CN102799635A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102799635A - 一种用户驱动的图像集合排序方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102799635A - 一种用户驱动的图像集合排序方法" title="专利 CN102799635A - 一种用户驱动的图像集合排序方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102799635 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210215796</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年11月28日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年6月27日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年6月27日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102799635B?hl=zh-CN&amp;cl=zh">CN102799635B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210215796.3, </span><span class="patent-bibdata-value">CN 102799635 A, </span><span class="patent-bibdata-value">CN 102799635A, </span><span class="patent-bibdata-value">CN 201210215796, </span><span class="patent-bibdata-value">CN-A-102799635, </span><span class="patent-bibdata-value">CN102799635 A, </span><span class="patent-bibdata-value">CN102799635A, </span><span class="patent-bibdata-value">CN201210215796, </span><span class="patent-bibdata-value">CN201210215796.3</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%BC%A0%E6%80%A1%22">张怡</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%B5%B5%E5%A4%A9%E6%98%8A%22">赵天昊</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9D%8E%E6%96%87%E5%8D%9A%22">李文博</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%8E%AB%E5%BF%97%E9%B9%8F%22">莫志鹏</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%22">天津大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102799635A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102799635A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102799635A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (4),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=I7SwBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210215796&amp;usg=AFQjCNFGiG1weA2Q7YwBOi7DoAQCqw9BGQ"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=I7SwBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102799635A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNFlfbdMrjjsdWSWxT62Giw1Z1S1iQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT117619489" lang="ZH" load-source="patent-office">一种用户驱动的图像集合排序方法</invention-title>
      </span><br><span class="patent-number">CN 102799635 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA102804073" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明属于网络图像搜索技术领域，涉及一种根据用户反馈筛选、排序图片的方法，包括以下步骤：统计用户在每张图像的浏览信息，包括浏览时间、下载次数和用户评分；利用以上信息加权后为已浏览图像评分并分类；提取出用户的偏好图像特征集进行训练，将利用不同特征组合的训练出的测试图片集标定结果与人工标定的结果进行比对，选定最适合此类图片集的特征集合，使图片集的展示更加符合用户偏好。本发明具有兼顾图像普适性和用户个人偏好的特点，既能够在图像搜索中提供总体的图像排序，又能为个体用户提供个性化搜索结果，具有良好的学习能力，能够根据反馈信息不断调整返回给用户的结果。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(2)</span></span></div><div class="patent-text"><div mxw-id="PCLM46750740" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种用户驱动的图片集合排序方法，包括下列步骤：  (1)根据用户在对某个关键字进行图像检索时检索到的图片集，统计用户的图像浏览信息，包括：每张图片的浏览时间、下载次数以及用户评分；  (2)在得到图像统计信息后，使用以下数学表达式为每张图片打分：    P = 60%* D + 30% * &#8212;+10%* &#8212;*(- If                    15	2  其中，P代表单个图片的打分结果，D代表图片总下载次数，B代表图片总浏览时间，G代表所有用户对图片的平均评价等级，G&gt;2. 5时，调整系数a=l ;G〈2.5时，调整系数a=-l。  (3)根据评分结果的从高到低，将图片集里的图片划分为用户偏好图片子集和不符合用户偏好图片集两个子集，另外将用户未浏览过的同类图像构成的子集作为测试图片集；  (4)分别对符合用户偏好图片集、不符合用户偏好图片集和测试图片集提取图像的三个主要特征：边界特征、纹理特征和颜色特征，建立各个图片子集的特征矩阵，  (5)在获得图片子集的特征矩阵后，首先针对三个单一特征对测试图片集里的图片进行SVM训练，得到判断测试图片集里的图像属于符合用户偏好或不符合用户偏好子集的分类器，进而得到认为符合用户偏好的图片；再将三个单一特征中的任意两个特征合并为一个特征，对测试图片集里图片进行SVM训练，得到认为符合用户偏好的图片；最后将三个单一特征特征矩阵合并为一个总的特征矩阵，对测试图片集里的图片进行SVM训练，得到认为符合用户偏好的图片；  (6)对测试图片集里的每张图片都进行人工标定，确定每张图像实际上是否符合用户偏好；  (7)将经过步骤（5)的不同特征组合的训练出的测试图片集标定结果与步骤（6)人工标定的结果进行比对，选定最适合此类图片集的特征集合；  (8)为每一位有记录的用户建立一个账户，在该账户下记录其搜索偏好，当用户再次搜索同一关键字或相似关键字时，根据已有的用户偏好信息，利用步骤（7)选定的最适合此类图片集的特征集合进行训练，实现分类和重排序，提供给用户与其个人喜好相近的图片。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求I所述的用户驱动的图片集合排序方法，其特征在于，基于Sobel算子进行边界特征提取，采用hsv模型抽取图片的颜色特征，采用gabor滤波器进行纹理特征提取。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES53077793" lang="ZH" load-source="patent-office" class="description">
    <p>一种用户驱动的图像集合排序方法</p>
    <p>所属技术领域</p>
    <p>[0001]	本发明属于网络图片搜索技术领域，特别涉及一种根据用户浏览与反馈数据筛选以及排序图片的方法。</p>
    <p>背景技术</p>
    <p>[0002]	在最近的几十年中，随着个人计算机以及网络的普及，通过网络搜索信息已经变成了人们获取信息的重要来源之一。其中，图片搜索是网络搜索的一个重要组成部分。一直以来，现有的图片搜索往往会给用户筛选出与需求不甚相符的结果。这是由于以下原因造成的：第一，绝大部分为搜索结果排序的算法是以浏览量等基本信息为依据的，无法为用户 提供个性化的搜索结果。由于每个用户的需求存在着显著的差异，这种一般的算法注定会降低用户体验；第二，最近形成的关联反馈系统，虽然能够比较好的反映群体用户对搜索结果的评价，但是并未考虑到每个用户都是一个单独的个体，也就会有差异性的需要。因此，我们需要一种图片筛选及排序方法，以便为用户提供更贴近个人需求的结果。</p>
    <p>发明内容</p>
    <p>[0003]	本发明针对现有图像搜索技术无法为用户提供更精确、更具个性化的搜索服务的问题，提出一种新的图片集合排序方法。本发明通过改进筛选及排序技术，使不同类型的用户都能够得到最大限度适合个人的搜索结果，减少用户重复搜索次数，提高用户搜索效率，提高用户对搜索的满意度。本发明的技术方案如下：</p>
    <p>[0004]	一种用户驱动的图片集合排序方法，包括下列步骤：</p>
    <p>[0005]	(I)根据用户在对某个关键字进行图像检索时检索到的图片集，统计用户的图像浏览信息，包括：每张图片的浏览时间、下载次数以及用户评分；</p>
    <p>[0006]	(2)在得到图像统计信息后，使用以下数学表达式为每张图片打分：</p>
    <p>[0007]	P = 60% D -I- 30% * &#8212;+ 10% * &#8212; -Vf</p>
    <p>          15	2</p>
    <p>[0008]	其中，P代表单个图片的打分结果，D代表图片总下载次数，B代表图片总浏览时间，G代表所有用户对图片的平均评价等级，G&gt;2. 5时，调整系数a =1 ；G&lt;2. 5时，调整系数</p>
    <p>Ct =_10</p>
    <p>[0009]	(3)根据评分结果的从高到低，将图片集里的图片划分为用户偏好图片子集和不符合用户偏好图片集两个子集，另外将用户未浏览过的同类图像构成的子集作为测试图片集;</p>
    <p>[0010]	(4)分别对符合用户偏好图片集、不符合用户偏好图片集和测试图片集提取图像的三个主要特征：边界特征、纹理特征和颜色特征，建立各个图片子集的特征矩阵，</p>
    <p>[0011]	(5)在获得图片子集的特征矩阵后，首先针对三个单一特征对测试图片集里的图片进行SVM训练，得到判断测试图片集里的图像属于符合用户偏好或不符合用户偏好子集的分类器，进而得到认为符合用户偏好的图片；再将三个单一特征中的任意两个特征合并为一个特征，对测试图片集里图片进行SVM训练，得到认为符合用户偏好的图片；最后将三个单一特征特征矩阵合并为一个总的特征矩阵，对测试图片集里的图片进行SVM训练，得到认为符合用户偏好的图片；</p>
    <p>[0012]	(6)对测试图片集里的每张图片都进行人工标定，确定每张图像实际上是否符合用户偏好；</p>
    <p>[0013]	(7)将经过步骤（5)的不同特征组合的训练出的测试图片集标定结果与步骤（6)人工标定的结果进行比对，选定最适合此类图片集的特征集合；</p>
    <p>[0014]	(8)为每一位有记录的用户建立一个账户，在该账户下记录其搜索偏好，当用户再次搜索同一关键字或相似关键字时，根据已有的用户偏好信息，利用步骤（7)选定的最适合此类图片集的特征集合进行训练，实现分类和重排序，提供给用户与其个人喜好相近的图片。 [0015]	所述的用户驱动的图片集合排序方法，可以基于Sobel算子进行边界特征提取，采用hsv模型抽取图片的颜色特征，采用gabor滤波器进行纹理特征提取。</p>
    <p>[0016]	本发明的分类模型的效用和性能共受到三个因素的影响：样本规模，不同特征在不同种图像分类中影响的显著性，噪音。我们共进行了三组实验来探究这三种因素的影响。在实验中，我们使用以下四个统计数据来衡量这三种因素对训练结果影响程度。</p>
    <p>[0017]	(I)命中率：系统筛选出的图像数量占图像总量的比率。</p>
    <p>[0018]	(2)查全率：系统筛选出的真正符合用户偏好的图像数量占实际符合用户偏好的图像数量的比率。</p>
    <p>[0019]	(3)查准率：系统筛选出的真正符合用户偏好的图像数量占系统筛选出的图像数</p>
    <p>量的比率。</p>
    <p>[0020]	(4)用户满意度：在排序后的集合前100张图片中，用户实际满意的图像的比率。。</p>
    <p>[0021]	在研究样本规模对模型性能影响时，我们在有噪声情况下对海滩和苹果两组图片集进行数量分别为30，60，80的测试，得到结果显示80为最优样本规模，此时得到的分类结果最准确。因此，我们认为样本规模的扩大能够使图片集更具一般性，提高分类精度。</p>
    <p>[0022]	在研究不同特征在不同种图像分类中影响的显著性对模型性能影响时，我们对海滩和苹果图片集进行测试，得到的结果显示，苹果图片集分类最有效的特征集合是颜色和边界特征，而海滩图片集分类最有效的特征集合是颜色和纹理特征。这证明了对于不同类型的图片，具有不同最适特征集合。我们考虑到了这一点，能够为用户带来更加精确适合个人偏好的搜索结果。</p>
    <p>[0023]	在研究噪声对模型的影响时，我们选择了容量均为80的海滩和苹果图像样本，并对两者均给予最适特征集合进行分类，得到结果显示分类的准确度在无噪声情况下要略优于有噪声情况，用户满意度也略好。但两者差别十分微小，在我们的可接受范围内。这说明了本发明模型具有良好的健壮性。</p>
    <p>[0024]	综上所述。本发明兼具了很高的有分类效性和良好的抗噪健壮性。</p>
    <p>附图说明</p>
    <p>[0025]	图I:模拟系统说明图；</p>
    <p>[0026]	图2 :有噪音的海滩图片，测试规模80 ；[0027]	图3 :有噪音的苹果图片，测试规模80 ；</p>
    <p>[0028]	图4 :颜色特征提取在有无噪音情况下的对比(海滩样本）；</p>
    <p>[0029]	图5 :边界特征提取在有无噪音情况下的对比(苹果样本）; [0030]	图6 :纹理特征提取在有无噪音情况下的对比(海滩)。</p>
    <p>具体实施方式</p>
    <p>[0031]	本发明最佳实施方案如下：</p>
    <p>[0032]	I.统计用户的图像浏览信息。</p>
    <p>[0033]	由于现有的浏览器不能存储我们需要的用户浏览信息，为此本发明建立了一个提取用户信息的模拟系统，如图I所示。在该系统中，我们导入了使用其他浏览器检索到的图片集，并根据用户行为统计各种浏览信息，包括：每张图片的浏览时间、下载次数以及用户评分。由于用户在浏览搜索结果时，通常会连续浏览结果图片集，因此我们认为单个图片的浏览次数对于图片筛选排序并不具有高度价值。而浏览时间和下载次数能够显著的体现一张图片对于用户的价值：用户对于图片越满意，浏览时间会越长，也具有越大的可能性去下载图片。用户评分是最直接反映用户对图像满意程度的信息。在得到图像统计信息后，我们使用一下数学表达式为图片打分：</p>
    <p>                            O</p>
    <p>[0034]	P = 60% *D + 30% *&#8212; + 10%li： &#8212;*(-!)"</p>
    <p>          15	2</p>
    <p>[0035]	其中，P代表单个图片的打分结果，D代表图片总下载次数，B代表图片总浏览时间，G代表所有用户对图片的平均评价等级，G&gt;2. 5时，调整系数a =1 ；G&lt;2. 5时，调整系数</p>
    <p>Ct =_10</p>
    <p>[0036]	该打分结果P具有两点作用。（I)利用P值从高到低对数据库中所有图像进行排序，得到一个一般性的排序结果。（2)对于某一位特定用户，他(她）显然只可能浏览到全部图像的一部分。利用P值对其已浏览过的图像进行排序，筛选出其中打分较高的部分作为符合其个人偏好的图片集合，打分较低的部分作为不符合个人偏好的图片集。在该用户未来搜索相同或相似关键字时，根据已有的个人偏好图像记录，对其未浏览过的同类图像进行训练，得到认为符合其偏好的图片集并返回给用户。</p>
    <p>[0037]	2.在为用户匹配符合个人喜好图片的过程中，我们提取了图片的边界、纹理以及颜色特征。现有技术中，已经有很多种特征提取的方法，只要是能够有效地提取图片的边界、纹理以及颜色特征，均可以一用。本实施例的特征提取技术方案如下所述。为了便于描述，我们取一张图像作为说明对象，记为I。</p>
    <p>[0038]	( I)边界特征提取</p>
    <p>[0039]	我们基于Sobel算子进行边界特征提取。由于Sobel算子对噪声具有平滑作用，从而能够提供较为精确的边缘方向信息。</p>
    <p>[0040]	首先，将I转为灰度图。I以矩阵的形式读入，记为i。利用Sobel算子对i矩阵进行处理提取出I边界图像的对应矩阵r。在！■中，位于边界上的像素点以白色表示，非边界像素为黑色，r即为0-1矩阵，其中I代表白色，0代表黑色。</p>
    <p>[0041]	根据图像大小，我们将r划分成16行X 16列共256个子矩阵，每一子矩阵大小相同且没有重叠。由于256个子矩阵可能无法完全覆盖i矩阵，我们在划分过程中，对子矩阵的大小向下取整，最后舍弃掉矩阵底部和右侧的多余部分。对于这部分舍弃掉的矩阵，我们认为由于其对应像素位于图像边缘，对于图像的边界特征贡献可以忽略不计。</p>
    <p>[0042]	在每一个子矩阵中，我们统计元素I个数与子矩阵总元素数的比值。这样，对于图片I，我们得到了一个256维的特征向量，该向量的每一维度代表了一个子图片中边界像素占总像素数的比例。</p>
    <p>[0043]	(2)颜色特征提取</p>
    <p>[0044]	相比其他模型而言，hsv模型更适合于抽取图片的颜色特征。我们采用hsv模型，需要提取图片的H (hue色相）、S (saturation饱和度）、V (value色调）三个特征。由于进行测试的图片I均为rgb模型，所以我们首先提取了 I的三原色特征并分别存在三个矩阵中。然后对I的每一个像素点进行hsv模型转换。</p>
    <p>[0045]	转换完成之后我们获得了 I中每一个像素点上的hsv特征值，对h，s，V三个特征的原始矩阵加以整理，使其合并为一个64位(其中，h :32位，V : 16位，s: 16位）向量作为I 的特征向量。</p>
    <p>[0046]	(3)纹理特征提取</p>
    <p>[0047]	我们采用了 gabor滤波器进行纹理特征提取。</p>
    <p>[0048]	首先，我们需要将i分割为若干正方形的子矩阵，以便使用gabor滤波器进行特征提取。考虑到进行测试的图像大小和特征提取的准确性，我们取子矩阵大小为32 X 32，且每个子矩阵都有一半的面积与其相邻子矩阵重合。对于i矩阵下侧和右侧无法被子矩阵覆盖的多余部分，我们采用和边界特征提取相同的处理方法，舍弃掉这一部分矩阵。假设我们最终划分出了 N个子矩阵。</p>
    <p>[0049]	在每个子矩阵中应用gabor滤波器,其中stage = 4, orientation = 6。最终每个子矩阵得到一个48维的特征向量，I图片得到了 N个48维的特征向量。但是不同大小图片N值是不同的，难以进行接下来的图像训练。为了解决该问题，同时考虑到样本图像的大小，我们对每张图片都提取出32个特征向量来进行训练。为了使这32个向量能够最大限度代表图像的纹理特征，我们使用了以下的筛选方法：首先，对I的N个特征向量采用kmeans算法分为8类，并得到每一类的中心向量Q。之后我们按照每类向量数的比例计算每类中取出的向量数量，并在每类中取出相应数量的、距离其中心向量Ci最近的向量。这样，每一张图片都可以得到一个32维的特征矩阵。</p>
    <p>[0050]	需要说明的是，由于标准kmeans算法初始化时每一类的中心向量为随机取得，并且我们无法预测每组的中心向量，这会导致每次分类结果的差异性，从而产生不同的训练结果。因此我们选取了几次训练的平均效果作为实验结果。</p>
    <p>[0051]	3.根据以上所述的获取图像颜色、纹理及边界特征的技术，我们提取出符合用户偏好、不符合用户偏好及测试图片集（即用户未浏览过的待分类图像集）的特征后对其进行训练。记录符合用户偏好的图片集为G，不符合用户偏好图片集为B，测试图片集为S。为了进行SVM训练，G与B集合大小相同。具体技术方案如下。</p>
    <p>[0052]	(I)单一特征训练</p>
    <p>[0053]	以边界特征训练为例，利用上述边界特征提取技术，对G、B、S集合中的图片进行边界特征提取，得到G、B、S各自的特征矩阵并进行SVM训练，得到一个判断待筛选图像属于符合偏好或不符合偏好集合的分类器，利用该分类器在S图片集中筛选出符合用户偏好的图片。颜色、纹理特征训练方法同上。</p>
    <p>[0054]	(2)混合特征训练</p>
    <p>[0055]	我们将每个图像的边界、颜色、纹理三个特征向量合并一个特征向量，以及将其中任意两个向量合并为一个，利用合成向量进行上述的SVM训练，筛选出S中符合用户偏好的图片。</p>
    <p>[0056]	(3)提取最优训练特征组合及验证训练有效性</p>
    <p>[0057]	对测试图片集里的每张图片都进行人工标定，确定每张图像实际上是否符合用户偏好。将通过上面的三种方式训练出的测试图片集标定结果与测试图片集的实际标定结果进行计算机比对，得到命中率、查全率、查准率和用户满意度等性能指标。对于同一组测试图片集的不同特征集合的训练结果中，性能最佳的即为最符合用户要求结果，选定其对应的特征组合为最适合该类图像集的特征组合。 [0058]	4.为每一位有记录的用户建立一个账户，在该账户下记录其搜索偏好，即用户搜索过的图像类别以及对应的图像特征信息，当用户再次搜索同一关键字或相似关键字时，根据已有的用户偏好信息，利用步骤（7)选定的最适合此类图片集的特征集合进行训练，实现分类和重排序，提供给用户与其个人喜好相近的图片。</p>
    <p>[0059]为了验证一般情况下样本规模对分类模型的影响，我们控制其他条件不变并在有噪声的情况下运用三种不同特征分类方法对苹果和海滩两个图片集进行了样本容量为30，60，80的6组实验，并且通过数据处理得到了命中率、查全率、查准率和用户满意度。具</p>
    <p>体实验结果如下。</p>
    <p>[0060]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102799635A/CN102799635AD00071.png"> <img id="idf0001" file="CN102799635AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102799635A/CN102799635AD00071.png" class="patent-full-image" alt="Figure CN102799635AD00071"> </a> </div>
    <p>[0061]</p>
    <p>        [0062][0063]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102799635A/CN102799635AD00081.png"> <img id="idf0002" file="CN102799635AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102799635A/CN102799635AD00081.png" class="patent-full-image" alt="Figure CN102799635AD00081"> </a> </div>
    <p>[0065]	通过上面的实验我们可以看出，当样本容量逐渐增大时，分类的准确性也随之上升，在这三组样本容量中，最佳样本容量为80.</p>
    <p>[0066]	由于不同用户针对不同图片集都会有自己的选择标准，所以分类时，特征的选择也会对最后分类的结果造成影响，设计本组实验正是为了研究针对特定图片集，单个特征和不同特征组合分类性能的差异。实验采用的图片依旧为海滩和苹果，得到的数据结果如图2、图3所示。</p>
    <p>[0067]	由图2、图3结果可以看出在对海滩图像进行测试时，分类结果会受到所选特征的影响，这是由在评价不同图片集图片时用户个人喜好差异所致。因此，我们设计了这样一组实验，目的是研究基于不同特征集合的结果差异。在这个实验中，我们使用了两组图片集。通过上面所列出的特征研究结果我们可以发现，在海滩图片集分类中，颜色和纹理特征是最有效的特征集合；而在苹果图片集分类中，颜色和边界是最好的特征集合。虽然有些特征集合会产生效果较差的结果，但是这三个特征的集合总能够筛选出用户乐于接受的结果。</p>
    <p>[0068]	分类模型在有噪音干扰和无噪音干扰下分类的结果差异大小是衡量分类模型健壮性的一个很重要的指标。本组实验研究了三类特征分类方法在两个不同图片集下有无噪音情况下的分类结果，为了保证实验效果最佳，我们选择了 80容量的样本集进行试验，其中苹果图片集被用来对边界进行测试，而沙滩则用来对纹理及颜色特征进行测验。测试结果如图4&#12316;图6所示。测试结果显示在有无噪音情况下筛选结果差距很小。尽管在计算用 户满意度时只选择了 100张图片，会出现误差，但是实验结果的微小差异仍然可以反映出用户满意度的差别。因为有噪声引起的用户满意度差异在我们的可接受范围内，我们可以认为系统健壮性良好。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101833565A?cl=zh">CN101833565A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年3月31日</td><td class="patent-data-table-td patent-date-value">2010年9月15日</td><td class="patent-data-table-td ">南京大学</td><td class="patent-data-table-td ">一种主动选择代表性图像的相关反馈方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102024049A?cl=zh">CN102024049A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年12月8日</td><td class="patent-data-table-td patent-date-value">2011年4月20日</td><td class="patent-data-table-td ">中国科学院自动化研究所</td><td class="patent-data-table-td ">一种用于电子商务平台上的图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030163467">US20030163467</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2002年2月27日</td><td class="patent-data-table-td patent-date-value">2003年8月28日</td><td class="patent-data-table-td ">Robert Cazier</td><td class="patent-data-table-td ">Metric based reorganization of data</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103235815A?cl=zh">CN103235815A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年4月25日</td><td class="patent-data-table-td patent-date-value">2013年8月7日</td><td class="patent-data-table-td ">北京小米科技有限责任公司</td><td class="patent-data-table-td ">一种应用软件的显示方法和设备</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103324750A?cl=zh">CN103324750A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年7月4日</td><td class="patent-data-table-td patent-date-value">2013年9月25日</td><td class="patent-data-table-td ">莫志鹏</td><td class="patent-data-table-td ">一种基于贝叶斯网络的图片集个性化筛选方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103995857A?cl=zh">CN103995857A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2014年5月14日</td><td class="patent-data-table-td patent-date-value">2014年8月20日</td><td class="patent-data-table-td ">北京奇虎科技有限公司</td><td class="patent-data-table-td ">一种实现图像搜索排序的方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014173168A1?cl=zh">WO2014173168A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年12月31日</td><td class="patent-data-table-td patent-date-value">2014年10月30日</td><td class="patent-data-table-td ">Xiaomi Inc.</td><td class="patent-data-table-td ">一种应用软件的显示方法和设备</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=I7SwBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009620000">G06K9/62</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=I7SwBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年11月28日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年1月23日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年10月28日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/8aab3cf0be2a9f4b864c/CN102799635A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E7%94%A8%E6%88%B7%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%9B%BE%E5%83%8F%E9%9B%86%E5%90%88%E6%8E%92%E5%BA%8F.pdf?id=I7SwBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U2ZJLCTBjp2_gBJwI_JBi_A7FmlwQ"},"sample_url":"https://www.google.com/patents/reader?id=I7SwBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>