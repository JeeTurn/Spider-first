<!DOCTYPE html><html><head><title>专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于用户情绪指示信息显示输入信息的方法与设备"><meta name="DC.contributor" content="邓腾" scheme="inventor"><meta name="DC.contributor" content="百度在线网络技术（北京）有限公司" scheme="assignee"><meta name="DC.date" content="2011-8-12" scheme="dateSubmitted"><meta name="DC.description" content="本发明的目的是提供一种基于用户情绪指示信息显示输入信息的方法与设备，用户设备获取用户通过用户设备输入的输入信息；对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；根据所述信息显示模式，将所述输入信息提供至所述用户设备。与现有技术相比，本发明通过获取用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，并以该信息显示模式显示所述用户所输入的输入信息，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。"><meta name="DC.date" content="2012-1-18"><meta name="DC.relation" content="CN:101370195:A" scheme="references"><meta name="DC.relation" content="CN:101388855:A" scheme="references"><meta name="DC.relation" content="CN:101669090:A" scheme="references"><meta name="DC.relation" content="CN:101901045:A" scheme="references"><meta name="DC.relation" content="CN:1380846:A" scheme="references"><meta name="citation_patent_publication_number" content="CN:102323919:A"><meta name="citation_patent_application_number" content="CN:201110231708"><link rel="canonical" href="https://www.google.com/patents/CN102323919A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102323919A?cl=zh"/><meta name="title" content="专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备"/><meta name="description" content="本发明的目的是提供一种基于用户情绪指示信息显示输入信息的方法与设备，用户设备获取用户通过用户设备输入的输入信息；对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；根据所述信息显示模式，将所述输入信息提供至所述用户设备。与现有技术相比，本发明通过获取用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，并以该信息显示模式显示所述用户所输入的输入信息，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。"/><meta property="og:title" content="专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102323919A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102323919A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=deaEBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102323919A&amp;usg=AFQjCNHActwEgkmBDS0bmvRHH7OhCf9xjg" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/b020329d0fbac8e5b40f/CN102323919A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/b020329d0fbac8e5b40f/CN102323919A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102323919A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102323919A?cl=en&amp;hl=zh-CN"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102323919A?cl=zh" style="display:none"><span itemprop="description">本发明的目的是提供一种基于用户情绪指示信息显示输入信息的方法与设备，用户设备获取用户通过用户设备输入的输入信息；对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；根据所述情绪指示信息，在显示模...</span><span itemprop="url">https://www.google.com/patents/CN102323919A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备" title="专利 CN102323919A - 一种基于用户情绪指示信息显示输入信息的方法与设备"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102323919 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201110231708</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年1月18日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2011年8月12日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2011年8月12日</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201110231708.4, </span><span class="patent-bibdata-value">CN 102323919 A, </span><span class="patent-bibdata-value">CN 102323919A, </span><span class="patent-bibdata-value">CN 201110231708, </span><span class="patent-bibdata-value">CN-A-102323919, </span><span class="patent-bibdata-value">CN102323919 A, </span><span class="patent-bibdata-value">CN102323919A, </span><span class="patent-bibdata-value">CN201110231708, </span><span class="patent-bibdata-value">CN201110231708.4</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%82%93%E8%85%BE%22">邓腾</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E7%99%BE%E5%BA%A6%E5%9C%A8%E7%BA%BF%E7%BD%91%E7%BB%9C%E6%8A%80%E6%9C%AF%EF%BC%88%E5%8C%97%E4%BA%AC%EF%BC%89%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">百度在线网络技术（北京）有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102323919A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102323919A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102323919A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=deaEBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201110231708&amp;usg=AFQjCNEYq-UBD-RnDxMYCoXqH3i44tVEWg"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=deaEBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102323919A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEYoKn6sUWs6GFkXJYkNZDpLJtJWA"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT110137274" lang="ZH" load-source="patent-office">一种基于用户情绪指示信息显示输入信息的方法与设备</invention-title>
      </span><br><span class="patent-number">CN 102323919 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA92366142" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明的目的是提供一种基于用户情绪指示信息显示输入信息的方法与设备，用户设备获取用户通过用户设备输入的输入信息；对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；根据所述信息显示模式，将所述输入信息提供至所述用户设备。与现有技术相比，本发明通过获取用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，并以该信息显示模式显示所述用户所输入的输入信息，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(25)</span></span></div><div class="patent-text"><div mxw-id="PCLM39634311" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种用于在用户设备端实现基于用户的情绪指示信息显示输入信息的方法，其中， 该方法包括以下步骤：a获取用户通过用户设备输入的输入信息； b对所述输入信息进行语义分析，以获得所述用户的情绪指示信息； c根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；d根据所述信息显示模式，将所述输入信息提供至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求1所述的方法，其中，所述步骤c中的匹配查询操作包括以下至少任一项：-精确匹配查询； -同义匹配查询； -近义匹配查询。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求1或2所述的方法，其中，该方法还包括： -获取所述用户的辅助状态信息；χ根据所述辅助状态信息，确定所述用户的辅助情绪指示信息； 其中，所述步骤b包括：-对所述输入信息进行语义分析，以获得所述用户的情绪指示信息； -根据所述用户的辅助情绪指示信息，更新所述情绪指示信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求3所述的方法，其中，所述辅助状态信息包括以下至少任一项： -输入操作信息；-表情状态信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求4所述的方法，其中，所述辅助状态信息包括输入操作信息； 其中，所述步骤χ包括：-根据所述输入操作信息，并结合所述用户的历史输入操作记录，确定所述辅助情绪指 /J^fn 息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.根据权利要求4或5所述的方法，其中，所述输入操作信息包括以下至少任一项： -所述用户的输入速度；-所述用户的输入节奏； -所述用户的敲击力度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.根据权利要求1至6中任一项所述的方法，其中，该方法还包括：-根据所述用户对所述信息显示模式的设置操作，调整所述信息显示模式，以获得调整后的所述信息显示模式； 其中，所述步骤d包括：-根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.根据权利要求1至7中任一项所述的方法，其中，所述信息显示模式包括以下至少任一项：-所述输入信息的颜色； -所述输入信息的字体格式； -与所述输入信息相匹配的图片。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求1至8中任一项所述的方法，其中，所述显示模式库包括在用户设备端的本地显示模式库；其中，所述步骤c包括：-根据所述情绪指示信息，在所述本地显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10.根据权利要求1至9中任一项所述的方法，其中，所述显示模式库包括在网络设备端的网络显示模式库；其中，所述步骤c包括： -将所述情绪指示信息发送至所述网络设备；-接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。</div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11.	一种用于在网络设备端辅助实现基于用户的情绪指示信息显示输入信息的方法， 其中，该方法包括以下步骤：A接收自用户设备发送的用户的情绪指示信息；B根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；C将所述信息显示模式发送至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12.根据权利要求11所述的方法，其中，该方法还包括：-获取大量用户所设置的情绪指示信息与信息显示模式的映射关系； -根据所述映射关系，建立或更新所述网络显示模式库。</div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13.	一种用于基于用户的情绪指示信息显示输入信息的用户设备，其中，该设备包括： 第一获取装置，用于获取用户通过用户设备输入的输入信息；分析装置，用于对所述输入信息进行语义分析，以获得所述用户的情绪指示信息； 第一匹配装置，用于根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；提供装置，用于根据所述信息显示模式，将所述输入信息提供至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14.根据权利要求13所述的用户设备，其中，所述第一匹配装置中的匹配查询操作包括以下至少任一项：-精确匹配查询； -同义匹配查询； -近义匹配查询。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15.根据权利要求13或14所述的用户设备，其中，该设备还包括： 第二获取装置，用于获取所述用户的辅助状态信息；确定装置，用于根据所述辅助状态信息，确定所述用户的辅助情绪指示信息； 其中，所述分析装置用于：-对所述输入信息进行语义分析，以获得所述用户的情绪指示信息； -根据所述用户的辅助情绪指示信息，更新所述情绪指示信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16.根据权利要求15所述的用户设备，其中，所述辅助状态信息包括以下至少任一项： -输入操作信息；-表情状态信息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17.根据权利要求16所述的用户设备，其中，所述辅助状态信息包括输入操作信息； 其中，所述确定装置用于：-根据所述输入操作信息，并结合所述用户的历史输入操作记录，确定所述辅助情绪指 /J^fn 息。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18.根据权利要求16或17所述的用户设备，其中，所述输入操作信息包括以下至少任一项：-所述用户的输入速度； -所述用户的输入节奏； -所述用户的敲击力度。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19.根据权利要求13至18中任一项所述的用户设备，其中，该设备还包括：调整装置，用于根据所述用户对所述信息显示模式的设置操作，调整所述信息显示模式，以获得调整后的所述信息显示模式； 其中，所述提供装置用于：-根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20.根据权利要求13至19中任一项所述的用户设备，其中，所述信息显示模式包括以下至少任一项：-所述输入信息的颜色； -所述输入信息的字体格式； -与所述输入信息相匹配的图片。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21.根据权利要求13至20中任一项所述的用户设备，其中，所述显示模式库包括在用户设备端的本地显示模式库；其中，所述第一匹配装置用于：-根据所述情绪指示信息，在所述本地显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22.根据权利要求13至21中任一项所述的用户设备，其中，所述显示模式库包括在网络设备端的网络显示模式库；其中，所述第一匹配装置用于： -将所述情绪指示信息发送至所述网络设备；-接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。</div>
    </div>
    </div> <div class="claim"> <div num="23" class="claim">
      <div class="claim-text">23.	一种用于辅助实现基于用户的情绪指示信息显示输入信息的网络设备，其中，该设备包括：接收装置，用于接收自用户设备发送的用户的情绪指示信息； 第二匹配装置，用于根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；发送装置，用于将所述信息显示模式发送至所述用户设备。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24.根据权利要求23所述的网络设备，其中，该设备还包括更新装置，用于： -获取大量用户所设置的情绪指示信息与信息显示模式的映射关系；-根据所述映射关系，建立或更新所述网络显示模式库。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. 一种用于实现基于用户的情绪指示信息显示输入信息的系统，包括如权利要求13 至22中任一项所述的用户设备及如权利要求23或M所述的网络设备。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES45148298" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于用户情绪指示信息显示输入信息的方法与设备</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及输入法技术领域，尤其涉及一种基于用户情绪指示信息显示输入信息的技术。</p>
    <p>背景技术</p>
    <p>[0002]	现有技术中，用户可以通过文本编辑模式更改其输入或选中的文本信息的颜色、 字体格式（例如字体、字号、倾斜、加粗、下划线、字体效果等），或者用户也可以通过选择某个图片代替其所输入的文本信息。然而，现有技术只能通过用户进行手动更改，系统无法根据用户的心情进行自适应匹配。因此，如何基于用户的情绪指示信息，以与之相匹配的信息显示模式，显示该用户所输入的输入信息，提升用户的使用体验，成为现今亟需解决的一个问题。</p>
    <p>发明内容</p>
    <p>[0003]	本发明的目的是提供一种基于用户情绪指示信息显示输入信息的方法与设备。</p>
    <p>[0004]	根据本发明的一个方面，提供了一种用于在用户设备端实现基于用户的情绪指示信息显示输入信息的方法，其中，该方法包括以下步骤：</p>
    <p>[0005]	a获取用户通过用户设备输入的输入信息；</p>
    <p>[0006]	b对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；</p>
    <p>[0007]	c根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；</p>
    <p>[0008]	d根据所述信息显示模式，将所述输入信息提供至所述用户设备。</p>
    <p>[0009]	根据本发明的另一方面，还提供了一种用于基于用户的情绪指示信息显示输入信息的用户设备，其中，该设备包括：</p>
    <p>[0010]	第一获取装置，用于获取用户通过用户设备输入的输入信息；</p>
    <p>[0011]	分析装置，用于对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；</p>
    <p>[0012]	第一匹配装置，用于根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；</p>
    <p>[0013]	提供装置，用于根据所述信息显示模式，将所述输入信息提供至所述用户设备。</p>
    <p>[0014]	根据本发明的又一方面，还提供了一种用于在网络设备端辅助实现基于用户的情绪指示信息显示输入信息的方法，其中，该方法包括以下步骤：</p>
    <p>[0015]	A接收自用户设备发送的用户的情绪指示信息；</p>
    <p>[0016]	B根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；</p>
    <p>[0017]	C将所述信息显示模式发送至所述用户设备。</p>
    <p>[0018]	根据本发明的再一方面，还提供了一种用于辅助实现基于用户的情绪指示信息显示输入信息的网络设备，其中，该设备包括：</p>
    <p>[0019]	接收装置，用于接收自用户设备发送的用户的情绪指示信息；</p>
    <p>[0020]	第二匹配装置，用于根据所述情绪指示信息，在网络显示模式库中进行匹配查询， 以获得与所述情绪指示信息相匹配的信息显示模式；</p>
    <p>[0021 ] 发送装置，用于将所述信息显示模式发送至所述用户设备。</p>
    <p>[0022]	根据本发明的再一方面，还提供了一种用于实现基于用户的情绪指示信息显示输入信息的系统，包括如上述的用户设备及如上述的网络设备。</p>
    <p>[0023]	与现有技术相比，本发明通过获取用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，并以该信息显示模式显示所述用户所输入的输入信息，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。</p>
    <p>附图说明</p>
    <p>[0024]	通过阅读参照以下附图所作的对非限制性实施例所作的详细描述，本发明的其它特征、目的和优点将会变得更明显：</p>
    <p>[0025]	图1示出根据本发明一个方面的基于用户情绪指示信息显示输入信息的设备示意图；</p>
    <p>[0026]	图2示出根据本发明一个方面的基于用户情绪指示信息显示输入信息的系统拓扑图；</p>
    <p>[0027]	图3示出根据本发明一个优选实施例的基于用户情绪指示信息显示输入信息的设备示意图；</p>
    <p>[0028]	图4示出根据本发明另一个方面的基于用户情绪指示信息显示输入信息的方法流程图；</p>
    <p>[0029]	图5示出根据本发明一个优选实施例的基于用户情绪指示信息显示输入信息的方法流程图。</p>
    <p>[0030]	附图中相同或相似的附图标记代表相同或相似的部件。 具体实施方式</p>
    <p>[0031]	下面结合附图对本发明作进一步详细描述。</p>
    <p>[0032]	图1示出根据本发明一个方面的基于用户情绪指示信息显示输入信息的设备示意图；其中，用户设备1包括第一获取装置11、分析装置12、第一匹配装置13和提供装置 14。在此，用户设备1包括但不限于任何一种可与用户通过键盘、鼠标、遥控器、触摸板、或手写设备等方式进行人机交互的电子产品，例如计算机、手机、PDA、掌上电脑PPC或IPTV寸。</p>
    <p>[0033]	其中，第一获取装置11用于获取用户通过用户设备1输入的输入信息。具体地， 用户通过与用户设备1的交互方式，包括但不限于在word文档、应用程序对话框或网页对话框等输入焦点，通过键入、复制等方式输入一定的字符串；第一获取装置11，例如通过调用所述用户所使用的输入法程序所提供的应用程序接口（API)，或者通过JSP、PHP、ASP等页面技术，获取该字符串，并作为用户通过用户设备输入的输入信息。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入字符串“今天很开心”，第一获取装置 11通过调用所述用户所使用的输入法程序所提供的应用程序接口（API)或通过页面技术， 例如JSP、PHP、ASP等页面技术，实时地获取该字符串，并将其作为该用户通过该计算机输入的输入信息“今天很开心”。本领域技术人员应能理解上述获取用户输入信息的方式仅为举例，其他现有的或今后可能出现的获取用户输入信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0034]	分析装置12用于对所述输入信息进行语义分析，以获得所述用户的情绪指示信息。具体地，分析装置12根据第一获取装置11所获取的用户输入的输入信息，例如通过词义分析、分词技术等，对所述输入信息进行语义分析，并根据分析结果，通过例如语义库查询等方式，获得所述用户的情绪指示信息。在所述语义库中可以存储有关于输入信息与情绪指示信息的映射关系，分析装置12根据语义分析得到的分析结果，通过在所述语义库中查询匹配等方式，获得所述用户的情绪指示信息。接上例，第一获取装置11获取所述用户输入的输入信息“今天很开心”，分析装置12通过对所述输入信息进行语义分析，例如通过分词技术，分别获得“今天”、“很”、“开心”三个词语，分析装置12根据上述三个词语，分别在语义库中进行匹配查询，最终根据语义库中存储的“开心”与“心情愉快”的映射关系，获取该用户的情绪指示信息为“心情愉快”。本领域技术人员应能理解上述获取用户情绪指示信息的方式仅为举例，其他现有的或今后可能出现的获取用户情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0035]	第一匹配装置13用于根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。具体地，在用户设备端或者网络设备端可以有一个显示模式库，在所述显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，第一匹配装置13根据分析装置12所获得的所述用户的情绪指示信息，通过在本地端的显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式，或者通过与网络设备的交互接收网络设备在网络端的显示模式库中匹配查询得到的所述信息显示模式。在此，本发明通过根据用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。接前例，第一匹配装置13根据分析装置 12所获取的用户的情绪指示信息为“心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息。优选地，第一匹配装置13根据所述情绪指示信息在显示模式库中进行匹配查询的方式包括但不限于精确匹配查询、同义匹配查询或近义匹配查询等方式。在此，精确匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段完全相同，例如通过精确匹配查询，情绪指示信息“心情愉快”所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“心情愉快”字段。在此，同义匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段词义相同，例如通过同义匹配查询，情绪指示信息“心情愉快”所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“愉快”、“愉悦”等与“心情愉快”词义相同的字段。在此，近义匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段词义相近，例如通过近义匹配查询，情绪指示信息“心情愉快”所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“兴高采烈”、“狂喜”等与“心情愉快”词义相近的字段。通过这多种匹配查询方式，本发明进一步地自动为用户提供相适应的信息显示模式，以获得更好的用户使用体验。本领域技术人员应能理解上述获取信息显示模式的方式仅为举例，其他现有的或今后可能出现的获取信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0036]	提供装置14用于根据所述信息显示模式，将所述输入信息提供至所述用户设备 1。具体地，提供装置14根据第一匹配装置13所匹配得到的信息显示模式，通过页面技术， 例如JSP、PHP、ASP等页面技术，或者通过调用word文本编辑模块等方式，将所述用户所输入的输入信息提供至用户设备1。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，第一获取装置11通过调用所述用户所使用的输入法程序所提供的应用程序接口（API)，实时地获取所述用户输入的输入信息“今天很开心”；分析装置12根据第一获取装置11实时获取的输入信息，在语义库中进行匹配查询，获取该用户的情绪指示信息为“心情愉快”;第一匹配装置13根据所述情绪指示信息为“心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息；随后，提供装置14根据所述信息显示模式，将所述输入信息“今天很开心”以红色的字体提供至所述用户设备1。本领域技术人员应能理解上述将输入信息提供至用户设备的方式仅为举例，其他现有的或今后可能出现的将输入信息提供至用户设备的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0037]	优选地，用户设备1的各个装置之间是持续不断工作的。具体地，第一获取装置11 获取用户通过用户设备输入的输入信息；分析装置12对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；第一匹配装置13根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；提供装置14根据所述信息显示模式，将所述输入信息提供至所述用户设备。在此，本领域技术人员应理解“持续”是指用户设备1的各装置分别按照设定的或实时调整的工作模式要求进行输入信息的获取、 情绪指示信息的获取、信息显示模式的获取及据此提供输入信息，直至该用户在较长时间内停止通过用户设备1输入输入信息。</p>
    <p>[0038]	优选地，用户设备1还可以包括第二获取装置（未示出）和确定装置（未示出）。 所述第二获取装置用于获取所述用户的辅助状态信息；所述确定装置用于根据所述辅助状态信息，确定所述用户的辅助情绪指示信息；其中，所述分析装置12还用于对所述输入信息进行语义分析，以获得所述用户的情绪指示信息，并根据所述用户的辅助情绪指示信息， 更新所述情绪指示信息。具体地，用户通过与用户设备1的交互，例如通过敲击键盘的方式输入一定的输入信息；第二获取装置，例如通过调用用户设备1自带的或第三方设备所带的传感器，如压力传感器、速度传感器等，获取所述用户的输入操作信息，例如输入速度、输入节奏、敲击力度等，又或者第二获取装置可以通过调用用户设备1自带的或第三方设备所带的摄像头，获取该用户的表情状态信息；随后，确定装置根据所述辅助状态信息，通过数据库查询、模式识别等方式，或者通过与大量用户的平均输入操作记录、该用户的历史输入操作记录等进行比较，确定所述用户的辅助情绪指示信息，例如，当用户的输入节奏较为轻快，判断该用户的辅助情绪指示信息为愉快，当用户的敲击力度高于平均敲击力度时，判断该用户的辅助情绪指示信息为愤怒。接着，所述分析装置12对用户输入的输入信息进行语义分析，例如通过语义库查询等方式，获得所述用户的情绪指示信息；根据所述用户的辅助情绪指示信息，例如通过分别为语义分析确定的情绪指示信息及所述辅助情绪指示信息分配一定的数值和权重，通过加权计算等方式，更新所述情绪指示信息。例如，假设，当心情赋值大于0时为“心情愉快”、心情赋值小于0时为“心情不愉快”，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，分析装置12根据第一获取装置11所获取的所述用户输入的输入信息“今天很开心”，进行语义分析，获得所述用户的情绪指示信息为“心情愉快”，其心情赋值为2、权重为0. 6 ；而第二获取装置通过调用该计算机自带的摄像头捕捉到该用户的表情，并通过图像特征提取等方式，获取该用户的表情状态信息为“微笑”；随后，确定装置根据该表情状态信息“微笑”，确定所述用户的辅助情绪指示信息为“心情愉快”，其心情赋值为1、权重为0. 4 ；则分析装置12根据上述情绪指示信息与辅助情绪指示信息，有综合心情赋值为2*0. 6+1*0. 4 = 1. 6，大于零，故更新该用户的情绪指示信息为“心情愉快”。接上例，假设用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，分析装置12根据第一获取装置11所获取的所述用户输入的输入信息“今天很开心”，进行语义分析，获得所述用户的情绪指示信息为“心情愉快”，其心情赋值为2、权重为0. 6 ；而第二获取装置通过调用该计算机自带压力传感器捕捉到该用户的输入操作信息，获取该用户的输入操作信息为“重击”；随后，确定装置根据该输入操作信息“重击”，通过数据库查询或模式识别等方式，确定所述用户的辅助情绪指示信息为“生气”，其心情赋值为_4、权重为0. 4 ；则分析装置12根据上述情绪指示信息与辅助情绪指示信息，有综合心情赋值为2*0. 6+(-4)*0. 4 = -0. 4，小于零，故更新该用户的情绪指示信息为“心情不愉快”。本领域技术人员应能理解上述结合用户的辅助状态信息更新情绪指示信息的方式仅为举例，其他现有的或今后可能出现的结合用户的辅助状态信息更新情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过用户的辅助状态信息更新其情绪指示信息，本发明进一步地准确获取用户的情绪指示信息，以支持提供更为适应的信息显示模式，从而进一步改善用户的使用体验。</p>
    <p>[0039]	更优选地，所述辅助状态信息包括但不限于以下至少任一项：</p>
    <p>[0040]-输入操作信息；</p>
    <p>[0041]-表情状态信息；</p>
    <p>[0042]	具体地，所述输入操作信息包括但不限于用户的输入速度、输入节奏、敲击力度等，用户通过与用户设备1的交互方式，例如通过敲击键盘的方式输入一定的输入信息，第二获取装置通过调用用户设备1自带的或第三方设备所带的传感器，例如压力传感器、速度传感器等，获取所述用户的输入操作信息。所述用户的表情状态信息可以通过调用用户设备1自带的或第三方设备所带的摄像头所获取，例如摄像头捕捉用户表情之后，用户设备1对生成的表情图片进行预处理，根据处理结果对图像进行特征提取，根据提取到的特征对用户的表情进行分类判断，以获得所述表情状态信息。本领域技术人员应能理解上述辅助状态信息仅为举例，其他现有的或今后可能出现的辅助状态信息如可适用于本发明， 也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过结合输入操作、表情状态等信息确定用户的辅助状态信息，以更新其情绪指示信息，本发明进一步地准确获取用户的情绪指示信息，以支持提供更为适应的信息显示模式，从而进一步改善用户的使用体验。</p>
    <p>[0043]	优选地，所述辅助状态信息包括输入操作信息；其中，所述确定装置还用于根据所述输入操作信息，并结合所述用户的历史输入操作记录，确定所述辅助情绪指示信息。具体地，当所述辅助状态信息包括输入操作信息时，所述确定装置根据所述输入操作信息，例如用户的输入速度、输入节奏、敲击力度等，并结合所述用户的历史输入操作记录，以确定所述辅助情绪指示信息，例如当用户的输入节奏相较于其历史输入操作记录更为轻快时，判断该用户的辅助情绪指示信息为“心情愉快”，当用户的敲击力度高于其历史敲击力度记录时，判断该用户的辅助情绪指示信息为“心情愤怒”。本领域技术人员应能理解上述确定用户的辅助情绪指示信息的方式仅为举例，其他现有的或今后可能出现的确定用户的辅助情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0044]	优选地，所述输入操作信息包括但不限于以下至少任一项：</p>
    <p>[0045]-所述用户的输入速度；</p>
    <p>[0046]-所述用户的输入节奏；</p>
    <p>[0047]-所述用户的敲击力度。</p>
    <p>[0048]	具体地，所述输入操作信息包括但不限于用户的输入速度、输入节奏、敲击力度等，用户通过与用户设备1的交互方式，例如通过敲击键盘的方式输入一定的输入信息，第二获取装置通过调用用户设备1自带的或第三方设备所带的传感器，例如压力传感器、速度传感器等，获取所述用户的输入操作信息。本领域技术人员应能理解上述输入操作信息仅为举例，其他现有的或今后可能出现的输入操作信息如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0049]	在一个优选的实施例（参照图1)中，用户设备1还可以包括调整装置（未示出）。 所述调整装置用于根据所述用户对所述信息显示模式的设置操作，调整所述信息显示模式，以获得调整后的所述信息显示模式；其中，所述提供装置还用于根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备。具体地，用户可以通过在用户设备端进行预设或者根据所提供的信息显示模式进行更改、选择等操作，设置其所需的信息显示模式； 调整装置例如通过调用所述用户设备1的应用程序接口（API)，获取所述用户对所述信息显示模式的设置操作，并根据所述设置操作，通过对与所述信息显示模式相对应的变量重新赋值或执行写操作等方式，调整所述信息显示模式，以获得调整后的所述信息显示模式； 随后，提供装置14根据所述调整后的信息显示模式，通过页面技术，例如JSP、PHP、ASP等页面技术，或者通过调用word文本编辑模块等方式，将所述输入信息提供至所述用户设备1。 本领域技术人员应能理解上述调整信息显示模式的方式仅为举例，其他现有的或今后可能出现的调整信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过根据用户对信息显示模式的设置操作，调整所述信息显示模式，本发明进一步地提供更为适应的信息显示模式，从而进一步改善用户的使用体验。</p>
    <p>[0050]	优选地，所述信息显示模式包括但不限于以下至少任一项：[0051]-所述输入信息的颜色；</p>
    <p>[0052]-所述输入信息的字体格式；</p>
    <p>[0053]-与所述输入信息相匹配的图片。</p>
    <p>[0054]	具体地，所述信息显示模式包括但不限于所述输入信息的颜色、字体格式，例如字体、字号、倾斜、加粗、下划线、字体效果等，或者与所述输入信息相匹配的图片，例如输入信息“哈哈”对应大笑的图片等。提供装置14根据所述信息显示模式，或者根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备1。本领域技术人员应能理解上述信息显示模式仅为举例，其他现有的或今后可能出现的信息显示模式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0055]	在一个优选实施例中（参照图1)，所述显示模式库包括在用户设备端的本地显示模式库；其中，所述第一匹配装置13还用于根据所述情绪指示信息，在所述本地显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。具体地，当所述显示模式库包括在用户设备端的本地模式库时，在所述本地显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，第一匹配装置13根据分析装置12所获得的所述用户的情绪指示信息，通过在本地端保存的本地显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，分析装置12根据第一获取装置11所获取的所述用户输入的输入信息“今天很开心”，进行语义分析，获得所述用户的情绪指示信息为“心情愉快”；第一匹配装置13根据分析装置12所获取的用户的情绪指示信息为“心情愉快”，在该计算机本地端的本地显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息“今天很开心”。本领域技术人员应能理解上述在本地显示模式库中进行匹配查询获得信息显示模式的方式仅为举例，其他现有的或今后可能出现的在本地显示模式库中进行匹配查询获得信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0056]	图2示出根据本发明一个方面的基于用户情绪指示信息显示输入信息的系统拓扑图；其中包括网络设备2以及多个与之经由网络相连接的用户设备1。其中，网络设备2 可与多个用户设备1经由网络相连接，并接收自用户设备1发送的用户的情绪指示信息；根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；将所述信息显示模式发送至所述用户设备。在此，网络包括但不限于互联网、广域网、城域网、局域网、VPN网络、无线自组织网络（Ad Hoc网络）等。</p>
    <p>[0057]	另外，网络设备2包括但不限于网络主机、单个网络服务器、多个网络服务器集或多个服务器构成的云。在此，云由基于云计算（Cloud Computing)的大量计算机或网络服务器构成，其中，云计算是分布式计算的一种，由一群松散耦合的计算机集组成的一个超级虚拟计算机。而用户设备1包括但不限于任何一种可与用户通过键盘、鼠标、遥控器、触摸板、 或手写设备等方式进行人机交互的电子产品，例如计算机、手机、PDA、掌上电脑PPC或IPTV 等。网络设备2与用户设备1之间的通信相互独立，包括但不限于基于诸如TCP/IP协议、 UDP协议等的分组数据传输。</p>
    <p>[0058]	本领域技术人员应能理解上述网络设备2、用户设备1以及连接其间的网络或通信方式仅为举例，其他现有的或今后可能出现的网络设备、用户设备或网络、通信方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0059]	本领域技术人员还应能理解，图2中仅为简明起见而示出的各类网络元素的数量可能小于一个实际网络中的数量，但这种省略无疑地是以不会影响对本发明进行清楚、充分的公开为前提的。</p>
    <p>[0060]	为简明起见，下面以一个用户设备1为例进行描述。本领域技术人员应能理解，网络设备2可以同时与多个用户设备1交互，并根据接收自从不同用户设备1发送的用户的情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式，将所述信息显示模式发送至与之相对应的用户设备1。</p>
    <p>[0061]	图3示出根据本发明一个优选实施例的基于用户情绪指示信息显示输入信息的设备示意图。其中，用户设备1包括第一获取装置11’、分析装置12’、第一匹配装置13’和提供装置14’ ；网络设备2包括接收装置21’、第二匹配装置22’和发送装置23’。其中，用户设备1中的第一获取装置11’、分析装置12’和提供装置14’分别与图1所示对应装置相同，故此处不再赘述，并通过引用的方式包含于此。</p>
    <p>[0062]	当所述显示模式库包括在网络设备端的网络显示模式库；用户设备1与网络设备 2的各个装置之间互相配合，以完成基于用户情绪指示信息显示输入信息。具体地，用户设备1中的第一匹配装置13’将所述情绪指示信息发送至所述网络设备；网络设备2中的接收装置21’接收自用户设备1发送的用户的情绪指示信息；第二匹配装置22’根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；发送装置23’将所述信息显示模式发送至所述用户设备1 ；用户设备1中的第一匹配装置13’接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。</p>
    <p>[0063]	用户设备1中的第一匹配装置13’用于将所述情绪指示信息发送至所述网络设备。具体地，用户设备1中的第一匹配装置13’根据分析装置12’分析所得的情绪指示信息，实时、应事件触发或定期地将所述用户的情绪指示信息发送至网络设备2，例如通过一次或多次调用网络设备2提供的应用程序接口（API)或其他约定的通信方式，将所述用户的情绪指示信息发送至网络设备2。本领域技术人员应能理解上述发送情绪指示信息的方式仅为举例，其他现有的或今后可能出现的发送情绪指示信息的方式如可适用于本发明， 也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0064]	网络设备2中的接收装置21’用于接收自用户设备1发送的用户的情绪指示信息。 具体地，接收装置21’接收自用户设备1发送的用户的情绪指示信息，例如通过前例中所述的应用程序接口（API)或其他约定的通信方式，接收自用户设备1发送的用户的情绪指示信息。本领域技术人员应能理解上述接收情绪指示信息的方式仅为举例，其他现有的或今后可能出现的接收情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0065]	第二匹配装置22’用于根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。具体地，在网络设备端可以有一个网络显示模式库，在所述网络显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，第二匹配装置22’根据接收装置21’接收到的所述用户的情绪指示信息，通过在所述网络显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式。例如，第二匹配装置22’根据接收装置21’所接收到的用户的情绪指示信息为“心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息。本领域技术人员应能理解上述获取信息显示模式的方式仅为举例，其他现有的或今后可能出现的获取信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0066]	发送装置23’用于将所述信息显示模式发送至所述用户设备。具体地，发送装置 23’实时、应事件触发或定期地将所述信息显示模式发送至用户设备1，例如通过一次或多次调用所述用户设备1提供的应用程序接口（API)或其他约定的通信方式，将所述第二匹配装置22’匹配获得的与所述情绪指示信息相匹配的信息显示模式发送至所述用户设备 1。</p>
    <p>[0067]	用户设备1中的第一匹配装置13’用于接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。具体地，第一匹配装置13’接收自所述网络设备2发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式，例如通过前例所述的中所述的应用程序接口（API) 或其他约定的通信方式，接收所述信息显示模式。</p>
    <p>[0068]	优选地，网络设备2还可以包括更新装置（未示出）。所述更新装置用于获取大量用户所设置的情绪指示信息与信息显示模式的映射关系；根据所述映射关系，建立或更新所述网络显示模式库。具体地，大量用户通过与用户设备1的交互，设置其情绪指示信息与信息显示模式的映射关系，例如大量用户为情绪指示信息“心情忧郁”设置了相对应的以蓝色显示输入信息的信息显示模式，更新装置获取所述大量用户所设置的情绪指示信息与信息显示模式的映射关系；进一步地，根据所述映射关系，通过数据库建立或数据库更新等方式，建立或更新所述网络显示模式库。本领域技术人员应能理解上述更新网络显示模式库的方式仅为举例，其他现有的或今后可能出现的更新网络显示模式库的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0069]	图4示出根据本发明另一个方面的基于用户情绪指示信息显示输入信息的方法流程图。</p>
    <p>[0070]	在此，用户设备1包括但不限于任何一种可与用户通过键盘、鼠标、遥控器、触摸板、或手写设备等方式进行人机交互的电子产品，例如计算机、手机、PDA、掌上电脑PPC或 IPTV 等。</p>
    <p>[0071]	其中，在步骤Sl中，用户设备1获取用户通过用户设备1输入的输入信息。具体地，用户通过与用户设备1的交互方式，包括但不限于在word文档、应用程序对话框或网页对话框等输入焦点，通过键入、复制等方式输入一定的字符串；在步骤Sl中，用户设备1 例如通过调用所述用户所使用的输入法程序所提供的应用程序接口（API)，或者通过JSP、 PHP、ASP等页面技术，获取该字符串，并作为用户通过用户设备输入的输入信息。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入字符串“今天很开心”，在步骤Sl中，用户设备1通过调用所述用户所使用的输入法程序所提供的应用程序接口（API) 或通过页面技术，例如JSP、PHP、ASP等页面技术，实时地获取该字符串，并将其作为该用户通过该计算机输入的输入信息“今天很开心”。本领域技术人员应能理解上述获取用户输入信息的方式仅为举例，其他现有的或今后可能出现的获取用户输入信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0072]	在步骤S2中，用户设备1对所述输入信息进行语义分析，以获得所述用户的情绪指示信息。具体地，在步骤S2中，用户设备1根据在步骤Sl中所获取的用户输入的输入信息，例如通过词义分析、分词技术等，对所述输入信息进行语义分析，并根据分析结果，通过例如语义库查询等方式，获得所述用户的情绪指示信息。在所述语义库中可以存储有关于输入信息与情绪指示信息的映射关系，在步骤S2中，用户设备1根据语义分析得到的分析结果，通过在所述语义库中查询匹配等方式，获得所述用户的情绪指示信息。接上例，在步骤Sl中，用户设备1获取所述用户输入的输入信息“今天很开心”，在步骤S2中，用户设备 1通过对所述输入信息进行语义分析，例如通过分词技术，分别获得“今天”、“很”、“开心”三个词语，用户设备1根据上述三个词语，分别在语义库中进行匹配查询，最终根据语义库中存储的“开心”与“心情愉快”的映射关系，获取该用户的情绪指示信息为“心情愉快”。本领域技术人员应能理解上述获取用户情绪指示信息的方式仅为举例，其他现有的或今后可能出现的获取用户情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0073]	在步骤S3中，用户设备1根据所述情绪指示信息，在显示模式库中进行匹配查询， 以获得与所述情绪指示信息相匹配的信息显示模式。具体地，在用户设备端或者网络设备端可以有一个显示模式库，在所述显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，在步骤S3中，用户设备1根据在步骤S2中所获得的所述用户的情绪指示信息，通过在本地端的显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式，或者通过与网络设备的交互接收网络设备在网络端的显示模式库中匹配查询得到的所述信息显示模式。在此，本发明通过根据用户的情绪指示信息，在显示模式库中自动匹配获得与之相对应的信息显示模式，实现了信息显示模式的自适应提供，从而避免了用户手动设置信息显示模式的不便，也极大改善了用户的使用体验。接前例，在步骤S3中，用户设备 1根据在步骤S2中所获取的用户的情绪指示信息为“心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息。优选地，在步骤S3中，用户设备1根据所述情绪指示信息在显示模式库中进行匹配查询的方式包括但不限于精确匹配查询、同义匹配查询或近义匹配查询等方式。在此，精确匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段完全相同，例如通过精确匹配查询，情绪指示信息“心情愉快”所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“心情愉快”字段。在此，同义匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段词义相同，例如通过同义匹配查询，情绪指示信息“心情愉快” 所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“愉快”、“愉悦”等与 “心情愉快”词义相同的字段。在此，近义匹配查询是指所述情绪指示信息与其在显示模式库中匹配查询获得的信息显示模式所对应的相关字段词义相近，例如通过近义匹配查询， 情绪指示信息“心情愉快”所对应的信息显示模式在该显示模式库中的对应数据库记录，应包含“兴高采烈”、“狂喜”等与“心情愉快”词义相近的字段。通过这多种匹配查询方式，本发明进一步地自动为用户提供相适应的信息显示模式，以获得更好的用户使用体验。本领域技术人员应能理解上述获取信息显示模式的方式仅为举例，其他现有的或今后可能出现的获取信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0074]	在步骤S4中，用户设备1根据所述信息显示模式，将所述输入信息提供至所述用户设备1。具体地，在步骤S4中，用户设备1根据在步骤S3中所匹配得到的信息显示模式， 通过页面技术，例如JSP、PHP、ASP等页面技术，或者通过调用word文本编辑模块等方式，将所述用户所输入的输入信息提供至用户设备1。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，在步骤Sl中，用户设备1通过调用所述用户所使用的输入法程序所提供的应用程序接口（API)，实时地获取所述用户输入的输入信息“今天很开心”；在步骤S2中，用户设备1根据在步骤Sl中实时获取的输入信息， 在语义库中进行匹配查询，获取该用户的情绪指示信息为“心情愉快”;在步骤S3中，用户设备1根据所述情绪指示信息为“心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息；随后，在步骤S4中，用户设备1根据所述信息显示模式，将所述输入信息“今天很开心”以红色的字体提供至所述用户设备1。本领域技术人员应能理解上述将输入信息提供至用户设备的方式仅为举例，其他现有的或今后可能出现的将输入信息提供至用户设备的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0075]	优选地，上述各个步骤之间是持续不断工作的。具体地，在步骤Sl中，用户设备1 获取用户通过用户设备输入的输入信息；在步骤S2中，用户设备1对所述输入信息进行语义分析，以获得所述用户的情绪指示信息；在步骤S3中，用户设备1根据所述情绪指示信息，在显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；在步骤S4中，用户设备1根据所述信息显示模式，将所述输入信息提供至所述用户设备。在此，本领域技术人员应理解“持续”是指上述各步骤分别按照设定的或实时调整的工作模式要求进行输入信息的获取、情绪指示信息的获取、信息显示模式的获取及据此提供输入信息，直至该用户在较长时间内停止通过用户设备1输入输入信息。</p>
    <p>[0076]	优选地，基于用户情绪指示信息显示输入信息的方法还可以包括步骤S6(未示出）和步骤S7(未示出）。在步骤S6中，用户设备1获取所述用户的辅助状态信息；在步骤S7中，用户设备1根据所述辅助状态信息，确定所述用户的辅助情绪指示信息；其中，在步骤S2中，用户设备1还可以对所述输入信息进行语义分析，以获得所述用户的情绪指示信息，并根据所述用户的辅助情绪指示信息，更新所述情绪指示信息。具体地，用户通过与用户设备1的交互，例如通过敲击键盘的方式输入一定的输入信息；在步骤S6中，用户设备1例如通过调用用户设备1自带的或第三方设备所带的传感器，如压力传感器、速度传感器等，获取所述用户的输入操作信息，例如输入速度、输入节奏、敲击力度等，又或者在步骤 S6中，用户设备1可以通过调用用户设备1自带的或第三方设备所带的摄像头，获取该用户的表情状态信息；随后，在步骤S7中，用户设备1根据所述辅助状态信息，通过数据库查询、模式识别等方式，或者通过与大量用户的平均输入操作记录、该用户的历史输入操作记录等进行比较，确定所述用户的辅助情绪指示信息，例如，当用户的输入节奏较为轻快，判断该用户的辅助情绪指示信息为愉快，当用户的敲击力度高于平均敲击力度时，判断该用户的辅助情绪指示信息为愤怒。接着，在步骤S2中，用户设备1对用户输入的输入信息进行语义分析，例如通过语义库查询等方式，获得所述用户的情绪指示信息；根据所述用户的辅助情绪指示信息，例如通过分别为语义分析确定的情绪指示信息及所述辅助情绪指示信息分配一定的数值和权重，通过加权计算等方式，更新所述情绪指示信息。例如，假设，当心情赋值大于0时为“心情愉快”、心情赋值小于0时为“心情不愉快”，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，在步骤S2中，用户设备1根据在步骤Sl中所获取的所述用户输入的输入信息“今天很开心”，进行语义分析， 获得所述用户的情绪指示信息为“心情愉快”，其心情赋值为2、权重为0. 6 ；而在步骤S6中， 用户设备1通过调用该计算机自带的摄像头捕捉到该用户的表情，并通过图像特征提取等方式，获取该用户的表情状态信息为“微笑”；随后，在步骤S7中，用户设备1根据该表情状态信息“微笑”，确定所述用户的辅助情绪指示信息为“心情愉快”，其心情赋值为1、权重为 0. 4 ；则在步骤S2中，用户设备1根据上述情绪指示信息与辅助情绪指示信息，有综合心情赋值为2*0. 6+1*0. 4 = 1. 6，大于零，故更新该用户的情绪指示信息为“心情愉快”。接上例， 假设用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，在步骤S2中，用户设备1根据在步骤Sl中所获取的所述用户输入的输入信息“今天很开心”，进行语义分析，获得所述用户的情绪指示信息为“心情愉快”，其心情赋值为2、权重为0. 6 ；而在步骤S6中，用户设备1通过调用该计算机自带压力传感器捕捉到该用户的输入操作信息，获取该用户的输入操作信息为“重击”；随后，在步骤S7中，用户设备1根据该输入操作信息“重击”，通过数据库查询或模式识别等方式，确定所述用户的辅助情绪指示信息为“生气”，其心情赋值为_4、权重为0. 4 ；则在步骤S2中，用户设备1根据上述情绪指示信息与辅助情绪指示信息，有综合心情赋值为2*0. 6+(-4)^0.4 = -0.4，小于零，故更新该用户的情绪指示信息为“心情不愉快”。本领域技术人员应能理解上述结合用户的辅助状态信息更新情绪指示信息的方式仅为举例，其他现有的或今后可能出现的结合用户的辅助状态信息更新情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过用户的辅助状态信息更新其情绪指示信息，本发明进一步地准确获取用户的情绪指示信息，以支持提供更为适应的信息显示模式，从而进一步改善用户的使用体验。</p>
    <p>[0077]	更优选地，所述辅助状态信息包括但不限于以下至少任一项：</p>
    <p>[0078]-输入操作信息；</p>
    <p>[0079]-表情状态信息；</p>
    <p>[0080]	具体地，所述输入操作信息包括但不限于用户的输入速度、输入节奏、敲击力度等，用户通过与用户设备1的交互方式，例如通过敲击键盘的方式输入一定的输入信息，在步骤S6中，用户设备1通过调用用户设备1自带的或第三方设备所带的传感器，例如压力传感器、速度传感器等，获取所述用户的输入操作信息。所述用户的表情状态信息可以通过调用用户设备1自带的或第三方设备所带的摄像头所获取，例如摄像头捕捉用户表情之后，用户设备1对生成的表情图片进行预处理，根据处理结果对图像进行特征提取，根据提取到的特征对用户的表情进行分类判断，以获得所述表情状态信息。本领域技术人员应能理解上述辅助状态信息仅为举例，其他现有的或今后可能出现的辅助状态信息如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过结合输入操作、表情状态等信息确定用户的辅助状态信息，以更新其情绪指示信息，本发明进一步地准确获取用户的情绪指示信息，以支持提供更为适应的信息显示模式，从而进一步改善用户的使用体验。</p>
    <p>[0081]	优选地，所述辅助状态信息包括输入操作信息；其中，在步骤S7中，用户设备1还可以根据所述输入操作信息，并结合所述用户的历史输入操作记录，确定所述辅助情绪指示信息。具体地，当所述辅助状态信息包括输入操作信息时，在步骤S7中，用户设备1根据所述输入操作信息，例如用户的输入速度、输入节奏、敲击力度等，并结合所述用户的历史输入操作记录，以确定所述辅助情绪指示信息，例如当用户的输入节奏相较于其历史输入操作记录更为轻快时，判断该用户的辅助情绪指示信息为“心情愉快”，当用户的敲击力度高于其历史敲击力度记录时，判断该用户的辅助情绪指示信息为“心情愤怒”。本领域技术人员应能理解上述确定用户的辅助情绪指示信息的方式仅为举例，其他现有的或今后可能出现的确定用户的辅助情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0082]	优选地，所述输入操作信息包括但不限于以下至少任一项：</p>
    <p>[0083]-所述用户的输入速度；</p>
    <p>[0084]-所述用户的输入节奏；</p>
    <p>[0085]-所述用户的敲击力度。</p>
    <p>[0086]	具体地，所述输入操作信息包括但不限于用户的输入速度、输入节奏、敲击力度等，用户通过与用户设备1的交互方式，例如通过敲击键盘的方式输入一定的输入信息，在步骤S6中，用户设备1通过调用用户设备1自带的或第三方设备所带的传感器，例如压力传感器、速度传感器等，获取所述用户的输入操作信息。本领域技术人员应能理解上述输入操作信息仅为举例，其他现有的或今后可能出现的输入操作信息如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0087]	在一个优选的实施例（参照图4)中，在步骤S8(未示出）中，用户设备1还可以根据所述用户对所述信息显示模式的设置操作，调整所述信息显示模式，以获得调整后的所述信息显示模式；其中，在步骤S4中，用户设备1根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备。具体地，用户可以通过在用户设备端进行预设或者根据所提供的信息显示模式进行更改、选择等操作，设置其所需的信息显示模式；在步骤S8中， 用户设备1例如通过调用所述用户设备1的应用程序接口（API)，获取所述用户对所述信息显示模式的设置操作，并根据所述设置操作，通过对与所述信息显示模式相对应的变量重新赋值或执行写操作等方式，调整所述信息显示模式，以获得调整后的所述信息显示模式； 随后，在步骤S4中，用户设备1根据所述调整后的信息显示模式，通过页面技术，例如JSP、 PHP、ASP等页面技术，或者通过调用word文本编辑模块等方式，将所述输入信息提供至所述用户设备1。本领域技术人员应能理解上述调整信息显示模式的方式仅为举例，其他现有的或今后可能出现的调整信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。在此，通过据用户对信息显示模式的设置操作，调整所述信息显示模式，本发明进一步地提供更为适应的信息显示模式，从而进一步改善用户的使用体验。[0088]	优选地，所述信息显示模式包括但不限于以下至少任一项：</p>
    <p>[0089]-所述输入信息的颜色；</p>
    <p>[0090]-所述输入信息的字体格式；</p>
    <p>[0091]-与所述输入信息相匹配的图片。</p>
    <p>[0092]	具体地，所述信息显示模式包括但不限于所述输入信息的颜色、字体格式，例如字体、字号、倾斜、加粗、下划线、字体效果等，或者与所述输入信息相匹配的图片，例如输入信息“哈哈”对应大笑的图片等。在步骤S4中，用户设备1根据所述信息显示模式，或者根据所述调整后的信息显示模式，将所述输入信息提供至所述用户设备1。本领域技术人员应能理解上述信息显示模式仅为举例，其他现有的或今后可能出现的信息显示模式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0093]	在一个优选实施例中（参照图4)，所述显示模式库包括在用户设备端的本地显示模式库；其中，在步骤S3中，用户设备1根据所述情绪指示信息，在所述本地显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。具体地，当所述显示模式库包括在用户设备端的本地模式库时，在所述本地显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，在步骤S3中，用户设备1根据在步骤S2中所获得的所述用户的情绪指示信息，通过在本地端保存的本地显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式。例如，用户在计算机上的即时通讯软件对话框内通过某一输入法软件输入输入信息“今天很开心”，在步骤S2中，用户设备1根据在步骤Sl中所获取的所述用户输入的输入信息“今天很开心”，进行语义分析，获得所述用户的情绪指示信息为“心情愉快”；在步骤S3中，用户设备1根据在步骤S2中所获取的用户的情绪指示信息为“心情愉快”，在该计算机本地端的本地显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息“今天很开心”。 本领域技术人员应能理解上述在本地显示模式库中进行匹配查询获得信息显示模式的方式仅为举例，其他现有的或今后可能出现的在本地显示模式库中进行匹配查询获得信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0094]	图5示出根据本发明一个优选实施例的基于用户情绪指示信息显示输入信息的方法流程图，其中，步骤S1’、S2 ’及S4 ’分别与图4所示对应步骤相同，故此处不再赘述，并通过引用的方式包含于此。</p>
    <p>[0095]	当所述显示模式库包括在网络设备端的网络显示模式库；用户设备1与网络设备 2之间互相配合，以完成基于用户情绪指示信息显示输入信息。具体地，在步骤S31’中，用户设备1将所述情绪指示信息发送至所述网络设备2 ；网络设备2接收自用户设备1发送的用户的情绪指示信息；在步骤S5’中，网络设备2根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式；在步骤S32’中， 网络设备2将所述信息显示模式发送至所述用户设备1 ；用户设备1接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。</p>
    <p>[0096]	在步骤S31’中，用户设备1将所述情绪指示信息发送至所述网络设备。具体地，在步骤S31’中，用户设备1根据在步骤S2’中分析所得的情绪指示信息，实时、应事件触发或定期地将所述用户的情绪指示信息发送至网络设备2，例如通过一次或多次调用网络设备2提供的应用程序接口（API)或其他约定的通信方式，将所述用户的情绪指示信息发送至网络设备2。本领域技术人员应能理解上述发送情绪指示信息的方式仅为举例，其他现有的或今后可能出现的发送情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0097]	网络设备2接收自用户设备1发送的用户的情绪指示信息。具体地，网络设备2接收自用户设备1发送的用户的情绪指示信息，例如通过前例中所述的应用程序接口（API) 或其他约定的通信方式，接收自用户设备1发送的用户的情绪指示信息。本领域技术人员应能理解上述接收情绪指示信息的方式仅为举例，其他现有的或今后可能出现的接收情绪指示信息的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0098]	在步骤S5’中，网络设备2根据所述情绪指示信息，在网络显示模式库中进行匹配查询，以获得与所述情绪指示信息相匹配的信息显示模式。具体地，在网络设备端可以有一个网络显示模式库，在所述网络显示模式库中存储有情绪指示信息与信息显示模式的映射关系，例如“忧郁”对应的信息显示模式为蓝色，“愤怒”对应的信息显示模式为字体加粗显示等，在步骤S5’中，网络设备2根据在步骤S31’中接收到的所述用户的情绪指示信息，通过在所述网络显示模式库中进行匹配查询，获得与所述情绪指示信息相匹配的信息显示模式。例如，在步骤S5’中，网络设备2根据在步骤S31’中所接收到的用户的情绪指示信息为 “心情愉快”，在该计算机本地端的显示模式库中进行匹配查询，获得与该情绪指示信息“心情愉快”相匹配的信息显示模式为以红色字体显示用户的输入信息。本领域技术人员应能理解上述获取信息显示模式的方式仅为举例，其他现有的或今后可能出现的获取信息显示模式的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。</p>
    <p>[0099]	在步骤S32’中，网络设备2将所述信息显示模式发送至所述用户设备1。具体地， 在步骤S32’中，网络设备2实时、应事件触发或定期地将所述信息显示模式发送至用户设备1，例如通过一次或多次调用所述用户设备1提供的应用程序接口（API)或其他约定的通信方式，将其在步骤S5’中匹配获得的与所述情绪指示信息相匹配的信息显示模式发送至所述用户设备1。</p>
    <p>[0100]	用户设备1接收自所述网络设备发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式。具体地，用户设备1接收自所述网络设备 2发送的基于所述情绪指示信息在所述网络显示模式库中匹配查询所获得的所述信息显示模式，例如通过前例所述的中所述的应用程序接口（API)或其他约定的通信方式，接收所述信息显示模式。</p>
    <p>[0101]	优选地，在步骤S9’（未示出）中，网络设备2还可以获取大量用户所设置的情绪指示信息与信息显示模式的映射关系；根据所述映射关系，建立或更新所述网络显示模式库。具体地，大量用户通过与用户设备1的交互，设置其情绪指示信息与信息显示模式的映射关系，例如大量用户为情绪指示信息“心情忧郁”设置了相对应的以蓝色显示输入信息的信息显示模式，在步骤S9’中，网络设备2获取所述大量用户所设置的情绪指示信息与信息显示模式的映射关系；进一步地，根据所述映射关系，通过数据库建立或数据库更新等方式，建立或更新所述网络显示模式库。本领域技术人员应能理解上述更新网络显示模式库的方式仅为举例，其他现有的或今后可能出现的更新网络显示模式库的方式如可适用于本发明，也应包含在本发明保护范围以内，并在此以引用方式包含于此。 [0102] 对于本领域技术人员而言，显然本发明不限于上述示范性实施例的细节，而且在不背离本发明的精神或基本特征的情况下，能够以其他的具体形式实现本发明。因此，无论从哪一点来看，均应将实施例看作是示范性的，而且是非限制性的，本发明的范围由所附权利要求而不是上述说明限定，因此旨在将落在权利要求的等同要件的含义和范围内的所有变化涵括在本发明内。不应将权利要求中的任何附图标记视为限制所涉及的权利要求。此外，显然“包括” 一词不排除其他单元或步骤，单数不排除复数。装置权利要求中陈述的多个单元或装置也可以由一个单元或装置通过软件或者硬件来实现。第一，第二等词语用来表示名称，而并不表示任何特定的顺序。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1380846A?cl=zh">CN1380846A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2001年4月2日</td><td class="patent-data-table-td patent-date-value">2002年11月20日</td><td class="patent-data-table-td ">索尼公司</td><td class="patent-data-table-td ">机器人设备、控制机器人设备动作的方法、以及外力检测设备和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101370195A?cl=zh">CN101370195A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年8月16日</td><td class="patent-data-table-td patent-date-value">2009年2月18日</td><td class="patent-data-table-td ">英华达(上海)电子有限公司</td><td class="patent-data-table-td ">移动终端中实现情绪调节的方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101388855A?cl=zh">CN101388855A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2007年9月14日</td><td class="patent-data-table-td patent-date-value">2009年3月18日</td><td class="patent-data-table-td ">英业达股份有限公司</td><td class="patent-data-table-td ">具有个人化物件的网络即时通讯系统及其方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101669090A?cl=zh">CN101669090A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年4月28日</td><td class="patent-data-table-td patent-date-value">2010年3月10日</td><td class="patent-data-table-td ">福特全球技术公司</td><td class="patent-data-table-td ">情绪提示系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101901045A?cl=zh">CN101901045A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2009年5月31日</td><td class="patent-data-table-td patent-date-value">2010年12月1日</td><td class="patent-data-table-td ">张苏渝</td><td class="patent-data-table-td ">一种输入方法和输入设备</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102946491A?cl=zh">CN102946491A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月23日</td><td class="patent-data-table-td patent-date-value">2013年2月27日</td><td class="patent-data-table-td ">广东欧珀移动通信有限公司</td><td class="patent-data-table-td ">一种根据用户心情自动调整壁纸的方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102946491B?cl=zh">CN102946491B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月23日</td><td class="patent-data-table-td patent-date-value">2014年11月5日</td><td class="patent-data-table-td ">广东欧珀移动通信有限公司</td><td class="patent-data-table-td ">一种根据用户心情自动调整壁纸的方法及系统</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=deaEBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017210000">G06F17/21</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=deaEBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0003010000">G06F3/01</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年1月18日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年3月14日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年11月4日</td><td class="patent-data-table-td ">C12</td><td class="patent-data-table-td ">Rejection of a patent application after its publication</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/b020329d0fbac8e5b40f/CN102323919A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%83%85%E7%BB%AA%E6%8C%87%E7%A4%BA%E4%BF%A1%E6%81%AF%E6%98%BE.pdf?id=deaEBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U0v6sqGQrS2ZIyMU3QlN3c4YYVRzg"},"sample_url":"https://www.google.com/patents/reader?id=deaEBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>