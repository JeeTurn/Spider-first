<!DOCTYPE html><html><head><title>专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种基于感性认知的图像检索结果排序方法"><meta name="DC.contributor" content="徐舒畅" scheme="inventor"><meta name="DC.contributor" content="林建聪" scheme="inventor"><meta name="DC.contributor" content="王海洋" scheme="inventor"><meta name="DC.contributor" content="郑聃" scheme="inventor"><meta name="DC.contributor" content="黄琦" scheme="inventor"><meta name="DC.contributor" content="杭州淘淘搜科技有限公司" scheme="assignee"><meta name="DC.date" content="2010-5-31" scheme="dateSubmitted"><meta name="DC.description" content="本发明公开了一种基于感性认知的图像检索结果排序方法，该方法主要基于图像的客观特征和主观感性认知对图像结果进行排序。在对图像理解的基础上，提取图像特征，获取相似度，并基于用户体验对显示方式进行布局。本发明在文本无法准确表达的情况下，用户可在该平台上更好地展示自己的购物需求，减少用户对商品的查找时间，更加有效地促成网络商品交易。同时，本发明将促进新型电子购物平台的发展，使中国电子商务网络平台更加多元化。"><meta name="DC.date" content="2010-10-6"><meta name="DC.relation" content="CN:101216841:A" scheme="references"><meta name="DC.relation" content="CN:101271476:A" scheme="references"><meta name="DC.relation" content="CN:101334796:A" scheme="references"><meta name="DC.relation" content="JP:2006332785" scheme="references"><meta name="DC.relation" content="US:6647157" scheme="references"><meta name="citation_patent_publication_number" content="CN:101853299:A"><meta name="citation_patent_application_number" content="CN:201010186515"><link rel="canonical" href="https://www.google.com/patents/CN101853299A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN101853299A?cl=zh"/><meta name="title" content="专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法"/><meta name="description" content="本发明公开了一种基于感性认知的图像检索结果排序方法，该方法主要基于图像的客观特征和主观感性认知对图像结果进行排序。在对图像理解的基础上，提取图像特征，获取相似度，并基于用户体验对显示方式进行布局。本发明在文本无法准确表达的情况下，用户可在该平台上更好地展示自己的购物需求，减少用户对商品的查找时间，更加有效地促成网络商品交易。同时，本发明将促进新型电子购物平台的发展，使中国电子商务网络平台更加多元化。"/><meta property="og:title" content="专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN101853299A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN101853299A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN101853299A&amp;usg=AFQjCNECGYWfkdEEodO0wEcp0o3blex2eA" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/b39babf3af1a8b17fc55/CN101853299A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/b39babf3af1a8b17fc55/CN101853299A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN101853299A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN101853299A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN101853299A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN101853299B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN101853299A?cl=zh" style="display:none"><span itemprop="description">本发明公开了一种基于感性认知的图像检索结果排序方法，该方法主要基于图像的客观特征和主观感性认知对图像结果进行排序。在对图像理解的基础上，提取图像特征，获取相似度，并基于用户体验对显示方式进行布局。本发...</span><span itemprop="url">https://www.google.com/patents/CN101853299A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法" title="专利 CN101853299A - 一种基于感性认知的图像检索结果排序方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN101853299 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201010186515</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2010年10月6日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2010年5月31日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2010年5月31日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101853299B?hl=zh-CN&amp;cl=zh">CN101853299B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201010186515.7, </span><span class="patent-bibdata-value">CN 101853299 A, </span><span class="patent-bibdata-value">CN 101853299A, </span><span class="patent-bibdata-value">CN 201010186515, </span><span class="patent-bibdata-value">CN-A-101853299, </span><span class="patent-bibdata-value">CN101853299 A, </span><span class="patent-bibdata-value">CN101853299A, </span><span class="patent-bibdata-value">CN201010186515, </span><span class="patent-bibdata-value">CN201010186515.7</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%BE%90%E8%88%92%E7%95%85%22">徐舒畅</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9E%97%E5%BB%BA%E8%81%AA%22">林建聪</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E6%B5%B7%E6%B4%8B%22">王海洋</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%83%91%E8%81%83%22">郑聃</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%BB%84%E7%90%A6%22">黄琦</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E6%9D%AD%E5%B7%9E%E6%B7%98%E6%B7%98%E6%90%9C%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">杭州淘淘搜科技有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101853299A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101853299A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101853299A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (19),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201010186515&amp;usg=AFQjCNG2XKfTOdUvRiHNH1n9er2z2OwGNw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D101853299A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHwLLowYk_nNCn8MWmRuonD5daQFA"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT98271531" lang="ZH" load-source="patent-office">一种基于感性认知的图像检索结果排序方法</invention-title>
      </span><br><span class="patent-number">CN 101853299 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA80751413" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明公开了一种基于感性认知的图像检索结果排序方法，该方法主要基于图像的客观特征和主观感性认知对图像结果进行排序。在对图像理解的基础上，提取图像特征，获取相似度，并基于用户体验对显示方式进行布局。本发明在文本无法准确表达的情况下，用户可在该平台上更好地展示自己的购物需求，减少用户对商品的查找时间，更加有效地促成网络商品交易。同时，本发明将促进新型电子购物平台的发展，使中国电子商务网络平台更加多元化。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(1)</span></span></div><div class="patent-text"><ol mxw-id="PCLM33440723" lang="ZH" load-source="patent-office" class="claims">
    <li class="claim"> <div num="1" class="claim">
      <div class="claim-text">一种基于感性认知的图像检索结果排序方法，其特征在于，包括如下步骤：(1)构建图像特征库。1.1)在构建图像特征库前，预先获得入库图像的掩膜区域。1.2)获得图像的掩膜区域后，得到图像的颜色特征。1.3)获得图像的掩膜区域后，得到图像的形状特征。1.4)获得图像的掩膜区域后，对于T恤等特殊类别服装，得到图案特征。1.5)构建检索数据库，整个数据库由原始图像和特征文件两部分构成。(2)以B/S架构在客户端建立导购平台，用户可选择库中图像或其它图像作为输入进行检索，检索结果返回客户端。(3)基于感性认知的相似性显示检索结果。3.1)根据图像的颜色、形状、图案特征，以及款式、风格主观特征对检索结果进行排序。3.2)显示页面中，以基于相似性的顺序排列或者基于特征的纵横交错布局，分别根据颜色、形状、图案特征的相似度进行排序。3.3)显示页面中，每张结果图同时含有商品价格、商家链接、比价链接等多种属性。其中，所述步骤(1.1)中，所述图像掩膜区域的获取方法是：采用目标自动定位方法，粗略估算图像中目标物体所在的长方形区域。对于自动定位不准确的图像，采用人工画框确定长方形区域。确定长方形区域后，利用图像分割算法获得目标的非规则性准确区域，即掩膜区域。所述步骤(1.2)中，图像的颜色特征获取方法为：首先将红、绿、蓝三原色进行量化，形成有限个格子。根据量化后的颜色分布，得到颜色直方图。取前N位颜色为初始聚类中心，利用K-均值算法进行颜色聚类。将最终聚类后的颜色从红、绿、蓝三原色转换到色调-饱和度-亮度颜色空间。色调-饱和度-亮度颜色空间空间被量化为M级，分别是H值M1级，S值和V值各M2级。最终将转换后的色调-饱和度-亮度颜色空间颜色分类以及该类颜色占的比重保存到特征文件。所述步骤(1.3)中，图像的形状特征获取方法为：在掩膜区域利用N线法，衡量每条线和掩膜区域宽度的比例，以N条线的比例值数组作为形状特征。而对于箱包之类的图像，形状特征还包括长宽比。所述步骤(1.4)中，图像的图案特征获取方法为：首先采用目标自动定位方法，粗略估算掩膜区域区域中图案所在的长方形区域。对于自动定位不准确的图像，采用人工画框确定长方形区域。确定长方形区域后，获取图案区域的缩放、旋转不变的特征变换SIFT特征作为图案特征。所述步骤(1.5)中，整个数据库中的图像数据来源于网络，每张图像在入库前，需要单独获取各种特征，并存入特征文件。采用分段方式存储图像特征，每次读入一个分段的所有图像的特征，加快后期的检索速度。所述步骤(2)中，检索步骤是：输入样图，首先获取样图的MASK区域，然后在MASK区域获取样图的特征。将样图特征与数据库中图像的特征进行比较，返回前N张结果图。样图可以是库中的，也可以是用户自己上传的。所述步骤(3)中，基于样图检索得到的结果图列表中，按照感知相似性进行结果图的展示。结果图中同时含有商品价格、相应网络商家链接。单击结果图，将可以该张结果图作为输入图进行新一轮的检索。</div>
    </div>
  </li> </ol>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES38706819" lang="ZH" load-source="patent-office" class="description">
    <p>一种基于感性认知的图像检索结果排序方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及图像搜索技术领域，尤其涉及一种基于感性认知的图像检索结果排序 方法。</p>
    <p>背景技术</p>
    <p>[0002]	目前市场上存在几个典型的搜索引擎，包括百度、Google、搜狐的搜狗和微软的 Bing。上述搜索引擎面向文本，适合各种用户，且已经基本占领了文本搜索的市场。</p>
    <p>[0003]	但是文本搜索引擎也具有某些不足和缺陷。当人们要搜索某些无法确切描述的 内容时，或者需要搜素的内容含有主观概念，又或者需要搜索和已知格式的数据（音频、视 频、图像、3D网格等各种多媒体数据）很类似的结果时，文本搜索就显示了它的不足。为此， 市场上出现了一些基于图像的搜索引擎。</p>
    <p>[0004]	基于图像的搜索引擎需要输入样例图，然后从数据库中查找和样例图的图像特 征相似的结果。比如，www, tinyeye. com, www. like, com就是基于图像检索（记为CBIR ： Content Based Image Retrieval)的例子。大部分基于图像的搜索引擎以图像内容理解和 模式识别等技术为基础，且面向各种各种的图像。</p>
    <p>[0005]	现有的CBIR具有以下不足之处：对于服饰类的CBIR系统，现有的应用仅仅依靠图 像的视觉特征进行检索，而没有考虑人们在现实中的购买环境，缺乏真实体验感。另外，现 有系统中的图像数据都相对比较统一，数据量也不多。</p>
    <p>[0006]	随着互联网上各类数据的爆炸式增长，人们寻找目标产品所需的时间越来越多。 另外，虽然存在各种功能强大的文本搜索引擎，但对于服饰类产品，用户常常无法利用文字 准确描述需求。</p>
    <p>发明内容</p>
    <p>[0007]	本发明的目的在于针对现有技术的不足，提供一种基于感性认知的图像检索结果 排序方法。本发明利用样图描述用户的需求，在基于图像内容的理解上，帮助用户快速寻找 到目标产品（主要是服饰类产品）。</p>
    <p>[0008]	本发明主要基于图像的客观特征和主观感性认知对图像结果进行排序。在对图像 理解的基础上，提取图像特征，取得相似度，并基于用户体验对显示方式进行布局。这种方 式不但能及时找到用户的目标产品，而且能给用户极大的视觉冲击，积极引导用户进入购 物环节。这是文本检索无法实现的，是一种新型的电子导购平台。</p>
    <p>[0009]	为了建立这样一种方便直观的智能导购平台，本发明采取以下步骤作为技术方案。</p>
    <p>[0010]	1)首先，建立含有大量图像的原始数据库。</p>
    <p>[0011]	为了建立图像数据库，需要利用网络爬虫去各种含有服饰类图像的网站抓取原始 数据。</p>
    <p>[0012]	2)针对库中的每张图像，获取图像中目标所在的精确区域，记为MASK区域（掩膜区域：记为MASK)。</p>
    <p>[0013]	为了获取MASK区域，需要开发一种半自动的目标定位子系统，用于确定服饰在图 像中的大概位置，并在此基础上利用图像分割技术获得准确的区域。</p>
    <p>[0014]	3)根据每张图像的MASK区域获取各种图像特征。</p>
    <p>[0015]	针对服饰类图像，能用于特征表述的参数有颜色、形状、纹理和图案等。不同种类 的衣服可能需要获取不同的特征。比如T恤不需要形状特征，但需要图案特征。</p>
    <p>[0016]	4)建立含有图像和特征数据的综合数据库。</p>
    <p>[0017]	整个数据库由原始图像数据库和特征数据库组成。为了方便存储和访问，需要将 图像数据和特征数据分段存储。库中每新增一张图像，都需要提取其特征，并将其存入特征 数据库。每删除一张图像，需要同时删除原始图像和其特征数据。</p>
    <p>[0018]	5)搭建B/S结构平台，向用户提供基于样例图的检索服务。</p>
    <p>[0019]	综合数据库放在服务器，客户端建立一个入口平台，方便用户上传图像，或者从库 中选择图像作为样例图进行检索。服务器端根据图像的颜色、形状、局部图案等特征，按照 相似性返回数据库中与样例图比较接近的系列图像，最终检索结果显示在客户端。</p>
    <p>[0020]	6)检索结果图中，根据感知相似度对结果进行排序。</p>
    <p>[0021]	基于样图检索得到的结果图列表中，按照布局方式，以客观特征（颜色特征、形状 或图案特征等）和主观特征（产品风格等）的相似性进行结果图的展示。结果图中同时含 有商品价格、相应网络商家链接等信息。单击结果图，将可以该张结果图作为输入图进行新 一轮的检索。</p>
    <p>[0022]	本发明的有益效果是：作为一种新型的电子购物引导平台，在文本无法准确表达 的情况下，结合主观特征和客观特征的图像相似性可以更快、更准确地找到目标商品。用户 可在该平台上更好地展示自己的购物需求，减少商品的查找时间，更加高效地促成网络商 品交易。同时，本发明将促进新型电子购物平台的发展，使中国电子商务网络平台更加多元 化。</p>
    <p>附图说明</p>
    <p>[0023]	图1是系统框架图；</p>
    <p>[0024]	图2是颜色特征提取流程图；</p>
    <p>[0025]	图3是形状特征提取示意图；</p>
    <p>[0026]	图4是B/S架构示意图；</p>
    <p>[0027]	图5是局部匹配模块流程图；</p>
    <p>[0028]	图6是客户端页面的搜索结果显示效果示意图。</p>
    <p>具体实施方式</p>
    <p>[0029]	下面以服饰类图像的检索和显示为例，结合附图对本发明做进一步详细的说明。 本发明涉及到的操作可综合为下表所示，而整个系统的框架及流程见图1所示。</p>
    <p>[0030]	本发明的基于感性认知的图像检索结果排序方法，包括以下步骤：</p>
    <p>[0031]	1. 1)在构建图像特征库前，采用目标自动定位方法，获取入库图像的掩膜区域。</p>
    <p>[0032]	由图1可见，目标提取模块包括网络数据抓取、初步过滤子模块、目标定位子模块、图像掩码提取子模块等四个步骤。网络数据抓取模块利用网络爬虫机器人，从互联网上 搜集相关的服饰图像。但是爬虫机器人只根据图像格式的判断进行抓取，因此并不是所有 下载到的图像都是所需的服饰类图像。初步过滤子模块的功能就是删除一些明显的非服饰 类图像，过滤策略包括：格式过滤，即只下载特定格式的图像。尺寸过滤，即根据图像大小、 长宽比等数据过滤一些无用的图像。图像属性过滤，去除所有非彩色的图像。由于网络上 各类图像没有标准和规范，非常不统一。因此很多图像中，没有固定的背景，一张图像中可 能含有好几件衣服或者服饰类物品，图像中可能含有模特等等。目标定位子模块主要用来 确定衣服等目标区域所在的大概位置。</p>
    <p>[0033]	在目标定位子模块中，需要对原始图像数据进行分类，对不同的分类采取不同的 方法进行定位。目前的分类有：</p>
    <p>[0034]	&#8226;衣服平铺类：基于平铺时拍摄者会将衣服放在与衣服颜色有区分度的背景下的 假设，因此采用大津法（0STU算法）直接进行二值化处理，然后分析二值图中的连通区域信 息，最终确定目标的合理位置。该类的定位效果比较理想，且能直接获得掩膜MASK数据，省 略了后面的掩膜区域获取子模块的处理。</p>
    <p>[0035]	&#8226;衣服模特类：很多衣服图像中都有模特，可采用人脸检测的算法，获得衣服的大 致区域。</p>
    <p>[0036]	&#8226;衣服分格类：首先检测衣服的分格区间，然后再每个区间分别采用不同的方法。</p>
    <p>[0037]	&#8226;其它分类：除了上述分类的其它分类。</p>
    <p>[0038]	目标定位结果只是一个长方形的框，框内除了目标物以外，还有可能存在其它物 体或者背景。因此，需要得到目标物的精确区域，这就需要掩码提取。图像掩码提取子模块 是在目标定位子模块的基础上，获取图像中目标的精确位置。目前采用基于最小能量的收 敛算法。</p>
    <p>[0039]	1. 2)获得图像的掩膜区域后，提取图像的颜色特征。</p>
    <p>[0040]	颜色特征的提取方法如图2所示。步骤如下：</p>
    <p>[0041]	&#8226;颜色量化：将每个通道8位共256级量化为16级，红、绿、蓝三原色RGB三个通 道共4096级，即4096个格子Bin。</p>
    <p>[0042]	&#8226;颜色聚类：根据量化后的颜色分布，获取颜色直方图。取前N(目前N = 8)位颜 色为初始聚类中心，利用K-Means进行颜色聚类。</p>
    <p>[0043]	特征保存：将最终聚类后的颜色从RGB转换到色调-饱和度-亮度颜色空间（HSV 空间)。HSV空间被量化为36000级，分别是H值360级，S值和V值各10级。将转换后的 HSV颜色分类以及该类颜色占的比重保存到特征文件。</p>
    <p>[0044]	1. 3)得到图像的掩膜区域后，获取图像的形状特征。</p>
    <p>[0045]	形状特征的获取主要采用“N线法”，如图3所示。在掩膜区域利用N线法，衡量每 条线和MASK宽度的比例，以N条线的比例值数组作为形状特征。针对不同的服饰类别，还 需要获取掩膜区域的长宽比作为一个简单的形状特征。</p>
    <p>[0046]	1. 4)获得图像的掩膜区域后，对于T恤等特殊类别服装，获取图案等特征。</p>
    <p>[0047]	图案特征只在特殊类目的图像中获取，其方法为：首先采用图案区域自动定位方 法，粗略估算MASK区域中T恤上的图案所在的长方形（RECT)区域。对于自动定位不准确的 图像，采用人工画框确定RECT区域。确定衣服上的图案RECT区域后，获取图案区域的SIFT</p>
    <p>6特征作为图案特征。</p>
    <p>[0048]	1. 5)构建检索数据库，整个数据库由原始图像和特征文件两部分组成。</p>
    <p>[0049]	构建检索数据库对应的是“ADD”操作，这个过程可称之为“入库”。为了使整个“入 库”过程自动化，需要建立一整套流程机制和处理、审核规范。如图1所示，不同类目的服饰 放在不同的文件目录中，原始图像根据规定的组织结构放在特定路径下，构成整个原始图 像库。而每入库一张图像，就获取其各种视觉特征，并在特征文件中增加响应记录。不同的 特征记录在不同的特征文件中。由于某些特征较复杂，可能还需要多个文件分别存放特征 数据。</p>
    <p>[0050]	2)以B/S架构在建立导购平台。</p>
    <p>[0051]	导购平台采用B/S架构，即互联网终端用户可通过终端浏览器访问导购平台。服 务端同时需要多台服务器，包括应用服务器、搜索引擎服务器、数据库服务器以及文件服务 器，整个架构如图4所示。其中，应用服务器提供对外网页接口，供用户访问，并收集用户的 请求。当用户发送搜索请求后，应用服务器将把请求转交给图像引擎服务器，由后者获取相 似度信息，并返回检索结果。在整个检索请求的处理过程中，还需要图像服务器和数据库服 务器的配合，共同将检索结果图像序列返回到应用服务器，并最终显示在客户端浏览器。</p>
    <p>[0052]	上述架构可支持大用户量的访问，各服务器节点都可进行扩展，采用集群方式，如 应用服务器、图像引擎服务器、文件服务器、数据库服务器，都可部署多台，统一向外提供服 务，可支持千万级别的日用户访问量。</p>
    <p>[0053]	3. 1)根据图像的视觉特征，对检索结果进行排序。</p>
    <p>[0054]	对商品图像进行排序时，首先考虑图像的局部特征是否相似，即先进行局部匹配， 获得相似性列表。然后在局部匹配的基础上根据颜色、形状或者图案等特征进行层级过滤， 得到二次排序。</p>
    <p>[0055]	局部匹配主要用于从数据库中检索完全含有，或者含有大部分输入样图的图像。 整个算法步骤如图5所示，具体如下：</p>
    <p>[0056]	&#8226;训练图像数据库中每张图像的特征，生成N个视觉单词（Visual Words)。首先 提取所有数据库中的SIFT特征，然后采用级联K-Means算法对SIFT特征进行聚类，生成N 个特征中心，并将此作为视觉单词集合。</p>
    <p>[0057]	&#8226;为了后续的SIFT特征匹配，获取每个SIFT特征的海明码，并连同SIFT特征保存。</p>
    <p>[0058]	&#8226;利用MSER(Most Stable External Region ：最稳外部区域）算法，获取图像数 据库中每张图像的MSER特征。</p>
    <p>[0059]	&#8226;将MSER和SIFT特征进行绑定。如果某个MSER特征对应的区域没有任何SIFT 特征，则去除该MSER特征。否则，以某个MSER特征对应的区域内含有的SIFT特征集作为 后续特征检索的基本特征单元。</p>
    <p>[0060]	&#8226;在进行检索前，需要保存上述的SIFT特征库，对应的海明码集合，以及视觉单 词集合。</p>
    <p>[0061]	&#8226;在进行检索时，首先获取样例图的MSER和SIFT的绑定特征。然后统计每个绑 定特征所对应的视觉单词集合，并根据集合中的每个视觉单词找到含有同样视觉单词的数 据库图像，衡量两者之间的匹配度。对样例图中的每个绑定特征实施上述步骤，并建立一个投票机制，记录匹配度。</p>
    <p>[0062]	&#8226;投票机制的过程如下：SIFT所映射的每一个视觉单词都在视觉单词集合中查 询，对查询到的含有该视觉单词的图像中的绑定特征进行投票打分，投票结果放在临时结 果队列中，投票结果附上绑定特征的编号，用于对投票结果的整理，所有视觉单词都查询完 后，整理临时结果队列，一个SIFT对一张图像的一个绑定特征，只保留一张得分最高的票， 重复的票都删除；将整理后的结果存入投票队列中。</p>
    <p>[0063]	&#8226;整理投票结果，统计每张图像的得分，按分数对图像进行排序，结果写回投票队 列。</p>
    <p>[0064]	为了得到最终的检索结果，采用层级过滤策略。首先利用局部特征匹配过程进行 初步筛选，将筛选后的结果送入颜色和形状特征（或者图案特征）模块进行更进一步的相 似度匹配。并将最终的结果返回给客户端。</p>
    <p>[0065]	3. 2)显示页面进行布局，对相似度进行排序。</p>
    <p>[0066]	所有的检索结果将返回给客户端，并显示在客户端浏览器。显示模式可以有多种 不同的布局。图6所示即为其中一种，客户端显示以斜对角线作为区分线，分别在X方向和 Y方向（以左上角为原点）上按照颜色特征和形状特征（图案特征、局部特征）的相似性进 行结果图的展示。结果图中同时含有商品价格、相应网络商家链接等信息。单击结果图，将 可以该张结果图作为输入图进行新一轮的检索。</p>
    <p>[0067]	3.3)显示页面中，每张结果图同时含有商品价格、商家链接、比价链接等多种属 性。</p>
    <p>[0068]	在结果页面，为了给用户提供更多的选择，引导客户更快地查看商品相关的信息， 以及商品之间的比较，在每个结果显示页面，除了提供结果图以外，还在结果图的周围（上 面或下面）提供商品价格信息、商品的商家链接、以及比价链接等信息。</p>
    <p>[0069]	在研究用户的购物习惯和用户网上购物体验以后，将对结果图像及其相关信息进 行特定的布局排列，使得用户更容易、更方便、更快速的购买到想要的商品。最终目的是为 了快速促成网络交易。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101216841A?cl=zh">CN101216841A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年1月14日</td><td class="patent-data-table-td patent-date-value">2008年7月9日</td><td class="patent-data-table-td ">南京搜拍信息技术有限公司</td><td class="patent-data-table-td ">交互式图像搜索系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101271476A?cl=zh">CN101271476A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年4月25日</td><td class="patent-data-table-td patent-date-value">2008年9月24日</td><td class="patent-data-table-td ">清华大学</td><td class="patent-data-table-td ">网络图像搜索中基于聚类的相关反馈检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101334796A?cl=zh">CN101334796A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年7月18日</td><td class="patent-data-table-td patent-date-value">2008年12月31日</td><td class="patent-data-table-td ">浙江师范大学</td><td class="patent-data-table-td ">一种个性化及协同化融合的网上多媒体检索与查询方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3D2006332785A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHVIhdrR2sO8ORtx4b4stxJiNzYvg">JP2006332785A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">没有名称</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6647157">US6647157</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2000年7月21日</td><td class="patent-data-table-td patent-date-value">2003年11月11日</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Image search apparatus and method</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102024049A?cl=zh">CN102024049A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年12月8日</td><td class="patent-data-table-td patent-date-value">2011年4月20日</td><td class="patent-data-table-td ">中国科学院自动化研究所</td><td class="patent-data-table-td ">一种用于电子商务平台上的图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102254043A?cl=zh">CN102254043A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年8月17日</td><td class="patent-data-table-td patent-date-value">2011年11月23日</td><td class="patent-data-table-td ">电子科技大学</td><td class="patent-data-table-td ">一种基于语义映射的服装图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102254043B?cl=zh">CN102254043B</a></td><td class="patent-data-table-td patent-date-value">2011年8月17日</td><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">电子科技大学</td><td class="patent-data-table-td ">一种基于语义映射的服装图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102270331A?cl=zh">CN102270331A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年8月14日</td><td class="patent-data-table-td patent-date-value">2011年12月7日</td><td class="patent-data-table-td ">黄斌</td><td class="patent-data-table-td ">基于可视化搜索的网络购物导航方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102270331B?cl=zh">CN102270331B</a></td><td class="patent-data-table-td patent-date-value">2011年8月14日</td><td class="patent-data-table-td patent-date-value">2014年5月7日</td><td class="patent-data-table-td ">黄斌</td><td class="patent-data-table-td ">基于可视化搜索的网络购物导航方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102567543A?cl=zh">CN102567543A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年1月12日</td><td class="patent-data-table-td patent-date-value">2012年7月11日</td><td class="patent-data-table-td ">北京搜狗信息服务有限公司</td><td class="patent-data-table-td ">一种服装图片的搜索方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102567543B?cl=zh">CN102567543B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年1月12日</td><td class="patent-data-table-td patent-date-value">2015年2月18日</td><td class="patent-data-table-td ">北京搜狗信息服务有限公司</td><td class="patent-data-table-td ">一种服装图片的搜索方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102622420A?cl=zh">CN102622420A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年2月22日</td><td class="patent-data-table-td patent-date-value">2012年8月1日</td><td class="patent-data-table-td ">哈尔滨工程大学</td><td class="patent-data-table-td ">基于颜色特征和形状上下文的商标图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102622420B?cl=zh">CN102622420B</a></td><td class="patent-data-table-td patent-date-value">2012年2月22日</td><td class="patent-data-table-td patent-date-value">2013年10月30日</td><td class="patent-data-table-td ">哈尔滨工程大学</td><td class="patent-data-table-td ">基于颜色特征和形状上下文的商标图像检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102650998A?cl=zh">CN102650998A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年2月28日</td><td class="patent-data-table-td patent-date-value">2012年8月29日</td><td class="patent-data-table-td ">鸿富锦精密工业（深圳）有限公司</td><td class="patent-data-table-td ">外观设计专利展示系统及方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102663401A?cl=zh">CN102663401A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月18日</td><td class="patent-data-table-td patent-date-value">2012年9月12日</td><td class="patent-data-table-td ">哈尔滨工程大学</td><td class="patent-data-table-td ">一种图像特征提取和描述方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102760144A?cl=zh">CN102760144A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年10月31日</td><td class="patent-data-table-td patent-date-value">2012年10月31日</td><td class="patent-data-table-td ">乐活在线（北京）网络技术有限公司</td><td class="patent-data-table-td ">信息搜索方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102799635A?cl=zh">CN102799635A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td patent-date-value">2012年11月28日</td><td class="patent-data-table-td ">天津大学</td><td class="patent-data-table-td ">一种用户驱动的图像集合排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102799635B?cl=zh">CN102799635B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td patent-date-value">2015年10月28日</td><td class="patent-data-table-td ">天津大学</td><td class="patent-data-table-td ">一种用户驱动的图像集合排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102842135A?cl=zh">CN102842135A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年7月17日</td><td class="patent-data-table-td patent-date-value">2012年12月26日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种商品图像主体区域检测方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103020120A?cl=zh">CN103020120A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年11月16日</td><td class="patent-data-table-td patent-date-value">2013年4月3日</td><td class="patent-data-table-td ">南京理工大学</td><td class="patent-data-table-td ">一种基于超图的图像混合摘要生成方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103049872A?cl=zh">CN103049872A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月19日</td><td class="patent-data-table-td patent-date-value">2013年4月17日</td><td class="patent-data-table-td ">江苏乐买到网络科技有限公司</td><td class="patent-data-table-td ">一种网络购物中自动核对促销信息的方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103455788A?cl=zh">CN103455788A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年3月29日</td><td class="patent-data-table-td patent-date-value">2013年12月18日</td><td class="patent-data-table-td ">东芝泰格有限公司</td><td class="patent-data-table-td ">商品识别装置及商品识别方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2012146136A1?cl=zh">WO2012146136A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月16日</td><td class="patent-data-table-td patent-date-value">2012年11月1日</td><td class="patent-data-table-td ">Beijing Baidu Netcom Science And Technology Co., Ltd.</td><td class="patent-data-table-td ">信息搜索方法及系统</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06Q0030000000">G06Q30/00</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=glV7BwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2010年11月24日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年1月25日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Granted</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/b39babf3af1a8b17fc55/CN101853299A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E6%84%9F%E6%80%A7%E8%AE%A4%E7%9F%A5%E7%9A%84%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2.pdf?id=glV7BwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U1vUiAWO2TMonLpp1unaPoUcIooZQ"},"sample_url":"https://www.google.com/patents/reader?id=glV7BwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>