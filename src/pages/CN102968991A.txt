<!DOCTYPE html><html><head><title>专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_5115ea495017d9115e613207d3810e5a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_5115ea495017d9115e613207d3810e5a__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种语音会议纪要的分类方法、设备和系统"><meta name="DC.contributor" content="詹五洲" scheme="inventor"><meta name="DC.contributor" content="华为技术有限公司" scheme="assignee"><meta name="DC.date" content="2012-11-29" scheme="dateSubmitted"><meta name="DC.description" content="本发明实施例提供一种语音会议纪要的分类方法、设备和系统，涉及通信领域，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。其方法为：根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。本发明实施例用于对语音会议纪要进行分类。"><meta name="DC.date" content="2013-3-13"><meta name="DC.relation" content="CN:101398475:A" scheme="references"><meta name="DC.relation" content="CN:102436812:A" scheme="references"><meta name="DC.relation" content="CN:102625077:A" scheme="references"><meta name="DC.relation" content="CN:1479525" scheme="references"><meta name="DC.relation" content="JP:2004023661" scheme="references"><meta name="citation_patent_publication_number" content="CN:102968991:A"><meta name="citation_patent_application_number" content="CN:201210499273"><link rel="canonical" href="https://www.google.com/patents/CN102968991A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102968991A?cl=zh"/><meta name="title" content="专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统"/><meta name="description" content="本发明实施例提供一种语音会议纪要的分类方法、设备和系统，涉及通信领域，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。其方法为：根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。本发明实施例用于对语音会议纪要进行分类。"/><meta property="og:title" content="专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102968991A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102968991A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102968991A&amp;usg=AFQjCNH-TUO-MPq16CEXKrzWpuWG56uifg" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/921d5a068882b8e2ede2/CN102968991A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/921d5a068882b8e2ede2/CN102968991A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102968991A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102968991A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102968991A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102968991B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102968991A?cl=zh" style="display:none"><span itemprop="description">本发明实施例提供一种语音会议纪要的分类方法、设备和系统，涉及通信领域，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。其方法为：根据会...</span><span itemprop="url">https://www.google.com/patents/CN102968991A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统" title="专利 CN102968991A - 一种语音会议纪要的分类方法、设备和系统"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102968991 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210499273</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2013年3月13日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年11月29日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年11月29日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102968991B?hl=zh-CN&amp;cl=zh">CN102968991B</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2922051A1?hl=zh-CN&amp;cl=zh">EP2922051A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2922051A4?hl=zh-CN&amp;cl=zh">EP2922051A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8838447?hl=zh-CN&amp;cl=zh">US8838447</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20140163970?hl=zh-CN&amp;cl=zh">US20140163970</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2014082445A1?hl=zh-CN&amp;cl=zh">WO2014082445A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210499273.6, </span><span class="patent-bibdata-value">CN 102968991 A, </span><span class="patent-bibdata-value">CN 102968991A, </span><span class="patent-bibdata-value">CN 201210499273, </span><span class="patent-bibdata-value">CN-A-102968991, </span><span class="patent-bibdata-value">CN102968991 A, </span><span class="patent-bibdata-value">CN102968991A, </span><span class="patent-bibdata-value">CN201210499273, </span><span class="patent-bibdata-value">CN201210499273.6</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%A9%B9%E4%BA%94%E6%B4%B2%22">詹五洲</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%8D%8E%E4%B8%BA%E6%8A%80%E6%9C%AF%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8%22">华为技术有限公司</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102968991A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102968991A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102968991A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (3),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (6),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210499273&amp;usg=AFQjCNGsroIQ1fPaoluqPLjwN0LJGP_XVg"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102968991A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGqNNz5zeQqzeLsPiptwyu5upum6g"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT122639550" lang="ZH" load-source="patent-office">一种语音会议纪要的分类方法、设备和系统</invention-title>
      </span><br><span class="patent-number">CN 102968991 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA109540781" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明实施例提供一种语音会议纪要的分类方法、设备和系统，涉及通信领域，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。其方法为：根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。本发明实施例用于对语音会议纪要进行分类。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(19)</span></span></div><div class="patent-text"><div mxw-id="PCLM52002488" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种语音会议纪要的分类方法，其特征在于，包括：  根据会场的音频数据进行声源定位，以获取所述音频数据所对应的声源的方位，并将所述声源的方位写入所述音频数据的附加域信息；  将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器，以使得所述录播服务器根据所述附加域信息对所述音频数据进行分类。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求I所述的方法，其特征在于，在将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器之前，所述方法还包括：  将语音激活标志写入所述附加域信息，其中所述语音激活标志包括已激活或未激活，以便所述录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测所述音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将所述音频数据发送至声纹识别系统。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求2所述的方法，其特征在于，所述将语音激活标志写入所述附加域信息包括：  对所述音频数据进行语音活动侦测处理，以识别所述音频数据是否为语音数据，若所述音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若所述音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</div>
    </div>
    </div> <div class="claim"> <div num="4" class="claim">
      <div class="claim-text">4.	一种语音会议纪要的分类方法，其特征在于，包括：  从多点控制单元接收会场的音频码流和音频码流的附加域信息，所述音频码流的附加域信息包括所述音频码流所对应的声源的方位；  将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中，并将所述码流文件中的音频数据发送至声纹识别系统；  从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求4所述的方法，其特征在于，在所述将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中之后，所述方法还包括：  将所述音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将所述至少两个音频码流按照时间信息进行排序。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.根据权利要求4或5所述的方法，其特征在于，将所述码流文件中的音频数据发送至声纹识别系统；从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中包括：  将所述码流文件中的第一单位时间内的音频数据发送至声纹识别系统；  从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入所述第一单位时间内的音频数据的附加域信息中。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.根据权利要求6所述的方法，其特征在于，在从所述声纹识别系统接收声纹识别结果之前还包括：  将所述码流文件中的第二单位时间内的音频数据的声纹识别结果发送至所述声纹识别系统，所述第二单位时间为所述第一单位时间的上一个单位时间，以便在所述声纹识别系统对所述第一单位时间内的音频数据进行声纹识别时，将所述第二单位时间内的音频数据的声纹识别结果作为参考。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.根据权利要求6或7所述的方法，其特征在于，在将所述码流文件中的音频数据发送至声纹识别系统之前，还包括：  检测所述第一单位时间内的音频数据的附加域信息，若所述第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将所述第一单位时间内的音频数据发送至声纹识别系统。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求4至8任意一项所述的方法，其特征在于，在将所述码流文件中的音频数据发送至声纹识别系统之前，还包括：  检测所述音频码流的附加域信息中的声源方位；  若所述音频码流的附加域信息中所述音频码流对应的声源方位只有一个，则将所述码流文件发送至声纹识别系统；  若所述音频码流的附加域信息包括的所述音频码流对应的声源方位至少有两个，且所述至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则将已识别出来的所述至少两个声源方位对应的参会者身份写入所述音频码流的附加域信息中。</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10.	一种视频设备，其特征在于，包括：  方位获取单元，用于根据会场的音频数据进行声源定位，以获取所述音频数据所对应的声源的方位，并将所述声源的方位写入所述音频数据的附加域信息，再将所述音频数据以及所述音频数据的附加域信息发送至发送单元；  发送单元，用于从所述方位获取单元接收所述音频数据以及所述音频数据的附加域信息，将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器，以使得所述录播服务器根据所述附加域信息对所述音频数据进行分类。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11.根据权利要求9所述的视频设备，其特征在于，所述视频设备还包括：  标志写入单元，用于将语音激活标志写入所述附加域信息，其中所述语音激活标志包括已激活或未激活，以便所述录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测所述音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将所述音频数据发送至声纹识别系统。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12.根据权利要求11所述的视频设备，其特征在于，所述标志写入单元具体用于：  对所述音频数据进行语音活动侦测处理，以识别所述音频数据是否为语音数据，若所述音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若所述音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13.	一种录播服务器，其特征在于，包括：  接收单元，用于从多点控制单元接收会场的音频码流和音频码流的附加域信息，所述音频码流的附加域信息包括所述音频码流所对应的声源的方位，并将所述音频码流发送至分类识别单元；分类识别单元，用于从所述接收单元接收所述音频码流，将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中，并将所述码流文件中的音频数据发送至声纹识别系统；  身份匹配单元，用于从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14.根据权利要求13所述的录播服务器，其特征在于，所述录播服务器还包括：  排序单元，用于将所述音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将所述至少两个音频码流按照时间信息进行排序。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15.根据权利要求13或14所述的录播服务器，其特征在于，包括：  所述分类识别单元具体用于将所述码流文件中的第一单位时间内的音频数据发送至声纹识别系统；  所述身份匹配单元具体用于从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入所述第一单位时间内的音频数据的附加域信息中。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16.根据权利要求15所述的录播服务器，其特征在于，包括：  所述分类识别单元还具体用于将所述码流文件中的第二单位时间内的音频数据的声纹识别结果发送至所述声纹识别系统，所述第二单位时间为所述第一单位时间的上一个单位时间，以便在所述声纹识别系统对所述第一单位时间内的音频数据进行声纹识别时，将所述第二单位时间内的音频数据的声纹识别结果作为参考。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17.根据权利要求15或16所述的录播服务器，其特征在于，在将所述码流文件中的音频数据发送至声纹识别系统之前，所述分类识别单元还用于：  检测所述第一单位时间内的音频数据的附加域信息，若所述第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将所述第一单位时间内的音频数据发送至声纹识别系统。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18.根据权利要求13至17任意一项所述的录播服务器，其特征在于，在将所述码流文件中的音频数据发送至声纹识别系统之前，  所述分类识别单元还具体用于检测所述音频码流的附加域信息中的声源方位；  若所述音频码流的附加域信息中所述音频码流对应的声源方位只有一个，则所述分类识别单元还具体用于将所述码流文件发送至声纹识别系统；  若所述音频码流的附加域信息包括的所述音频码流对应的声源方位至少有两个，且所述至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则所述身份匹配单元还具体用于将已识别出来的所述至少两个声源方位对应的参会者身份写入所述音频码流的附加域信息中。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19.	一种视频会议系统，包括声纹识别系统和多点控制单元，其特征在于，所述系统还包括：如权利要求10至12任意一项所述的视频设备；如权利要求13至18任意一项所述的录播服务器。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES59078709" lang="ZH" load-source="patent-office" class="description">
    <p>一种语音会议纪要的分类方法、设备和系统</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及通信领域，尤其涉及一种语音会议纪要的分类方法、设备和系统。</p>
    <p>背景技术</p>
    <p>[0002]	随着视频会议技术的飞速发展，类似于普通会议开会过程中人工产生会议记录，在多点视频会议中，也同样存在会议纪要的需求。现有产品已经可以实现在视频会议过程中自动记录整个会议的音视频、数据等内容，如果只是对音频数据单纯的记录下来，当对会议的重点内容或者特定内容进行回顾时，就无法达到普通会议那种可以按发言人进行分类的会议纪要整理需求。</p>
    <p>[0003]	在视频会议进行中，如果可以确定整个语音文件只有一个人在讲话，就可以直接 将整个文件的语音数据发送至声纹识别系统进行识别。如果语音文件中有多个人的语音，则需要先对语音文件进行分段，然后对每段语音数据分别进行声纹识别。现有的声纹识别系统，通常需要10秒以上的语音数据，数据越长，准确度越高。因此，在对语音数据进行分段时，段不能太短。由于在视频会议中，自由交谈的场景较多，因此当对语音数据的分段较长时，一段语音可能包含多个人的语音，在将这多个人的语音数据段送到声纹识别系统进行识别时，识别结果将是不可靠的。</p>
    <p>发明内容</p>
    <p>[0004]	本发明的实施例提供一种语音会议纪要的分类方法、设备和系统，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0005]	为达到上述目的，本发明的实施例采用如下技术方案：</p>
    <p>[0006]	第一方面，提供一种语音会议纪要的分类方法，包括：</p>
    <p>[0007]	根据会场的音频数据进行声源定位，以获取所述音频数据所对应的声源的方位，并将所述声源的方位写入所述音频数据的附加域信息；</p>
    <p>[0008]	将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器，以使得所述录播服务器根据所述附加域信息对所述音频数据进行分类。</p>
    <p>[0009]	在一种可能实现的方式中，结合第一方面，在将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器之前，所述方法还包括：</p>
    <p>[0010]	将语音激活标志写入所述附加域信息，其中所述语音激活标志包括已激活或未激活，以便所述录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测所述音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将所述音频数据发送至声纹识别系统。</p>
    <p>[0011]	在第二种可能实现的方式中，结合第一方面的第一种可能的实现方式，所述将语音激活标志写入所述附加域信息包括：[0012]	对所述音频数据进行语音活动侦测处理，以识别所述音频数据是否为语音数据，若所述音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若所述音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0013]	第二方面，提供一种语音会议纪要的分类方法，包括：</p>
    <p>[0014]	从多点控制单元接收会场的音频码流和音频码流的附加域信息，所述音频码流的附加域信息包括所述音频码流所对应的声源的方位；</p>
    <p>[0015]	将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中，并将所述码流文件中的音频数据发送至声纹识别系统；</p>
    <p>[0016]	从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所 对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中。</p>
    <p>[0017]	在第一种可能实现的方式中，结合第二方面，在所述将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中之后，所述方法还包括：</p>
    <p>[0018]	将所述音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将所述至少两个音频码流按照时间信息进行排序。</p>
    <p>[0019]	在第二种可能实现的方式中，结合第二方面或第二方面的第一种可能实现的方式，将所述码流文件中的音频数据发送至声纹识别系统；从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中包括：</p>
    <p>[0020]	将所述码流文件中的第一单位时间内的音频数据发送至声纹识别系统；</p>
    <p>[0021]	从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入所述第一单位时间内的音频数据的附加域信息中。</p>
    <p>[0022]	在第三种可能实现的方式中，结合第二方面的第二种可能实现的方式，从所述声纹识别系统接收声纹识别结果之前还包括：</p>
    <p>[0023]	将所述码流文件中的第二单位时间内的音频数据的声纹识别结果发送至所述声纹识别系统，所述第二单位时间为所述第一单位时间的上一个单位时间，以便在所述声纹识别系统对所述第一单位时间内的音频数据进行声纹识别时，将所述第二单位时间内的音频数据的声纹识别结果作为参考。</p>
    <p>[0024]	在第四种可能实现的方式中，结合第二方面或第二方面的第一种更可能实现的方式至第三种可能实现的方式，在将所述码流文件中的音频数据发送至声纹识别系统之前，还包括：</p>
    <p>[0025]	检测所述第一单位时间内的音频数据的附加域信息，若所述第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将所述第一单位时间内的音频数据发送至声纹识别系统。[0026]	在第五种可能实现的方式中，结合第二方面或第二方面的第一种可能实现的方式至第四种可能实现的方式，在将所述码流文件中的音频数据发送至声纹识别系统之前，还包括：</p>
    <p>[0027]	检测所述音频码流的附加域信息中的声源方位；</p>
    <p>[0028]	若所述音频码流的附加域信息中所述音频码流对应的声源方位只有一个，则将所述码流文件发送至声纹识别系统；</p>
    <p>[0029]	若所述音频码流的附加域信息包括的所述音频码流对应的声源方位至少有两个，且所述至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则将已识别出来的所述至少两个声源方位对应的参会者身份写入所述音频码流的附加域信息中。 </p>
    <p>[0030]	第三方面，提供一种视频设备，包括：</p>
    <p>[0031]	方位获取单元，用于根据会场的音频数据进行声源定位，以获取所述音频数据所对应的声源的方位，并将所述声源的方位写入所述音频数据的附加域信息，再将所述音频数据以及所述音频数据的附加域信息发送至发送单元；</p>
    <p>[0032]	发送单元，用于从方位获取单元和标志写入单元接收所述音频数据以及所述音频数据的附加域信息，将所述音频数据打包成音频码流，将所述音频码流和所述音频码流的附加域信息发送至录播服务器，以使得所述录播服务器根据所述附加域信息对所述音频数据进行分类。</p>
    <p>[0033]	在一种可能实现的方式中，结合第三方面，所述视频设备还包括：</p>
    <p>[0034]	标志写入单元，用于将语音激活标志写入所述附加域信息，其中所述语音激活标志包括已激活或未激活，以便所述录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测所述音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将所述音频数据发送至声纹识别系统。</p>
    <p>[0035]	在第二种可能实现的方式中，结合第三方面的第一种可能的实现方式，所述标志写入单元具体用于：</p>
    <p>[0036]	对所述音频数据进行语音活动侦测处理，以识别所述音频数据是否为语音数据，若所述音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若所述音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0037]	第四方面，提供一种录播服务器，包括：</p>
    <p>[0038]	接收单元，用于从多点控制单元接收会场的音频码流和音频码流的附加域信息，所述音频码流的附加域信息包括所述音频码流所对应的声源的方位，并将所述音频码流发送至分类识别单元；</p>
    <p>[0039]	分类识别单元，用于从所述接收单元接收所述音频码流，将所述音频码流所解码出的音频数据，存储到与所述音频码流所属的会场号以及所述音频码流所对应的声源的方位对应的码流文件中，并将所述码流文件中的音频数据发送至声纹识别系统；</p>
    <p>[0040]	身份匹配单元，用于从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述音频数据所对应的声源的方位对应的参会者身份，并将所述音频数据所对应的声源的方位对应的参会者身份写入所述音频码流的附加域信息中。</p>
    <p>[0041]	在第一种可能实现的方式中，结合第四方面，所述录播服务器还包括：[0042]	排序单元，用于将所述音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将所述至少两个音频码流按照时间信息进行排序。</p>
    <p>[0043]	在第二种可能实现的方式中，结合第四方面或第四方面的第一种可能实现的方式，</p>
    <p>[0044]	所述分类识别单元具体用于将所述码流文件中的第一单位时间内的音频数据发送至声纹识别系统；</p>
    <p>[0045]	所述身份匹配单元具体用于从所述声纹识别系统接收声纹识别结果，所述声纹识别结果包括所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将所述第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入所述第一单 位时间内的音频数据的附加域信息中。</p>
    <p>[0046]	在第三种可能实现的方式中，结合第四方面的第二种可能实现的方式中，包括：</p>
    <p>[0047]	所述分类识别单元还具体用于将所述码流文件中的第二单位时间内的音频数据的声纹识别结果发送至所述声纹识别系统，所述第二单位时间为所述第一单位时间的上一个单位时间，以便在所述声纹识别系统对所述第一单位时间内的音频数据进行声纹识别时，将所述第二单位时间内的音频数据的声纹识别结果作为参考。</p>
    <p>[0048]	在第四种可能实现的方式中，结合第四方面或第四方面的第一种可能实现的方式至第三种可能实现的方式，在将所述码流文件中的音频数据发送至声纹识别系统之前，所述分类识别单元还用于：</p>
    <p>[0049]	检测所述第一单位时间内的音频数据的附加域信息，若所述第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将所述第一单位时间内的音频数据发送至声纹识别系统。</p>
    <p>[0050]	在第五种可能实现的方式中，结合第四方面或第四方面的第一种可能实现的方式至第四种可能实现的方式，在将码流文件中的音频数据发送至声纹识别系统之前，</p>
    <p>[0051]	所述分类识别单元还具体用于检测所述音频码流的附加域信息中的声源方位；</p>
    <p>[0052]	若所述音频码流的附加域信息中所述音频码流对应的声源方位只有一个，则所述分类识别单元还具体用于将所述码流文件发送至声纹识别系统；</p>
    <p>[0053]	若所述音频码流的附加域信息包括的所述音频码流对应的声源方位至少有两个，且所述至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则所述身份匹配单元还具体用于将已识别出来的所述至少两个声源方位对应的参会者身份写入所述音频码流的附加域信息中。</p>
    <p>[0054]	第五方面，提供一种视频会议，包括声纹识别系统和多点控制单元，还包括所述视频设备和所述录播服务器。</p>
    <p>[0055]	本发明实施例提供一种语音会议纪要的分类方法、设备和系统，通过根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>附图说明</p>
    <p>[0056]	为了更清楚地说明本发明实施例或现有技术中的技术方案，下面将对实施例或现有技术描述中所需要使用的附图作简单地介绍，显而易见地，下面描述中的附图仅仅是本发明的一些实施例，对于本领域普通技术人员来讲，在不付出创造性劳动的前提下，还可以根据这些附图获得其他的附图。</p>
    <p>[0057]	图I为本发明实施例提供的一种语音会议纪要的分类方法流程示意图；</p>
    <p>[0058]	图2为本发明实施例提供的另一种语音会议纪要的分类方法流程示意图；</p>
    <p>[0059]	图3为本发明另一实施例提供的一种语音会议纪要的分类方法流程示意图； [0060]	图4为本发明又一实施例提供的一种视频设备结构示意图；</p>
    <p>[0061]	图5为本发明又一实施例提供的另一种视频设备结构示意图；</p>
    <p>[0062]	图6为本发明又一实施例提供的一种录播服务器结构示意图；</p>
    <p>[0063]	图7为本发明又一实施例提供的另一种录播服务器结构示意图；</p>
    <p>[0064]	图8为本发明又一实施例提供的又一种视频设备结构示意图；</p>
    <p>[0065]	图9为本发明又一实施例提供的又一种录播服务器结构示意图；</p>
    <p>[0066]	图10为本发明又一实施例提供的一种视频会议系统结构示意图。</p>
    <p>具体实施方式</p>
    <p>[0067]	下面将结合本发明实施例中的附图，对本发明实施例中的技术方案进行清楚、完整地描述，显然，所描述的实施例仅仅是本发明一部分实施例，而不是全部的实施例。基于本发明中的实施例，本领域普通技术人员在没有做出创造性劳动前提下所获得的所有其他实施例，都属于本发明保护的范围。</p>
    <p>[0068]	本发明实施例提供一种语音会议纪要的分类方法，该方法应用于视频会议系统，该视频会议系统由会场、多点控制单元、录播服务器、声纹识别系统组成，其中会场中包括一个或多个视频设备和麦克风，基于视频设备侧的方法，如图I所示，包括：</p>
    <p>[0069]	S101、视频设备根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息。</p>
    <p>[0070]	S102、视频设备将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类。</p>
    <p>[0071]	进一步的，在执行S102之前还可以包括：视频设备将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，以便录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将音频数据发送至声纹识别系统。</p>
    <p>[0072]	示例性的，可以对音频数据进行语音活动侦测处理，以识别音频数据是否为语音数据，若音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0073]	需要说明的是，一般情况下视频设备需要将音频码流和音频码流的附加域信息先发送至多点控制单元（Multi-Control Unit, MCU)再由多点控制单元转发至发送至录播服务器。</p>
    <p>[0074]	本发明实施例提供另一种语音会议纪要的分类方法，基于录播服务器侧，如图2所示，包括：</p>
    <p>[0075]	S201、录播服务器从多点控制单元接收会场的音频码流和音频码流的附加域信息，音频码流的附加域信息包括音频码流所对应的声源的方位。</p>
    <p>[0076]	S202、 录播服务器将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中，并将码流文件中的音频数据发送至声纹识别系统。</p>
    <p>[0077]	S203、录播服务器从声纹识别系统接收声纹识别结果，声纹识别结果包括音频数据所对应的声源的方位对应的参会者身份，并将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0078]	本发明实施例提供一种语音会议纪要的分类方法，通过根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0079]	本发明又一实施例提供一种语音会议纪要的分类方法，如图3所示，包括：</p>
    <p>[0080]	S301、视频设备根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息。</p>
    <p>[0081]	具体的，可以通过声源定位技术获取会场中声源的方位，该声源定位技术是通过麦克风阵列拾取语音信号，并用数字信号处理技术对其进行分析和处理来得到声源方位的。</p>
    <p>[0082]	S302、视频设备将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活。</p>
    <p>[0083]	可选的，视频设备在将语音激活标志写入附加域信息之前，要先识别音频数据是否为语音数据。具体的，可以通过VAD(Voice Activation Detection,语音活动侦测）处理识别音频数据是否为语音数据，对音频数据进行语音活动侦测处理，以识别音频数据是否为语音数据，若音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0084]	这样做的目的是从音频数据的信号流里识别非语音数据，以便在进行声纹识别时不对非语音数据进行识别，以达到节省资源的目的。</p>
    <p>[0085]	S303、视频设备将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至多点控制单元。</p>
    <p>[0086]	具体的，在视频设备将会场的音频码流和音频码流的附加域信息发送至录播服务器之前，可以通过MCU从视频设备接收音频码流和音频码流的附加域信息，并将音频码流和音频码流的附加域信息转发至NRS (Net Record Server,录播服务器）其中，音频码流的附加域信息包括音频码流所对应的声源的方位。[0087]	其中，多点控制单元是视频会议系统的核心部分，可以为视频会议用户提供群组会议、多组会议的连接服务。视频设备在将打包后的音频码流和音频码流的附加域信息发送至多点控制单元时，音频码流和附加域信息是通过不同的信道进行传输的。</p>
    <p>[0088]	S304、多点控制单元从视频设备接收会场的音频码流和音频码流的附加域信息，并将接收的会场的音频码流和音频码流的附加域信息发送至录播服务器，音频码流的附加域信息包括音频码流所对应的声源的方位。</p>
    <p>[0089]	其中，由于多点控制单元接收到的音频码流和附加域信息是经过打包的，因此多点控制单元在接收到音频码流和附加域信息后，需要通过解码器对接收到的音频码流和音频码流的附加域信息进行解码，以恢复音频码流和附加域信息，其中，音频码流的附加域信息包括音频码流所对应的声源的方位。</p>
    <p>[0090]	进一步的，当有多个会场发送音频码流时，多点控制单元在接收到各个会场的音频码流和音频码流的附加域信息后，可以对各个会场的音频码流的增益大小进行排序，而 后选取音频码流增益最大的前N个会场。例如，可以选取各个会场中的音频码流增益最大的前3个会场或者前4个会场。而后，多点控制单元将语音最大的前N个会场的音频码流和音频码流的附加域信息发送至录播服务器。</p>
    <p>[0091]	S305、录播服务器将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中。</p>
    <p>[0092]	示例性的，录播服务器在接收到由多点控制单元筛选出的最大的前N个会场的音频码流和音频码流的附加域信息时，可以根据这些音频码流所属会场的会场号以及这些音频码流的附加域信息中的声源方位创建对应的码流文件，并将音频码流存储在对应的码流文件中，这样通过将音频码流与会场及声源方位进行关联，实现了音频码流的精准分类，以便在进行声纹识别时能够准确的识别出该音频码流在该会场中所属声源方位的参会者身份。</p>
    <p>[0093]	S306、录播服务器将音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将至少两个音频码流按照时间信息进行排序。</p>
    <p>[0094]	其中，在录播服务器将解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中之后，录播服务器要将音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将至少两个音频码流按照时间信息进行排序。具体的，在做会议纪要记录的同时，需要对与会者的说话先后顺序进行排序，因此需要将接收到的各条音频码流的时间信息写入该音频码流的附加域信息中，以便将各个会场中的各个音频码流按照时间信息进行排序，做到会议纪要记录的清晰准确。</p>
    <p>[0095]	S307、录播服务器检测音频码流的附加域信息中的声源方位。</p>
    <p>[0096]	S308、录播服务器判断音频码流对应的附加域信息中是否只有一个声源方位，若音频码流的附加域信息包括的音频码流对应的声源方位至少有两个，则执行S309 ;若音频码流的附加域信息中音频码流对应的声源方位只有一个，则执行S310。</p>
    <p>[0097]	S309、录播服务器判断至少两个声源方位对应的参会者身份是否已经在上一次进行声纹识别时识别出来，若至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则执行S311 ;若至少两个声源方位对应的参会者身份没有在上一次进行声纹识别时识别出来，则再次执行S307&#12316;S309。</p>
    <p>[0098]	S310、录播服务器将码流文件中的音频数据发送至声纹识别系统（TheVoiceprint identification System, VPS),而后执行 S312。</p>
    <p>[0099]	其中，在录播服务器将码流文件中的音频数据发送至声纹识别系统之前，录播服务器要检测第一单位时间内的音频数据的附加域信息，若第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将第一单位时间内的音频数据发送至声纹识别系统。</p>
    <p>[0100]	具体的，录播服务器检测第η个单位时间内存储的属于某一会场的某一方位的音频码流的附加域信息中的语音激活标志，若这段音频码流的所有附加域信息中的语音激活标志都未激活，则不进行处理，这样，当该第η个单位时间内存储的属于某一会场的某一方位的音频码流的附加域信息中的语音激活标志都未激活时，表示这些音频数据不是语音， 就不用将该音频数据发送至声纹识别系统进行检测了，节省了资源。</p>
    <p>[0101]	示例性的，上述的单位时间的时间长度可以根据实际情况来设置，例如，可以将单位时间的时间长度设置为lrnin。</p>
    <p>[0102]	声纹识别技术是一种生物特征识别技术，也称为说话人识别，具体的，声纹识别系统可以将需要识别的语音数据的声纹特征与声纹库中的所有声纹进行匹配，以识别说话人身份。一般为了进行可靠的识别，通常需要10秒以上的语音数据存储，数据越长，准确度越闻。</p>
    <p>[0103]	S311、录播服务器将已识别出来的至少两个声源方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0104]	具体的，由于当一段音频码流的附加域信息中出现至少两个声源方位时，若将该音频码流送入声纹识别系统可能会到导致识别错误，因此，当音频码流的附加域信息存在至少两个声源的方位时，由于之前已经检测出的该方位的姓名信息，就不再将音频码流进行声纹识别，而是将之前检测出的该方位对应的参会者身份即姓名信息直接写入该方位出现重叠的音频码流附加域的姓名信息上，从而实现了在多个人同时说话时识别声源方位的目的，也增加会议纪要分类的准确度。</p>
    <p>[0105]	S312、录播服务器从声纹识别系统接收声纹识别结果，声纹识别结果包括音频数据所对应的声源的方位对应的参会者身份，并将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0106]	示例性的，录播服务器可以将在第一单位时间内的音频码流所解码出的音频数据存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件，并将码流文件中的第一单位时间内的音频数据发送至声纹识别系统；</p>
    <p>[0107]	而后录播服务器从声纹识别系统接收声纹识别结果，声纹识别结果包括在第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入第一单位时间内的音频数据的附加域信息中。</p>
    <p>[0108]	或者，优选的，在从声纹识别系统接收声纹识别结果之前还可以包括：将码流文件中的第二单位时间内的音频数据的声纹识别结果发送至声纹识别系统，第二单位时间为第一单位时间的上一个单位时间，以便在声纹识别系统对第一单位时间内的音频数据进行声纹识别时，将第二单位时间内的音频数据的声纹识别结果作为参考。这样，在有前一次识别结果作为参考的情况下进行识别，可以提高声纹识别速度。</p>
    <p>[0109]	本发明实施例提供一种语音会议纪要的分类方法，通过根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0110]	本发明又一实施例提供一种视频设备01，如图4所示，包括：</p>
    <p>[0111]	方位获取单元011，用于根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将音频数据以及音频数据的附加域信息发送至发送单元013。</p>
    <p>[0112]	发送单元013，用于从方位获取单元011接收音频数据以及音频数据的附加域信息，将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类。</p>
    <p>[0113]	进一步的，如图5所示，视频设备01还可以包括：</p>
    <p>[0114]	标志写入单元012，用于在将音频数据以及音频数据的附加域信息发送至发送单元013之前，从方位获取单元011接收附加域信息，并将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，而后将附加域信息发送至发送单元013，以便录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将音频数据发送至声纹识别系统。 [0115]	其中，标志写入单元012可以具体用于：</p>
    <p>[0116]	对音频数据进行语音活动侦测处理，以识别音频数据是否为语音数据，若音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若音频数据不是语音数据，则在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0117]	本发明实施例提供一种视频设备，通过会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，将语音激活标志写入附加域信息，将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0118]	本发明又一实施例提供一种录播服务器02，如图6所示，包括：</p>
    <p>[0119]	接收单元021，用于从多点控制单元接收会场的音频码流和音频码流的附加域信息，音频码流的附加域信息包括音频码流所对应的声源的方位，并将音频码流发送至分类识别单元022。</p>
    <p>[0120]	分类识别单元022，用于从接收单元021接收音频码流，将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中，并将码流文件中的音频数据发送至声纹识别系统。[0121]	身份匹配单元023，用于从分类识别单元022的声纹识别系统接收声纹识别结果，声纹识别结果包括音频数据所对应的声源的方位对应的参会者身份，并将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0122]	进一步的，如图7所示，录播服务器02还可以包括：</p>
    <p>[0123]	排序单元024，用于将音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将至少两个音频码流按照时间信息进行排序。</p>
    <p>[0124]	再进一步的，分类识别单元022可以具体用于将在第一单位时间内的音频码流所解码出的音频数据存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件，并将码流文件中的第一单位时间内的音频数据发送至声纹识别系统。</p>
    <p>[0125]	身份匹配单元023可以具体用于从声纹识别系统接收声纹识别结果，声纹识别结果包括第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将第一单位 时间内的音频数据所对应的声源的方位对应的参会者身份写入第一单位时间内的音频数据的附加域信息中。</p>
    <p>[0126]	优选的，在从声纹识别系统接收声纹识别结果之前，分类识别单元022还可以具体用于将码流文件中的第二单位时间内的音频数据的声纹识别结果发送至声纹识别系统，第二单位时间为第一单位时间的上一个单位时间，以便在声纹识别系统对第一单位时间内的音频数据进行声纹识别时，将第二单位时间内的音频数据的声纹识别结果作为参考。</p>
    <p>[0127]	再进一步的，在将码流文件中的音频数据发送至声纹识别系统之前，分类识别单元还用于022还可以具体用于：</p>
    <p>[0128]	检测第一单位时间内的音频数据的附加域信息，若第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将第一单位时间内的音频数据发送至声纹识别系统。</p>
    <p>[0129]	更进一步的，在将码流文件中的音频数据发送至声纹识别系统之前，分类识别单元022还可以具体用于：</p>
    <p>[0130]	检测音频码流的附加域信息中的声源方位；</p>
    <p>[0131]	若音频码流的附加域信息中音频码流对应的声源方位只有一个，则分类识别单元022还可以用于将码流文件发送至声纹识别系统；</p>
    <p>[0132]	若音频码流的附加域信息包括的音频码流对应的声源方位至少有两个，且至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则身份匹配单元023还可以用于将已识别出来的两个声源方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0133]	本发明实施例提供一种录播服务器，通过从多点控制单元接收会场的音频码流和音频码流的附加域信息，音频码流的附加域信息包括音频码流所对应的声源的方位，将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中，并将码流文件发送至声纹识别系统，从声纹识别系统接收声纹识别结果，再将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0134]	本发明又一实施例提供一种视频设备05，包括第一接收机051、第一存储器052和总线055,如图8所示,还包括：</p>
    <p>[0135]	第一处理器053，用于根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息。</p>
    <p>[0136]	第一发射机054，用于将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类。</p>
    <p>[0137]	进一步的，第一处理器053在将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至第一发射机054之前，还用于将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，以便录播服务器在将解码出的音频数据发送至声纹识别系统之前，检测音频数据的附加域信息中的语音激活标志，并在语音激活标志为已激活时将音频数据发送至声纹识别系统。其中，第一处理器053还可以具体用于：</p>
    <p>[0138]	对音频数据进行语音活动侦测处理，以识别音频数据是否为语音数据，若音频数据为语音数据，则在附加域信息中将语音激活标志写为激活；若音频数据不是语音数据，则 在附加域信息中将语音激活标志写为未激活。</p>
    <p>[0139]	本发明实施例提供一种视频设备，通过会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，将语音激活标志写入附加域信息，将音频数据打包成音频码流，将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0140]	本发明又一实施例提供一种录播服务器06，包括第二存储器061、第二发射机063和总线065,如图9所示,还包括：</p>
    <p>[0141]	第二接收机062，用于从多点控制单元接收会场的音频码流和音频码流的附加域信息，音频码流的附加域信息包括音频码流所对应的声源的方位。</p>
    <p>[0142]	第二存储器061，用于将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中，并通过第二发射机063将码流文件中的音频数据发送至声纹识别系统。</p>
    <p>[0143]	第二处理器064，用于从声纹识别系统接收声纹识别结果，声纹识别结果包括音频数据所对应的声源的方位对应的参会者身份，并将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0144]	进一步的，在将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中之后，第二处理器064还可以用于：</p>
    <p>[0145]	将音频码流的时间信息写入音频码流的附加域信息中，以便当接收到的音频码流为至少两个时，将至少两个音频码流按照时间信息进行排序。</p>
    <p>[0146]	再进一步的，第二存储器061可以具体用于将在第一单位时间内的音频码流所解码出的音频数据存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件，并通过第二发射机063将码流文件中的音频数据发送至声纹识别系统；</p>
    <p>[0147]	第二接收机062可以具体用于从声纹识别系统接收声纹识别结果，声纹识别结果包括在第一单位时间内的音频数据所对应的声源的方位对应的参会者身份，并将第一单位时间内的音频数据所对应的声源的方位对应的参会者身份写入第一单位时间内的音频数据的附加域信息中。</p>
    <p>[0148]	再进一步的，在从声纹识别系统接收声纹识别结果之前，可以通过第二发射机063将码流文件中的第二单位时间内的音频数据的声纹识别结果发送至声纹识别系统，第二单位时间为第一单位时间的上一个单位时间，以便在声纹识别系统对第一单位时间内的音频数据进行声纹识别时，将第二单位时间内的音频数据的声纹识别结果作为参考。</p>
    <p>[0149]	再进一步的，在将码流文件中的音频数据发送至声纹识别系统之前，第二处理器064还可以用于：</p>
    <p>[0150]	检测第一单位时间内的音频数据的附加域信息，若第一单位时间内的音频数据的所有附加域信息中的语音激活标志都为未激活，则不将第一单位时间内的音频数据发送至声纹识别系统。</p>
    <p>[0151]	更进一步的，在将码流文件中的音频数据发送至声纹识别系统之前，第二处理器 064还可以用于：</p>
    <p>[0152]	检测音频码流的附加域信息中的声源方位；</p>
    <p>[0153]	若音频码流的附加域信息中音频码流对应的声源方位只有一个，则将码流文件发送至声纹识别系统；</p>
    <p>[0154]	若音频码流的附加域信息包括的音频码流对应的声源方位至少有两个，且至少两个声源方位对应的参会者身份已经在上一次进行声纹识别时识别出来，则通过第二发射机063将已识别出来的两个声源方位对应的参会者身份写入音频码流的附加域信息中。</p>
    <p>[0155]	本发明实施例提供一种录播服务器，通过从多点控制单元接收会场的音频码流和音频码流的附加域信息，音频码流的附加域信息包括音频码流所对应的声源的方位，将音频码流所解码出的音频数据，存储到与音频码流所属的会场号以及音频码流所对应的声源的方位对应的码流文件中，并将码流文件发送至声纹识别系统，从声纹识别系统接收声纹识别结果，再将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0156]	本发明又一实施例提供一种视频会议系统1，包括多点控制单元03和声纹识别系统04，如图10所示，还包括：前述实施例提供的视频设备01和录播服务器02，或视频设备05和录播服务器06。</p>
    <p>[0157]	本发明实施例提供一种视频会议系统，通过根据会场的音频数据进行声源定位，以获取音频数据所对应的声源的方位，并将声源的方位写入音频数据的附加域信息，再将语音激活标志写入附加域信息，其中语音激活标志包括已激活或未激活，而后将音频数据打包成音频码流，并将音频码流和音频码流的附加域信息发送至录播服务器，以使得录播服务器根据附加域信息对音频数据进行分类，将音频数据所对应的声源的方位对应的参会者身份写入音频码流的附加域信息中，能够将会场中的语音数据按照说话人的方位分别进行声纹识别，提高了声纹识别准确率，进而提高了语音会议纪要分类的可靠性。</p>
    <p>[0158]	在本申请所提供的几个实施例中，应该理解到，所揭露方法、设备和系统，可以通过其它的方式实现。例如，以上所描述的设备实施例仅仅是示意性的，例如，单元的划分，仅仅为一种逻辑功能划分，实际实现时可以有另外的划分方式，例如多个单元或组件可以结合或者可以集成到另一个系统，或一些特征可以忽略，或不执行。另一点，所显示或讨论的相互之间的耦合或直接耦合或通信连接可以是通过一些接口，装置或单元的间接耦合或通信连接，可以是电性，机械或其它的形式。</p>
    <p>[0159]	另外，在本发明各个实施例中，各功能单元可以集成在一个处理单元中，也可以是各个单元单独物理包括，也可以两个或两个以上单元集成在一个单元中。且上述的各单元既可以采用硬件的形式实现，也可以采用硬件加软件功能单元的形式实现。</p>
    <p>[0160]	实现上述方法实施例的全部或部分步骤可以通过程序指令相关的硬件来完成，前述的程序可以存储于一计算机可读取存储介质中，该程序在执行时，执行包括上述方法实施例的步骤；而前述的存储介质包括：U盘、移动硬盘、只读存储器（Read Only Memory,简称ROM)、随机存取存储器（Random Access Memory，简称RAM)、磁碟或者光盘等各种可以存储程序代码的介质。</p>
    <p>[0161]	以上所述，仅为本发明的具体实施方式，但本发明的保护范围并不局限于此，任何熟悉本技术领域的技术人员在本发明揭露的技术范围内，可轻易想到变化或替换，都应涵 盖在本发明的保护范围之内。因此，本发明的保护范围应以所述权利要求的保护范围为准。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1479525B?cl=zh">CN1479525B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2003年6月27日</td><td class="patent-data-table-td patent-date-value">2010年5月12日</td><td class="patent-data-table-td ">微软公司</td><td class="patent-data-table-td ">捕获音视频数据的系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101398475A?cl=zh">CN101398475A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2008年9月26日</td><td class="patent-data-table-td patent-date-value">2009年4月1日</td><td class="patent-data-table-td ">索尼株式会社</td><td class="patent-data-table-td ">声源方向检测装置和方法以及声源方向检测相机</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102436812A?cl=zh">CN102436812A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月1日</td><td class="patent-data-table-td patent-date-value">2012年5月2日</td><td class="patent-data-table-td ">展讯通信（上海）有限公司</td><td class="patent-data-table-td ">会议记录装置及利用该装置对会议进行记录的方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102625077A?cl=zh">CN102625077A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年1月27日</td><td class="patent-data-table-td patent-date-value">2012年8月1日</td><td class="patent-data-table-td ">深圳市合智创盈电子有限公司</td><td class="patent-data-table-td ">一种会议记录方法、会议摄像装置、客户机及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3D2004023661A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEYM1ascI5OwEC2ka8Tf5vxkj1wpg">JP2004023661A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">没有名称</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8838447">US8838447</a></td><td class="patent-data-table-td patent-date-value">2013年11月29日</td><td class="patent-data-table-td patent-date-value">2014年9月16日</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd.</td><td class="patent-data-table-td ">Method for classifying voice conference minutes, device, and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014082445A1?cl=zh">WO2014082445A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年7月1日</td><td class="patent-data-table-td patent-date-value">2014年6月5日</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd.</td><td class="patent-data-table-td ">一种语音会议纪要的分类方法、设备和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014173370A1?cl=zh">WO2014173370A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2014年6月12日</td><td class="patent-data-table-td patent-date-value">2014年10月30日</td><td class="patent-data-table-td ">Zte Corporation</td><td class="patent-data-table-td ">会议纪要的提取方法及装置</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G10L0017140000">G10L17/14</a></span></td></tr><tr><td class="patent-data-table-td "> 合作分类</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/56">H04M3/56</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2203/301">H04M2203/301</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2242/30">H04M2242/30</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2201/41">H04M2201/41</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=NDzXBwABERAJ&amp;q=http://worldwide.espacenet.com/classification&amp;usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/42221">H04M3/42221</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2013年3月13日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年4月10日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2015年1月21日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/921d5a068882b8e2ede2/CN102968991A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_5115ea495017d9115e613207d3810e5a.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E8%AF%AD%E9%9F%B3%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81%E7%9A%84%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95.pdf?id=NDzXBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U09PYTQNL2rKSD6fNPfOtKjVK3itQ"},"sample_url":"https://www.google.com/patents/reader?id=NDzXBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>