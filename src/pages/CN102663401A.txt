<!DOCTYPE html><html><head><title>专利 CN102663401A - 一种图像特征提取和描述方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种图像特征提取和描述方法"><meta name="DC.contributor" content="王立国" scheme="inventor"><meta name="DC.contributor" content="王莹" scheme="inventor"><meta name="DC.contributor" content="赵春晖" scheme="inventor"><meta name="DC.contributor" content="齐滨" scheme="inventor"><meta name="DC.contributor" content="哈尔滨工程大学" scheme="assignee"><meta name="DC.date" content="2012-4-18" scheme="dateSubmitted"><meta name="DC.description" content="本发明涉及图像处理与计算机视觉领域，具体提供了一种适用于BoW(Bag of Words)模型应用在计算机视觉领域的图像特征提取与描述方法。本发明包括：对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换为HSV模型；选取尺度参数；采用均匀采样方法，按选取的尺度参数，以相同像素间隔对图像的特征点进行提取，计算图像H通道、S通道、V通道的DF-SIFT描述子，将颜色信息应用到分类任务中，采样密度由参数步长进行控制，得到图像的密集特征；对密集特征进行描述。本发明通过密集采样，使视觉词典更加准确可靠，利用双线性插值代替图像与高斯核函数卷积过程，使实现过程变得更简单、高效。"><meta name="DC.date" content="2012-9-12"><meta name="DC.relation" content="CN:101853299:A" scheme="references"><meta name="DC.relation" content="CN:102184411:A" scheme="references"><meta name="citation_patent_publication_number" content="CN:102663401:A"><meta name="citation_patent_application_number" content="CN:201210114061"><link rel="canonical" href="https://www.google.com/patents/CN102663401A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN102663401A?cl=zh"/><meta name="title" content="专利 CN102663401A - 一种图像特征提取和描述方法"/><meta name="description" content="本发明涉及图像处理与计算机视觉领域，具体提供了一种适用于BoW(Bag of Words)模型应用在计算机视觉领域的图像特征提取与描述方法。本发明包括：对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换为HSV模型；选取尺度参数；采用均匀采样方法，按选取的尺度参数，以相同像素间隔对图像的特征点进行提取，计算图像H通道、S通道、V通道的DF-SIFT描述子，将颜色信息应用到分类任务中，采样密度由参数步长进行控制，得到图像的密集特征；对密集特征进行描述。本发明通过密集采样，使视觉词典更加准确可靠，利用双线性插值代替图像与高斯核函数卷积过程，使实现过程变得更简单、高效。"/><meta property="og:title" content="专利 CN102663401A - 一种图像特征提取和描述方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN102663401A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN102663401A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=Vs6gBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN102663401A&amp;usg=AFQjCNFubCxcbYuueizcA0V-qvrVgQl9Uw" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/5076b16a5def2c41feed/CN102663401A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/5076b16a5def2c41feed/CN102663401A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN102663401A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN102663401A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN102663401A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN102663401B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN102663401A?cl=zh" style="display:none"><span itemprop="description">本发明涉及图像处理与计算机视觉领域，具体提供了一种适用于BoW(Bag of Words)模型应用在计算机视觉领域的图像特征提取与描述方法。本发明包括：对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换 ...</span><span itemprop="url">https://www.google.com/patents/CN102663401A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN102663401A - 一种图像特征提取和描述方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN102663401A - 一种图像特征提取和描述方法" title="专利 CN102663401A - 一种图像特征提取和描述方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN102663401 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 201210114061</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2012年9月12日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2012年4月18日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2012年4月18日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102663401B?hl=zh-CN&amp;cl=zh">CN102663401B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">201210114061.1, </span><span class="patent-bibdata-value">CN 102663401 A, </span><span class="patent-bibdata-value">CN 102663401A, </span><span class="patent-bibdata-value">CN 201210114061, </span><span class="patent-bibdata-value">CN-A-102663401, </span><span class="patent-bibdata-value">CN102663401 A, </span><span class="patent-bibdata-value">CN102663401A, </span><span class="patent-bibdata-value">CN201210114061, </span><span class="patent-bibdata-value">CN201210114061.1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E7%AB%8B%E5%9B%BD%22">王立国</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E7%8E%8B%E8%8E%B9%22">王莹</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%B5%B5%E6%98%A5%E6%99%96%22">赵春晖</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%BD%90%E6%BB%A8%22">齐滨</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%22">哈尔滨工程大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN102663401A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102663401A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN102663401A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (2),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (2),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=Vs6gBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D201210114061&amp;usg=AFQjCNF2xpNyzGmYYzm9-gNmNAQ-bjF5tA"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=Vs6gBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D102663401A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHxabRfX3oZVbNmPi2iTJL2oEW_yQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT115272809" lang="ZH" load-source="patent-office">一种图像特征提取和描述方法</invention-title>
      </span><br><span class="patent-number">CN 102663401 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA100184346" lang="ZH" load-source="patent-office">
    <div class="abstract">本发明涉及图像处理与计算机视觉领域，具体提供了一种适用于BoW(Bag&#160;of&#160;Words)模型应用在计算机视觉领域的图像特征提取与描述方法。本发明包括：对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换为HSV模型；选取尺度参数；采用均匀采样方法，按选取的尺度参数，以相同像素间隔对图像的特征点进行提取，计算图像H通道、S通道、V通道的DF-SIFT描述子，将颜色信息应用到分类任务中，采样密度由参数步长进行控制，得到图像的密集特征；对密集特征进行描述。本发明通过密集采样，使视觉词典更加准确可靠，利用双线性插值代替图像与高斯核函数卷积过程，使实现过程变得更简单、高效。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(9)</span></span></div><div class="patent-text"><div mxw-id="PCLM45094041" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1.	一种图像特征提取和描述方法，其特征是包括：  (1)对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换为HSV模型；  (2)选取尺度参数；  (3)采用均匀采样方法，按选取的尺度参数，以相同像素间隔对图像的特征点进行提取，计算图像H通道、S通道、V通道的DF-SIFT描述子，将颜色信息应用到分类任务中，采样密度由参数步长进行控制，得到图像的密集特征；  (4)对密集特征进行描述。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2.根据权利要求I所述的一种图像特征提取和描述方法，其特征是：所述图像H通道、S通道、V通道的DF-SIFT描述子的模型为： <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AC00021.png"> <img id="icf0001" file="CN102663401AC00021.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AC00021.png" class="patent-full-image" alt="Figure CN102663401AC00021"> </a> </div>  其中h代表色度，s代表饱和度、v代表亮度，max表示r，g，b三个分量中的最大值，min表示r, g, b三个分量中的最小值。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3.根据权利要求I或2所述的一种图像特征提取和描述方法，其特征是：所述对密集特征进行描述，包括：  (1)将特征调整至0°	；  (2)以特征点为圆心，以统一尺度为半径构造圆形区域，将落在该圆形区域的像素点分成4X4个不重叠的子区域；   (3)在每个子区域计算0°、45。、90。、135。、180。、225。、270。、315。八个方向的梯度值；  (4)对每个子区域进行均匀加权，应用高斯函数的均值对每个子区域进行加权，应用双线性插值完成对梯度的累积，每个特征由一个4*4*8的128维的向量进行描述。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4.根据权利要求I或2所述的一种图像特征提取和描述方法，其特征是：对密集特征进行描述后，统计不同步长参数的准确率结果，得到最优步长参数，以最优步长参数重新提取图像的密集特征并进行描述。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5.根据权利要求3所述的一种图像特征提取和描述方法，其特征是：对密集特征进行描述后，统计不同步长参数的准确率结果，得到最优步长参数，以最优步长参数重新提取图像的密集特征并进行描述。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6.根据权利要求I或2所述的一种图像特征提取和描述方法，其特征是：所述尺度参数分别为4、6、8、10。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7.根据权利要求3所述的一种图像特征提取和描述方法，其特征是：所述尺度参数分别为 4、6、8、10。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8.根据权利要求4所述的一种图像特征提取和描述方法，其特征是：所述尺度参数分别为 4、6、8、10。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9.根据权利要求5所述的一种图像特征提取和描述方法，其特征是：所述尺度参数分别为 4、6、8、10。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES51465266" lang="ZH" load-source="patent-office" class="description">
    <p>一种图像特征提取和描述方法</p>
    <p>技术领域</p>
    <p>[0001]	本发明涉及图像处理与计算机视觉领域，具体提供了一种适用于BoW(Bag ofWords)模型应用在计算机视觉领域的图像特征提取与描述方法。</p>
    <p>背景技术</p>
    <p>[0002]	图像分类作为图像处理的基础应用，长期以来受到各国专家学者以及工程技术人员的广泛关注。而BoW模型最初应用于文档处理领域，将文档表示成顺序无关的关键词的组合，通过统计文档中关键词出现的频率来进行匹配。近几年来，计算机视觉领域的研究者们成功地将该模型的思想移植到图像处理领域，通过对图像进行特征提取和描述，得到大量特征进行处理，从而得到用来表示图像的单词，并在此基础上构建视觉词典，然后对待分类图像采用相同的处理方法，将结果代入到训练的分类器中进行分类。该模型中最关 键的步骤即为特征的提取与描述，传统方法中采用尺度无关特征变换（Scale-invariantFeature Transform, SIFT)方法应用于BoW模型,然而SIFT描述子只是针对图像的稳定特征点进行提取和描述，因此其必然存在信息丢失和遗漏的问题。在对图像应用SIFT描述子时，需要求图像具有较为标准的形式，例如图像尺寸足够大，关键物体所占比例足够大，这样才能保证有足够的特征点被提取后用到后续匹配。在特征点进行提取和描述的过程中，复杂度很高，需要消耗大量的计算时间，这也是对图像识别和分类任务不利的一面。而在BoW模型中，在进行特征提取环节之后，要应用聚类方法来生成视觉单词，因此如果在特征提取环节不能提供足够丰富的信息，则会直接影响生成的视觉单词的代表性，进而影响后续的分类准确度。因此，研究者们一直致力于对SIFT描述子进行改进或者用新的描述子来代替SIFT描述子应用于BoW模型中。例如采用PCA-SIFT描述子，通过一个正交矩阵变换把原数据变换到一个新的坐标系统中，从而实现高维数据向低维数据的转换，降低计算的复杂度。此外，还有加速鲁棒特征（Speeded Up Robust of Features, SURF)描述子,主要针对SIFT描述子进行了改进，其效率更高，鲁棒性更强。</p>
    <p>发明内容</p>
    <p>[0003]	本发明的目的在于提供一种在应用于BoW模型时更高效的DF-SIFT (DenseFast-SIFT)图像特征提取与描述方法。</p>
    <p>[0004]	本发明的目的是这样实现的：</p>
    <p>[0005]	本发明的一种图像特征提取和描述方法，包括：</p>
    <p>[0006]	(I)对输入图像进行格式判断，若是灰度图像则不作处理，若不是灰度图像，则转换为HSV模型；</p>
    <p>[0007]	(2)选取尺度参数；</p>
    <p>[0008]	(3)采用均匀采样方法，按选取的尺度参数，以相同像素间隔对图像的特征点进行提取，计算图像H通道、S通道、V通道的DF-SIFT描述子，将颜色信息应用到分类任务中，采样密度由参数步长进行控制，得到图像的密集特征；[0009]	(4)对密集特征进行描述。</p>
    <p>[0010]	图像H通道、S通道、V通道的DF-SIFT描述子的模型为：</p>
    <p>[0011]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00051.png"> <img id="idf0001" file="CN102663401AD00051.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00051.png" class="patent-full-image" alt="Figure CN102663401AD00051"> </a> </div>
    <p>[0014]	其中h代表色度，s代表饱和度、V代表亮度，max表示r，g，b三个分量中的最大值，min表示r，g，b三个分量中的最小值。</p>
    <p>[0015]	对密集特征进行描述，包括：</p>
    <p>[0016]	⑴将特征调整至O ° ；</p>
    <p>[0017]	(2)以特征点为圆心，以统一尺度为半径构造圆形区域，将落在该圆形区域的像素点分成4X4个不重叠的子区域；</p>
    <p>[0018]	(3)在每个子区域计算 0°、45。、90。,135° ,180° ,225° ,270° ,315° 八个方向的梯度值；</p>
    <p>[0019]	(4)对每个子区域进行均匀加权，应用高斯函数的均值对每个子区域进行加权，应用双线性插值完成对梯度的累积，每个特征由一个4*4*8的128维的向量进行描述。</p>
    <p>[0020]	对密集特征进行描述后，统计不同步长参数的准确率结果，得到最优步长参数，以最优步长参数重新提取图像的密集特征并进行描述。</p>
    <p>[0021]	尺度参数分别为4、6、8、10。</p>
    <p>[0022]	本发明的有益效果在于：</p>
    <p>[0023]	本发明所述的图像特征提取与描述方法采用简化的特征点提取方法，通过密集采样得到了充分表征图像信息的特征，使应用聚类方法进行聚类而生成的视觉词典更加准确可靠，采用多尺度描述方法保证了图像特征的尺度不变性。颜色信息的引入使得图像信息的利用更加完整，从而为后续的分类以及识别环节提供了更全面而准确的特征信息。 [0024]	用矩形窗代替传统SIFT描述子中的高斯窗对图像进行平滑，利用双线性插值代  替图像与高斯核函数卷积的过程，简化了实现过程。对特征进行统一的多尺度分配，避免了复杂的尺度计算过程。最优步长参数的选取使本发明在保证了准确性的基础上提高了效率。附图说明</p>
    <p>[0025]	图I是本发明流程示意图；</p>
    <p>[0026]	图2是应用DF-SIFT描述子进行特征描述时得到的特征结果示意图；</p>
    <p>[0027]	图3是不同步长参数设置下应用DF-SIFT描述子于BoW模型中进行图像分类时所得到的分类正确率统计结果示意图；</p>
    <p>[0028]	图4是SIFT与DF-SIFT描述子应用于BoW模型时的单个类别图像分类正确率比较结果示意图。</p>
    <p>具体实施方式</p>
    <p>[0029]	本发明的目的在于将原本应用于文本处理领域的BoW模型应用于图像分类领域 时，能够通过应用DF-SIFT描述子，得到精确描述图像信息的特征，且适用于后续的构建词典以及SVM分类过程，从而克服现有图像特征提取与描述方法的复杂度高及分类结果欠佳的问题。在应用BoW模型进行图像表示时，其比较关键的步骤为对图像进行特征的提取和描述，并且需要大量丰富的特征来保证图像的信息得到完整的描述。因此，本发明提出的DF-SIFT描述子采用均匀采样的方法，逐像素进行特征点的提取，从而得到密集的图像特征，采样的密度由参数“步长”来控制。然而，这并不意味着特征点越多越好，因为特征点的增多会给后续的聚类步骤带来复杂的计算负担，因此本发明通过大量的随机实验进行了最优参数的选取。颜色信息是表征图像内容的重要信息，DF-SIFT描述子采用了自动HSV选择模型，即首先对图像表示模型进行判断，对灰度图像外的图像进行模型转换，在H、S、V三个通道上分别计算特征。</p>
    <p>[0030]	在完成特征提取对特征进行描述时，SIFT采用的是利用高斯窗函数进行梯度的加权累积，而在DF-SIFT中，将高斯窗函数用矩形窗来代替，对特征点的邻域进行均匀加权，而不是高斯加权，这样仅仅应用双线性插值来代替与高斯函数的卷积便可以完成对梯度的累积，之后应用高斯函数的均值对其所在单元进行统一加权。这种近似方法既提高了速度，又保证了性能不受损失。由于DF-SIFT采用的是均匀抽取关键点的方法，因此其特征的尺度不变性性能会受到一定的破坏，为了保证尺度不变性，我们采用多尺度提取的方法，对每一个关键点用多个不同的尺度进行提取和描述。大尺度对应于图像的概貌特征，小尺度对应于图像的细节特征，这样得到的特征同样能保证尺度的不变性。</p>
    <p>[0031]	下面结合附图举例对本发明做更详细地描述：</p>
    <p>[0032]	I.从Caltech 101与Caltech 256两个数据库中分别随机选取10类图像，再从每一类中随机抽取相应数目的图像进行训练。首先对每个训练图像进行格式判断，如果不是灰度图像，则转换为HSV模型，如果是灰度图像，则跳出，直接进行特征提取处理；</p>
    <p>[0033]	2.对图像进行均匀采样，逐相同像素间隔进行特征点的提取，首先在尺度为4的情况下进行，这样便得到了一系列尺度为4的特征区域；</p>
    <p>[0034]	3.对图像进行多尺度再次提取。这里，将尺度设定为6，8，10。至此便得到了大量的多尺度图像特征；</p>
    <p>[0035]	4.对每个特征进行描述。以特征点为圆心，尺度为半径构造圆形区域，将该区域分为4*4的子区域，在每个子区域里计算其8个方向（O。，45° ,90° ,135° ,180° ,225°，270° ,315° )的梯度累加值。计算过程中用矩形窗对子区域进行平滑，这样用双线性插值就可以完成梯度的累积，最后应用高斯函数的均值对各个子区域进行加权。这样，每一个特征都可以用一个4*4*8 = 128维的向量来进行描述。</p>
    <p>[0036]	5.对特征进行提取和描述之后，采用k-means聚类方法对得到的大量特征进行聚类，将聚类中心作为视觉单词；</p>
    <p>[0037]	6.将步骤5得到的视觉单词进行整合，构造视觉词典，每一类图像用基于视觉词典的直方图进行表示；</p>
    <p>[0038]	7.对待分类图像重复1-4的过程，得到待分类图像的特征提取与描述结果。然后通过计算每个特征向量与视觉单词库中的视觉单词的距离，判断其归属于哪一个视觉单词，并将图像表示成基于视觉词典的直方图； [0039]	8.将上述视觉词典直方图输入到SVM分类器中进行分类；</p>
    <p>[0040]	9.为保证方法的效率，进行大量的随机实验对不同步长设置下所得到的分类准确率结果进行统计，通过结果找到最优步长设置的参数值。</p>
    <p>[0041]	参照图2，图I表示的是应用DF-SIFT提取与描述方法多得到的特征示意图，从该图可以看出，应用DF-SIFT描述子可以得到的密集的多尺度特征。</p>
    <p>[0042]	参照图3，旨在对DF-SIFT描述子进行最优参数选取。从Caltech 101和Caltech256两个数据库中随机抽取10类进行实验，并从中随机抽取20张用来训练，再随机抽取20张用来测试。步长的选择从2到20。假设在用来测试的20幅图像中，正确分类的有N幅图像，则显然，分类正确率为N/20。从图中可以看出，对于Caltech 101和Caltech 256两个数据库来讲，分类准确率在步长小于等于8时几乎保持不变，对Caltech 101的分类准确率为91%左右，而对Caltech 256的分类准确率为55. 5%左右。可以看出，对于Caltech 256库的分类正确率较低，这是因为该库的图像具有较大的变化特性，因此分类难度更高。但这并不能否定DF-SIFT的性能，因为在与传统的SIFT算子进行比较时，其结果仍然具有较大的提高，当步长大于8时，分类的正确率开始下降。因此将参数“步长”设为8，这样在保证效率的基础上提高了分类的准确率。</p>
    <p>[0043]表	I</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00071.png"> <img id="idf0002" file="CN102663401AD00071.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00071.png" class="patent-full-image" alt="Figure CN102663401AD00071"> </a> </div>
    <p>[0045]	表I是SIFT与DF-SIFT描述子在应用于BoW模型时的图像分类正确率比较结果，旨在对在BoW模型中应用DF-SIFT描述子与应用SIFT描述子的分类正确率结果进行比较。同样选用Caltech 101和Caltech 256两个数据库进行验证，来保证实验结果的说服性。从数据库中随机抽取10类，将训练图像数目分别设为5，10，15，20，25，30。再随机抽取20张图像作为测试。实验结果为20次实验的统计平均。应用DF-SIFT描述子所得到的分类正确率要远远好于SIFT描述子的结果，这就验证了本发明提出的方法的有效性。</p>
    <p>[0046]	参照图4，旨在进一步验证算法的性能，对单次实验10类图像的每类分类结果进行了跟踪统计，这 10 类图像包括：Faces, Faces_easy, Leopards, Motorbikes, Airplanes,Bonsai, Brain, Buddha, Butterfly, car_side。实验结果为20次实验的统计平均。图4从左到右依次为训练图为5，10，15，20，25，30时各个具体类别在应用SIFT以及DF-SIFT描述子时的分类准确率比较，其中横轴1-10分别依次代表上述的10类对象，纵轴代表分类的正确率，条形图中，每对条形图形的左侧条形代表基于SIFT描述子的每类图像的分类正确率，右侧条形代表基于DF-SIFT描述子的每类图像的分类正确率，可以看出，右侧条形图的值要高于左侧的条形图。结合上述的实验结果，我们可以得出结论，无论是对于整体的平均分类正确率，还是对于具体类别的分类正确率，DF-S IFT的性能都要好于SIFT。</p>
    <p>[0047]表	2</p>
    <p>     [0048]</p>
    <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00081.png"> <img id="idf0003" file="CN102663401AD00081.tif" img-content="drawing" img-format="tif" src="//patentimages.storage.googleapis.com/CN102663401A/CN102663401AD00081.png" class="patent-full-image" alt="Figure CN102663401AD00081"> </a> </div>
    <p>[0049]	表2是SIFT与DF-SIFT描述子应用于BoW模型进行图像分类时的处理时间统计结果，旨在验证方法的复杂度。如果仅仅考虑特征抽取环节，DF-SIFT因为避免了大量的复杂运算，所以其抽取速度要明显快于SIFT描述子。但是将其应用到BoW模型中时，其密集的特征区域必然会给后续的聚类环节带来一定的数据负担。为了解决这一问题，我们对DF-SIFT进行了最优参数选取，根据实验结果将参数“步长”的值设为8。在实验时，我们统计了当步长的值为8时的DF-SIFT以及SIFT的算法运行时间，这里，运行时间包括训练时间和测试时间。从图中可以看出，经过参数优化的DF-SIFT描述子的运行时间要小于应用SIFT描述子的运行时间，这也验证了对DF-SIFT描述子进行参数选取的必要性和意义。</p>
    <p>[0050]	上述为本发明特举之实施例，并非用以限定本发明。本发明提供的DF-SIFT特征提取和描述方法同样也适用于其他的图像识别领域。在不脱离本发明的实质和范围内，可做些许的调整和优化，以本发明的保护范围以权利要求为准。</p>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用的专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102184411A?cl=zh">CN102184411A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年5月9日</td><td class="patent-data-table-td patent-date-value">2011年9月14日</td><td class="patent-data-table-td ">中国电子科技集团公司第二十八研究所</td><td class="patent-data-table-td ">基于彩色信息的尺度不变特征点描述和匹配方法</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103093226A?cl=zh">CN103093226A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月20日</td><td class="patent-data-table-td patent-date-value">2013年5月8日</td><td class="patent-data-table-td ">华南理工大学</td><td class="patent-data-table-td ">一种用于图像特征处理的ratmic描述子的构造方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103093226B?cl=zh">CN103093226B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月20日</td><td class="patent-data-table-td patent-date-value">2016年1月20日</td><td class="patent-data-table-td ">华南理工大学</td><td class="patent-data-table-td ">一种用于图像特征处理的ratmic描述子的构造方法</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=Vs6gBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06K0009460000">G06K9/46</a></span>, <span class="nested-value"><a href="https://www.google.com/url?id=Vs6gBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2012年9月12日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2012年11月7日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Entry into substantive examination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2013年11月20日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Grant of patent or utility model</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/5076b16a5def2c41feed/CN102663401A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%8F%8F%E8%BF%B0%E6%96%B9%E6%B3%95.pdf?id=Vs6gBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U1G0mpVsTypb0puOttkOUHKHCzeww"},"sample_url":"https://www.google.com/patents/reader?id=Vs6gBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>