<!DOCTYPE html><html><head><title>专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法 -  Google 专利</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_6e802a6b2b28d51711baddc2f3bec198/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_6e802a6b2b28d51711baddc2f3bec198__zh_cn.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "zh",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="一种个性化及协同化融合的网上多媒体检索与查询方法"><meta name="DC.contributor" content="朱信忠" scheme="inventor"><meta name="DC.contributor" content="赵建民" scheme="inventor"><meta name="DC.contributor" content="青 李" scheme="inventor"><meta name="DC.contributor" content="徐慧英" scheme="inventor"><meta name="DC.contributor" content="浙江师范大学" scheme="assignee"><meta name="DC.date" content="2008-7-18" scheme="dateSubmitted"><meta name="DC.description" content="一种个性化及协同化融合的网上多媒体检索与查询方法，包括以下步骤：(1)利用已有的语义信息，进行媒体对象语义的自动标注；(2)建立包含用户信息及个人喜好的用户侧档，检索系统按照用户意图对检索结果进行排序和优化；(3)根据用户相关反馈，动态调整用户侧档中各关键短语的权重，更准确体现用户意图；(4)建立用户侧档→群组侧档→社区侧档的多层侧档模式，层次间具有继承与共享机制，求同存异，支持海量存储；(5)对多模态信息融合分析进行多媒体语义理解，实现跨模态的多媒体对象检索。本发明能准确把握用户的意图，实现高精度、个性化、跨模态的多媒体检索。"><meta name="DC.date" content="2008-12-31"><meta name="citation_patent_publication_number" content="CN:101334796:A"><meta name="citation_patent_application_number" content="CN:200810137992"><link rel="canonical" href="https://www.google.com/patents/CN101334796A?cl=zh"/><meta property="og:url" content="https://www.google.com/patents/CN101334796A?cl=zh"/><meta name="title" content="专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法"/><meta name="description" content="一种个性化及协同化融合的网上多媒体检索与查询方法，包括以下步骤：(1)利用已有的语义信息，进行媒体对象语义的自动标注；(2)建立包含用户信息及个人喜好的用户侧档，检索系统按照用户意图对检索结果进行排序和优化；(3)根据用户相关反馈，动态调整用户侧档中各关键短语的权重，更准确体现用户意图；(4)建立用户侧档→群组侧档→社区侧档的多层侧档模式，层次间具有继承与共享机制，求同存异，支持海量存储；(5)对多模态信息融合分析进行多媒体语义理解，实现跨模态的多媒体对象检索。本发明能准确把握用户的意图，实现高精度、个性化、跨模态的多媒体检索。"/><meta property="og:title" content="专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}

#gbar { padding:.3em .6em !important;}</style></head><body ><div id=gbar><nobr><a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&sa=N&tab=tw">搜索</a> <a class=gb1 href="https://www.google.com/search?cl=zh&hl=zh-CN&tbm=isch&source=og&sa=N&tab=ti">图片</a> <a class=gb1 href="https://maps.google.com/maps?cl=zh&hl=zh-CN&sa=N&tab=tl">地图</a> <a class=gb1 href="https://play.google.com/?cl=zh&hl=zh-CN&sa=N&tab=t8">Play</a> <a class=gb1 href="https://www.youtube.com/results?cl=zh&hl=zh-CN&sa=N&tab=t1">YouTube</a> <a class=gb1 href="https://news.google.com/nwshp?hl=zh-CN&tab=tn">新闻</a> <a class=gb1 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a class=gb1 href="https://drive.google.com/?tab=to">云端硬盘</a> <a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/zh-CN/options/"><u>更多</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN&hl=zh-CN" class=gb4>登录</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="https://www.google.com/patents/CN101334796A?cl=zh&amp;hl=zh-CN&amp;output=html_text" title="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"><img border="0" src="//www.google.com/images/cleardot.gif"alt="屏幕阅读器用户请注意：点击此链接可进入无障碍模式。阅读器在无障碍模式下具有同样的基本功能，但可让用户获得更好的体验。"></a></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents?hl=zh-CN"> 专利</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/CN101334796A"></a><a id="appbar-patents-discuss-this-link" href="https://www.google.com/url?id=g51mBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpublication%3DCN101334796A&amp;usg=AFQjCNHtd9dJX8UXW7Cw0rR0BHtBxkjG1A" data-is-grant="false"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/e023ce8a5b502cd380e2/CN101334796A.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/e023ce8a5b502cd380e2/CN101334796A.pdf"></a><a class="appbar-content-language-link" data-selected="true" data-label="中文" href="/patents/CN101334796A?cl=zh&amp;hl=zh-CN"></a><a class="appbar-content-language-link" data-label="英语" href="/patents/CN101334796A?cl=en&amp;hl=zh-CN"></a><a class="appbar-application-grant-link" data-selected="true" data-label="申请" href="/patents/CN101334796A?hl=zh-CN&amp;cl=zh"></a><a class="appbar-application-grant-link" data-label="授权" href="/patents/CN101334796B?hl=zh-CN&amp;cl=zh"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="https://www.google.com/patents/CN101334796A?cl=zh" style="display:none"><span itemprop="description">一种个性化及协同化融合的网上多媒体检索与查询方法，包括以下步骤：(1)利用已有的语义信息，进行媒体对象语义的自动标注；(2)建立包含用户信息及个人喜好的用户侧档，检索系统按照用户意图对检索结果进行排序和优化；...</span><span itemprop="url">https://www.google.com/patents/CN101334796A?cl=zh&amp;utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法</span><img itemprop="image" src="https://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法" title="专利 CN101334796A - 一种个性化及协同化融合的网上多媒体检索与查询方法"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="https://www.google.com/advanced_patent_search?hl=zh-CN"> 高级专利搜索</a></li></ol></div><div id="volume-main"><div id="volume-center"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata patent-drawings-missing"><tr><td class="patent-bibdata-heading"> 公开号</td><td class="single-patent-bibdata">CN101334796 A</td></tr><tr><td class="patent-bibdata-heading">发布类型</td><td class="single-patent-bibdata">申请</td></tr><tr><td class="patent-bibdata-heading"> 专利申请号</td><td class="single-patent-bibdata">CN 200810137992</td></tr><tr><td class="patent-bibdata-heading">公开日</td><td class="single-patent-bibdata">2008年12月31日</td></tr><tr><td class="patent-bibdata-heading"> 申请日期</td><td class="single-patent-bibdata">2008年7月18日</td></tr><tr><td class="patent-bibdata-heading"> 优先权日<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="优先日期属于假设性质，不具任何法律效力。Google 对于所列日期的正确性并没有进行法律分析，也不作任何陈述。"></span></td><td class="single-patent-bibdata">2008年2月29日</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">公告号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101334796B?hl=zh-CN&amp;cl=zh">CN101334796B</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading"> 公开号</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">200810137992.7, </span><span class="patent-bibdata-value">CN 101334796 A, </span><span class="patent-bibdata-value">CN 101334796A, </span><span class="patent-bibdata-value">CN 200810137992, </span><span class="patent-bibdata-value">CN-A-101334796, </span><span class="patent-bibdata-value">CN101334796 A, </span><span class="patent-bibdata-value">CN101334796A, </span><span class="patent-bibdata-value">CN200810137992, </span><span class="patent-bibdata-value">CN200810137992.7</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 发明者</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E6%9C%B1%E4%BF%A1%E5%BF%A0%22">朱信忠</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E8%B5%B5%E5%BB%BA%E6%B0%91%22">赵建民</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E9%9D%92+%E6%9D%8E%22">青 李</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=ininventor:%22%E5%BE%90%E6%85%A7%E8%8B%B1%22">徐慧英</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading"> 申请人</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="https://www.google.com/search?tbo=p&amp;tbm=pts&amp;hl=en&amp;q=inassignee:%22%E6%B5%99%E6%B1%9F%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%22">浙江师范大学</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">导出引文</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101334796A.bibtex?cl=zh">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101334796A.enw?cl=zh">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101334796A.ris?cl=zh">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#forward-citations"> 被以下专利引用</a> (37),</span> <span class="patent-bibdata-value"><a href="#classifications">分类</a> (1),</span> <span class="patent-bibdata-value"><a href="#legal-events">法律事件</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">外部链接:&nbsp;</span><span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=g51mBwABERAJ&amp;q=http://211.157.104.87:8080/sipo/zljs/hyjs-yx-new.jsp%3Frecid%3D200810137992&amp;usg=AFQjCNEqWrukyLxUFJZpyqfEJdmrHLVnaw"> 中国国家知识产权局</a>, </span><span class="patent-bibdata-value"><a href="https://www.google.com/url?id=g51mBwABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DCN%26NR%3D101334796A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNFYei8vlcjV_jwoykl6ofHw9fi6BQ"> 欧洲专利数据库 (Espacenet)</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT71045235" lang="ZH" load-source="patent-office">一种个性化及协同化融合的网上多媒体检索与查询方法</invention-title>
      </span><br><span class="patent-number">CN 101334796 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title"> 摘要</span></div><div class="patent-text"><abstract mxw-id="PA109500285" lang="ZH" load-source="patent-office">
    <div class="abstract">一种个性化及协同化融合的网上多媒体检索与查询方法，包括以下步骤：(1)利用已有的语义信息，进行媒体对象语义的自动标注；(2)建立包含用户信息及个人喜好的用户侧档，检索系统按照用户意图对检索结果进行排序和优化；(3)根据用户相关反馈，动态调整用户侧档中各关键短语的权重，更准确体现用户意图；(4)建立用户侧档→群组侧档→社区侧档的多层侧档模式，层次间具有继承与共享机制，求同存异，支持海量存储；(5)对多模态信息融合分析进行多媒体语义理解，实现跨模态的多媒体对象检索。本发明能准确把握用户的意图，实现高精度、个性化、跨模态的多媒体检索。</div>
  </abstract>
  </div></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">权利要求<span class="patent-section-count">(6)</span></span></div><div class="patent-text"><div mxw-id="PCLM51289044" lang="ZH" load-source="patent-office" class="claims">
    <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1、一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：该方法包括以下步骤：</div>
      <div class="claim-text">(1)、对多媒体信息进行语义的自动标注：利用多媒体信息数据库的各种已有的高层语义，所述各种已有的高层语义包括文本语义标注、多媒体信息间的超链接说明、主题词、图像的主体名及其视觉特征描述词、Web页面内的多媒体信息间的关联描述，通过统计学习模型从中自动选取最能表达多媒体内容的若干关键字作为媒体信息的语义，并结合多媒体信息的底层特征相似度检索，进行关键字传播和多媒体语义的自动信息标注；</div>
      <div class="claim-text">(2)、建立用户侧档，其中包含用户的信息及个人喜好，按照用户的喜好程度，对检索结果进行优化排序，剔除用户不感兴趣的内容；</div>
      <div class="claim-text">用户侧档的基本结构定义如下：</div>
      <div class="claim-text">UP＝&lt;UInfo，P，UPL&gt;</div>
      <div class="claim-text">UInfo＝&lt;UID，UN，UD&gt;</div>
      <div class="claim-text">其中UPL表示用户感兴趣的关键短语的相关信息，P是指向用户所属群组的公共侧档的指针；UInfo表示用户信息，UID表示用户唯一标识符，UN表示用户名，UD表示用户其它描述信息；</div>
      <div class="claim-text">在用户使用过程中，按照用户搜索的结果进行聚类分析，确定用户最感兴趣的关键短语；</div>
      <div class="claim-text">(3)在每次检索结束后，用户对系统当前查询结果的满意程度进行反馈，系统接收用户的相关反馈意见，然后根据用户的反馈意见进行查询调整，动态调整用户侧档中各关键短语的权重，在下一次检索时能够按照新的关键短语的优先度对检索结果进行排序；</div>
      <div class="claim-text">(4)、用户选择属于某一群组，系统为该群组建立公共侧档来描述群组的共同行为和群组成员的普遍爱好；当一个用户新加入一个群组，从这个群组的公共侧档中继承属性；同样，群组侧档又能够从范围更大的社区侧档中继承属性；</div>
      <div class="claim-text">公共侧档的基本结构定义为：</div>
      <div class="claim-text">CP＝&lt;CInfo，WL，Suc&gt;</div>
      <div class="claim-text">CInfo＝&lt;GID，NAME，DE&gt;</div>
      <div class="claim-text">其中WL表示该公共侧档中用户的共同偏好，Suc表示该公共侧档的继承关系；CInfo表示该公共侧档的信息，GID表示本公共侧档唯一标识符，NAME表示公共侧档的名称，DE表示该公共侧档其它的描述信息；</div>
      <div class="claim-text">公共侧档的建立过程：在系统建立时，根据已有的经验知识，为不同的群组事先指定共同偏好；同时，公共侧档根据内部各成员的检索偏好及相关反馈的情况，动态调整预先制定的共同爱好；在公共侧档进行更新时，通过限制每个用户对特定关键字的投票次数，并结合公共侧档的用户副本在线更新模式；</div>
      <div class="claim-text">(5)、系统对多模态信息融合分析进行多媒体语义的理解，建立不同模态媒体对象之间的语义链，用户实现跨模态的多媒体信息查询，即用户提交任意模态的检索例子去检索任意模态的媒体对象或者多媒体文档。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2、如权利要求1所述的一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：所述的步骤(1)中，具体步骤如下：</div>
      <div class="claim-text">(1.1)提取多媒体数据库中的语义信息，包括文本描述、多媒体信息之间的超媒体链接说明，以及同一WEB页面内的图片、音频、视频、文本之间，以及同一站点内的多媒体信息之间的都存在上下文关联，并对关键字内容做出注释和说明；</div>
      <div class="claim-text">(1.2)用一个四元组MMEAN＝&lt;SID，ID，Keywords&gt;来描述每一个多媒体对象的语义，其中SID代表该媒体对象所属的分类，ID代表它在该分类中的唯一编号，Keywords＝{w1，w2，…，wi}代表按照步骤(1.1)得到的若干关键字；</div>
      <div class="claim-text">(1.3)采用“关键字传播”的手段，通过相似性检索来得到语义；具体步骤如下：</div>
      <div class="claim-text">(1.3.1)对各模态的多媒体对象提取底层特征并进行量化；</div>
      <div class="claim-text">(1.3.2)将无语义描述的多媒体对象与现有已经具有描述的同模态多媒体对象底层特征进行比较，将最相似的多媒体对象的语义描述作为自己的语义描述的一部分；并参考最相似的多媒体对象所在的多媒体文档中其他模态多媒体对象的语义描述，取所有这些描述中出现频率最高的若干关键字作为该多媒体对象的语义。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3、如权利要求1或2所述的一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：所述的步骤(2)中，用户侧档的建立以及通过学习进行更新，具体方法描述如下：</div>
      <div class="claim-text">(2.1)对搜索结果进行聚类，动态地得到一些搜索结果的关键短语；将提取出来的关键短语加入用户侧档，用来描述个人喜好的信息；</div>
      <div class="claim-text">(2.2)用下述形式来描述各关键短语及它在用户侧档中的关键性：</div>
      <div class="claim-text">UPL＝&lt;&lt;UW1，UPW1，UWE1&gt;，…，&lt;UWi，UPWi，UWEi&gt;&gt;&#160;&#160;&#160;&#160;(4)</div>
      <div class="claim-text">其中UWi表示用户检索时使用的短语，UPWi表示该短语所属类的标签，UWEi表示该短语的权重，权重越大则说明用户对该短语所代表的内容的兴趣越大；假设用户共进行了m次查询，且在某次查询时点击了结果中的n个多媒体对象，则权重UWEi的计算方法如下：</div>
      <div class="claim-text">上式中，Cik表示第i个短语在用户点击的第k个页面中出现的次数，</div>
      <div class="claim-text">
        <span class="patent-image-not-available"> </span>
      </div>
      <div class="claim-text">表示第i个短语在这n个页面中出现的总次数，而表示所有短语出现总次数的最大值；按照权重UWEi对用户检索时使用的关键短语进行排序，UWEi越大，则该关键短语可以理解为用户对相关内容的喜好程度更高；</div>
      <div class="claim-text">个性化的多媒体检索即指检索系统按照用户输入的检索条件得出检索结果，对搜索结果按照关键词的权重高低进行排序，优先显示权重更高的检索内容；</div>
      <div class="claim-text">(2.3)在用户信息UInfo的UD中进行约束并赋给它一个足够小的负数权重，使得检索系统不会再显示相关的内容；</div>
      <div class="claim-text">(2.4)用户侧档中关于关键短语的信息在下述情况需要进行更新：一是用户提交检索关键字进行检索，如果原先没有此关键字，则此时系统就将得到的关键字添加到用户侧档中，同时计算其相应的权重，如有，则只需重新计算权值；二是用户对检索结果做出评价时，系统需要根据用户的反馈调整各关键短语的权重。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4、如权利要求3所述的一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：所述的步骤(3)中，用户相关反馈模型，具体描述如下：</div>
      <div class="claim-text">(3.1)用户在提交一个查询后，返回的结果中，采用用户反馈机制，自动进行查询调整，反馈模型的定义如下：</div>
      <div class="claim-text">其中α、β、γ是适当的常数，Q是原检索点，Q′是经反馈修正后的检索点，DR、DN分别代表相关和不相关的媒体对象集，NR、NN分别代表DR、DN中所含媒体对象个数；</div>
      <div class="claim-text">(3.2)用户相关反馈机制设置为：系统以层次结构方式呈现给用户一个查询结果列表，用户可以对每个查询结果进行评价，评价分为正相关和负相关；现假设对检索结果Di进行评价，又设Di的关键短语集为(W1，W2，…，Wi)：</div>
      <div class="claim-text">(3.2.1)当评价为正相关时，对于某一关键短语Wi，如果用户侧档中没有Wi，则将其加入用户侧档中，其权值按用户侧档中介绍的权值计算法进行计算。若用户侧档中有Wi时，则UWEi更新公式如下：</div>
      <div class="claim-text">UWEi(now)表示按上述用户侧档中权值计算公式算出的当次查询的Wi的权值；</div>
      <div class="claim-text">(3.2.2)当评价为负相关时，如果用户侧档中没有关键短语Wi，则将其加入用户侧档中，权值计算如下：</div>
      <div class="claim-text">UWEi＝-tkUWEi(now)&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(8)</div>
      <div class="claim-text">当用户侧档中有关键短语Wi时，则UWEi更新公式如下：</div>
      <div class="claim-text">其中n为一个常值；</div>
      <div class="claim-text">(3.3)加入反馈机制后，调整侧档作用机制的过程叙述如下：</div>
      <div class="claim-text">用户对某查询结果的某一检索结果做出评价后，对于该结果的关键短语集(W1，W2，…，Wi)中的任一关键短语Wi，重新计算其权值，并更新用户侧档库；下一次的查询结果中有这个关键短语时，如果权值为正，则对查询结果按照权值由大到小排序；如果它的权值为负，则将所有负权值的关键短语按权值的绝对值排序由小到大排序，将绝对值大的关键短语剔除出结果集或将其排到后面。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5、如权利要求4述的一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：所述的步骤(4)中，公共侧档的建立、协同化以及安全保护，具体描述如下：</div>
      <div class="claim-text">(4.1)多层侧档模式即用户侧档→群组侧档→社区侧档的三层侧档模式，将群组侧档以及社区侧档统称为公共侧档，不同层次之间的侧档具有继承与被继承的关系，用Suc来表示；个人用户在首次使用多媒体检索系统时，按照自己的实际情况加入某个或某些群组中，由于群组侧档具有一些预先设定的公共喜好的关键短语，所以个人用户就继承这些关键短语作为初始化的个人喜好信息；同时，由于群组侧档也一样从范围更大的社区侧档中继承相应的关键短语，所以个人用户实际上也继承社区侧档的部分属性；对侧档中所含的关键短语的数量设置一个限制值，如超过这个限制值，则删去权重最小的关键短语；</div>
      <div class="claim-text">(4.2)公共侧档中同样存在各种关键短语，描述如下：</div>
      <div class="claim-text">WL＝&lt;&lt;W1，WE1&gt;，…，&lt;Wi，WEi&gt;&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(10)</div>
      <div class="claim-text">其中Wi表示词语或短语，WEi表示这个词语或短语的权重；WEi的计算方法如下所示：</div>
      <div class="claim-text">count(WEu(k)(Wi)＞F)k＝1n表示统计词Wi在各用户侧档中权值大于阀值F的次数；</div>
      <div class="claim-text">(4.3)由个人侧档中的UPL＝&lt;UW，UPW，UWE&gt;三元组中相同类UPW的所有关键短语UW的权值UWE相加后取平均值，再设一个阈值t，使所有平均值大于t的UPW进入公共侧档；</div>
      <div class="claim-text">(4.4)公共侧档的安全策略有两种：</div>
      <div class="claim-text">(4.4.1)对公共侧档媒体资料的描述符中，限制每位用户就特定关键字的投票次数，并对用户投票设有时间限定，超过时间限定后，用户针对同一关键短语和多媒体对象更改其投票；</div>
      <div class="claim-text">(4.4.2)、每位用户都存储有公共侧档的副本，原先公共侧档的升级转变为对副本更新，从而形成新版公共侧档，本地仅记录变化部分；在特定的时间段，系统会自动运行一个处理进程将所有本地公共侧档融合到一个中心公共侧档，每个本地版本都会被手动或自动程序检查后决定是否放入公共侧档，生成一个新的公共侧档，并且所有的本地版本都与它保持一致。</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6、如权利要求5所述的一种个性化及协同化融合的网上多媒体检索与查询方法，其特征在于：所述的步骤(5)中，基于多模态信息融合分析的跨媒体检索，包含以下步骤：</div>
      <div class="claim-text">(5.1)提取出各种模态媒体对象的底层特征，计算同种模态媒体所有对象间两两的距离，并将所有距离进行高斯归一化；</div>
      <div class="claim-text">(5.2)通过非线性方法对不同多媒体文档内的声音、视频、图像、文本等对象所携带的信息进行融合分析，求得步骤(5.1)中得到的各个距离的最大值maxdis和最小值mindis，定义多媒体文档之间的距离Dis如下：</div>
      <div class="claim-text">Dis＝λ×min&#160;dis+(α+ln(β×(max&#160;dis-min&#160;dis)+1))+A&#160;&#160;&#160;&#160;(12)</div>
      <div class="claim-text">其中α、β、λ和A是根据数据库大小和数据分布情况可调节的常数；</div>
      <div class="claim-text">(5.3)建立多媒体文档关联图，每个多媒体文档是该图上的一个顶点，任意两点间有一条边，边的权重即为步骤(5.2)中计算的距离，表示两个多媒体文档的相似关系；</div>
      <div class="claim-text">(5.4)重构多媒体文档关联图，首先设置一个阈值，将权重大于阈值的边的权全部设为无穷大；然后对所有的边，用两点间最短路径作为该边的新权重；</div>
      <div class="claim-text">(5.5)采用多向度量法将多媒体文档关联图投影到多媒体语义空间，所有多媒体文档都在该空间有唯一的坐标，所有多媒体文档内的媒体对象也都被该坐标所指向；</div>
      <div class="claim-text">(5.6)用户检索时，首先找到该媒体对象在多媒体语义空间的坐标，再计算与其他所有媒体对象的距离，并返回距离最近的目标模态的媒体对象。</div>
    </div>
  </div> </div>
  </div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title"> 说明</span></div><div class="patent-text"><div mxw-id="PDES58330950" lang="ZH" load-source="patent-office" class="description">
    <invention-title lang="ZH">一种个性化及协同化融合的网上多媒体检索与查询方法</invention-title>
    <technical-field>
      <p>[0001] 技术领域</p>
      <p>[0002] 本发明涉及一种基于用户侧档的网上多媒体的检索方法，尤其涉及一种基于多模态信息融合分析和交互检索的跨媒体检索方法。</p>
    </technical-field>
    <background-art>
      <p>[0003] 背景技术</p>
      <p>[0004] 目前Internet的信息量以每1.6年翻一倍的速度急剧增长。随着20世纪90年代多媒体技术的迅速发展，以及新的有效的多媒体编码技术的不断出现，大量的视频、音频和图像等多媒体信息成为了网络中不可或缺的重要资源。面对着日益增多的多媒体信息，使得对这些信息资源的存储、管理和利用变得非常困难。对于普通用户而言，他们需要面对的就是如何从浩瀚的信息海洋中准确快速地检索出自身所需的信息；而对于检索系统而言，就必须能够准确理解用户的意图，并按照用户的意图检索出用户最感兴趣的信息。</p>
      <p>[0005] 传统的数据类型主要是整型、实型、布尔型和字符型，因此其数据库技术可以采用基于关键字的检索方法。而在多媒体数据处理中，除了上述数据类型外，还包括图形、图像、声音、视频流等数据类型。因此，在基于关键字的检索系统中，网络开发人员必须事先对多媒体对象进行标注以利于用户的检索。但是这种模式明显存在着弊端：(1)、由于网络上多媒体信息的数量不断增多，数据量巨大，标注过程本身工作量浩繁，手工标注日益变得不切实际；(2)、标注本身存在着很大的主观性，针对同一个多媒体对象，不同的标注者完全可能有不同的理解，并标注不同的关键字，因此标注的关键字并不能完全准确、客观地反映多媒体对象所涵盖的语义，自然也不利于网络用户的检索了；(3)、无法体现检索的信息在视觉或听觉上的相似程度。</p>
      <p>[0006] 在这种情况下，基于内容的多媒体检索技术应运而生，并成为计算机视觉和信息检索领域的研究热点。上世纪90年代初期人们提出了基于内容的图像检索技术，从图像提取底层的视觉特征，如颜色、纹理、形状等底层特征作为图像的索引。这种技术思路后来也被运用到视频检索和音频检索中。基于内容的多媒体检索方法早期有以QBIC、VideoQ等为代表的原型系统，当时由于缺乏高层语义的支持，在准确率和效率上不能满足用户要求；之后例子学习、融合分析和流形学习等方法被用来实现多媒体语义理解，以填补多媒体信息底层特征和高层语义之间的鸿沟；接着为了克服训练样本的不足，又引入了相关反馈机制等。以上各种技术的应用，在一定程度上缩小了语义鸿沟，提高了网络多媒体检索的性能。</p>
      <p>[0007] 然而，现有的多媒体检索系统依然存在很多问题：(1)、传统的基于内容的多媒体检索系统通常通过提取色彩、形状、纹理等底层特征进行相似度比较，并根据相似度来建立与高层语义之间的联系和映射。然而单纯地对提取的多媒体底层特征进行相似度比较，在很多情况下并没有任何实质意义。例如当用户搜索“鸡肉”或“家禽肉类”等菜谱图片时，搜索引擎根据底层特征的相似度比较几乎无法准确区分出鸡肉、鸭肉和鹅肉甚至猪肉等的不同，更别说是烤鸭和烤鹅照片区别了，因它们之间颜色等底层特征的相似度很高。因此，利用这种方法进行检索的准确性较低；(2)、传统的多媒体检索系统不能很好地理解用户的真实意图，因此也无法准确地根据用户意图对检索结果进行优化和排序，即优先提供用户最感兴趣的内容；如上述肉类的菜谱图片检索过程中，某些种族用户有风俗忌讳(如伊斯兰穆斯林不吃猪肉、猛禽等)，要适当对结果进行过滤；某些人喜欢吃烧鸡，不喜欢吃烤鸡、烤鸭、酱鸭，则尽量将他所感兴趣的烧鸡菜谱图片靠前排序。(3)、传统的多媒体检索系统往往只能检索包含单一模态的多媒体数据库，或虽能检索多模态媒体数据，但不能支持跨媒体的检索，即根据一种模态的多媒体对象检索其它模态的多媒体对象；</p>
    </background-art>
    <disclosure>
      <p>[0008] 发明内容</p>
      <p>[0009] 为了克服已有的主流网络多媒体检索方法存在的基于内容的多媒体查询精确度上不去、基于相似度的检索往往结果对个性化用户没有意义、多媒体底层特征与高层语义之间存在鸿沟、检索准确率低下、不能准确理解用户意图并按照用户意图对检索结果进行优化和排序、不支持跨模态媒体检索等不足，本发明提供了一种个性化及协同化融合的网上多媒体检索与查询的方法，通过合理建立多媒体信息底层特征与高层语义之间的联系和映射，结合个性化的用户侧档与公共侧档，能够准确地理解用户的真实意图，并按照用户意图进行跨模态的多媒体网络检索，并对检索结果进行优化与排序，实现了用户检索的个性化，并提高了多媒体网络检索的精确度。</p>
      <p>[0010] 本发明解决其技术问题所采用的技术方案是：</p>
      <p>[0011] 一种个性化及协同化融合的网上多媒体检索与查询方法，该方法包括以下步骤：</p>
      <p>[0012] (1)、对多媒体信息进行语义的自动标注：利用多媒体信息数据库的各种已有的高层语义，所述各种已有的高层语义包括文本语义标注、多媒体信息间的超链接说明、主题词、图像的主体名及其视觉特征描述词、Web页面内的多媒体信息间的关联描述，通过统计学习模型从中自动选取最能表达多媒体内容的若干关键字作为媒体信息的语义，并结合多媒体信息的底层特征相似度检索，进行关键字传播和多媒体语义的自动信息标注；</p>
      <p>[0013] (2)、建立用户侧档，其中包含用户的信息及个人喜好，按照用户的喜好程度，对检索结果进行优化排序，剔除用户不感兴趣的内容；</p>
      <p>[0014] 用户侧档的基本结构定义如下：</p>
      <p>[0015] UP＝&lt;UInfo，P，UPL&gt;</p>
      <p>[0016] UInfo＝&lt;UID，UN，UD&gt;</p>
      <p>[0017] 其中UPL表示用户感兴趣的关键短语的相关信息，P是指向用户所属群组的公共侧档的指针；UInfo表示用户信息，UID表示用户唯一标识符，UN表示用户名，UD表示用户其它描述信息；</p>
      <p>[0018] 在用户使用过程中，按照用户搜索的结果进行聚类分析，确定用户最感兴趣的关键短语；</p>
      <p>[0019] (3)、在每次检索结束后，用户对系统当前查询结果的满意程度进行反馈，系统接收用户的相关反馈意见，然后根据用户的反馈意见进行查询调整，动态调整用户侧档中各关键短语的权重，在下一次检索时能够按照新的关键短语的优先度对检索结果进行排序；</p>
      <p>[0020] (4)、用户选择属于某一群组，系统为该群组建立公共侧档来描述群组的共同行为和群组成员的普遍爱好；当一个用户新加入一个群组，从这个群组的公共侧档中继承属性；同样，群组侧档又能够从范围更大的社区侧档中继承属性；</p>
      <p>[0021] 公共侧档的基本结构定义为：</p>
      <p>[0022] CP＝&lt;CInfo，WL，Suc&gt;</p>
      <p>[0023] CInfo＝&lt;GID，NAME，DE&gt;</p>
      <p>[0024] 其中WL表示该公共侧档中用户的共同偏好，Suc表示该公共侧档的继承关系；CInfo表示该公共侧档的信息，GID表示本公共侧档唯一标识符，NAME表示公共侧档的名称，DE表示该公共侧档其它的描述信息；</p>
      <p>[0025] 公共侧档的建立过程：在系统建立时，根据已有的经验知识，为不同的群组事先指定共同偏好；同时，公共侧档根据内部各成员的检索偏好及相关反馈的情况，动态调整预先制定的共同爱好；在公共侧档进行更新时，通过限制每个用户对特定关键字的投票次数，并结合公共侧档的用户副本在线更新模式；</p>
      <p>[0026] (5)、系统对多模态信息融合分析进行多媒体语义的理解，建立不同模态媒体对象之间的语义链，用户实现跨模态的多媒体信息查询，即用户提交任意模态的检索例子去检索任意模态的媒体对象或者多媒体文档。</p>
      <p>[0027] 作为优选的一种方案：所述的步骤(1)中，具体步骤如下：</p>
      <p>[0028] (1.1)提取各种已经存在的语义信息，包括文本描述、多媒体信息之间的超媒体链接说明，以及同一WEB页面内的图片、音频、视频、文本之间，以及同一站点内的多媒体信息之间的都存在的上下文关联，并对关键字内容做出注释和说明；</p>
      <p>[0029] (1.2)用一个四元组MMEAN＝&lt;SID，ID，Keywords&gt;来描述每一个多媒体对象的语义，其中SID代表该媒体对象所属的分类，ID代表它在该分类中的唯一编号，Keywords＝{w1，w2，…，wi}代表按照步骤(1.1)得到的若干关键字；</p>
      <p>[0030] (1.3)采用“关键字传播”的手段，通过相似性检索来得到语义；具体步骤如下：</p>
      <p>[0031] (1.3.1)对各模态的多媒体对象提取底层特征并进行量化；</p>
      <p>[0032] (1.3.2)将无语义描述的多媒体对象与现有已经具有描述的同模态多媒体对象底层特征进行比较，将最相似的多媒体对象的语义描述作为自己的语义描述的一部分；并参考最相似的多媒体对象所在的多媒体文档中其他模态多媒体对象的语义描述，取所有这些描述中出现频率最高的若干关键字作为该多媒体对象的语义。</p>
      <p>[0033] 作为优选的另一种方案：所述的步骤(2)中，用户侧档的建立以及通过学习进行更新，具体方法描述如下：</p>
      <p>[0034] (2.1)对搜索结果进行聚类，动态地得到一些搜索结果的关键短语；将提取出来的关键短语加入用户侧档，用来描述个人喜好的信息；</p>
      <p>[0035] (2.2)用下述形式来描述各关键短语及它在用户侧档中的关键性：</p>
      <p>[0036] UPL＝&lt;&lt;UW1，UPW1，UWE1&gt;，…，&lt;UWi，UPWi，UWEi&gt;&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(4)</p>
      <p>[0037] 其中UWi表示用户检索时使用的短语，UPWi表示该短语所属类的标签，UWEi表示该短语的权重，权重越大则说明用户对该短语所代表的内容的兴趣越大；假设用户共进行了m次查询，且在某次查询时点击了结果中的n个多媒体对象，则权重UWEi的计算方法如下：</p>
      <p>[0038] </p>
      <p>[0039] 上式中，Cik表示第i个短语在用户点击的第k个页面中出现的次数，</p>
      <p>
        <span class="patent-image-not-available"> </span>
      </p>
      <p>表示第i个短语在这n个页面中出现的总次数，而表示所有短语出现总次数的最大值；按照权重UWEi对用户检索时使用的关键短语进行排序，UWEi越大，则该关键短语可以理解为用户对相关内容的喜好程度更高；</p>
      <p>[0040] 个性化的多媒体检索即指检索系统按照用户输入的检索条件得出检索结果，对搜索结果按照关键词的权重高低进行排序，优先显示权重更高的检索内容；</p>
      <p>[0041] (2.3)在用户信息UInfo的UD中进行约束并赋给它一个足够小的负数权重，使得检索系统不会再显示相关的内容；</p>
      <p>[0042] (2.4)用户侧档中关于关键短语的信息在下述情况需要进行更新：一是用户提交检索关键字进行检索，如果原先没有此关键字，则此时系统就将得到的关键字添加到用户侧档中，同时计算其相应的权重，如有，则只需重新计算权值；二是用户对检索结果做出评价时，系统需要根据用户的反馈调整各关键短语的权重。</p>
      <p>[0043] 作为优选的在另一种方案：所述的步骤(3)中，用户相关反馈模型，具体描述如下：</p>
      <p>[0044] (3.1)用户在提交一个媒体查询(如图片查询)请求后，返回的结果中，采用用户反馈机制，自动进行查询调整，反馈模型的定义如下：</p>
      <p>[0045] </p>
      <p>[0046] 其中α、β、γ是适当的常数，Q是原检索点，Q′是经反馈修正后的检索点，DR、DN分别代表相关和不相关的媒体对象集，NR、NN分别代表DR、DN中所含媒体对象个数；</p>
      <p>[0047] (3.2)用户相关反馈机制设置为：系统以层次结构方式呈现给用户一个查询结果列表，用户可以对每个查询结果进行评价，评价分为正相关和负相关；现假设对检索结果Di进行评价，又设Di的关键短语集为(W1，W2，…，Wi)：</p>
      <p>[0048] (3.2.1)当评价为正相关时，对于某一关键短语Wi，如果用户侧档中没有Wi，则将其加入用户侧档中，其权值按用户侧档中介绍的权值计算法进行计算。若用户侧档中有Wi时，则UWEi更新公式如下：</p>
      <p>[0049] </p>
      <p>[0050] UWEi(now)表示按上述用户侧档中权值计算公式算出的当次查询的Wi的权值；</p>
      <p>[0051] (3.2.2)当评价为负相关时，如果用户侧档中没有关键短语Wi，则将其加入用户侧档中，权值计算如下：</p>
      <p>[0052] UWEi＝-tkUWEi(now)&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(8)</p>
      <p>[0053] 当用户侧档中有关键短语Wi时，则UWEi更新公式如下：</p>
      <p>[0054] </p>
      <p>[0055] 其中n为一个常值；</p>
      <p>[0056] (3.3)加入反馈机制后，调整侧档作用机制的过程叙述如下：</p>
      <p>[0057] 用户对某查询结果的某一检索结果作出评价后，对于该结果的关键短语集(W1，W2，…，Wi)中的任一关键短语Wi，重新计算其权值，并更新用户侧档库；下一次的查询结果中有这个关键短语时，如果权值为正，则对查询结果按照权值由大到小排序；如果它的权值为负，则将所有负权值的关键短语按权值的绝对值排序由小到大排序，将绝对值大的关键短语剔除出结果集或将其排到后面。</p>
      <p>[0058] 进一步，所述的步骤(4)中，公共侧档的建立、协同化以及安全保护，具体描述如下：</p>
      <p>[0059] (4.1)多层侧档模式即用户侧档→群组侧档→社区侧档的三层侧档模式，将群组侧档以及社区侧档统称为公共侧档，不同层次之间的侧档具有继承与被继承的关系，用Suc来表示；个人用户在首次使用多媒体检索系统时，按照自己的实际情况加入某个或某些群组中，由于群组侧档具有一些预先设定的公共喜好的关键短语，所以个人用户就继承这些关键短语作为初始化的缺省默认个人喜好信息；同时，由于群组侧档也一样从范围更大的社区侧档中继承相应的关键短语，所以个人用户实际上也继承社区侧档的部分属性；对侧档中所含的关键短语的数量设置一个限制值，如超过这个限制值，则删去权重最小的关键短语，提高搜索引擎的响应速度；</p>
      <p>[0060] (4.2)公共侧档中同样存在各种关键短语，描述如下：</p>
      <p>[0061] WL＝&lt;&lt;W1，WE1&gt;，…，&lt;Wi，WEi&gt;&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(10)</p>
      <p>[0062] 其中Wi表示词语或短语，WEi表示这个词语或短语的权重；WEi的计算方法如下所示：</p>
      <p>[0063] </p>
      <p>[0064] count(WEu(k)(Wi)＞F)k＝1n表示统计词Wi在各用户侧档中权值大于阀值F的次数；</p>
      <p>[0065] (4.3)由个人侧档中的UPL＝&lt;UW，UPW，UWE&gt;三元组中相同类UPW的所有关键短语UW的权值UWE相加后取平均值，再设一个阈值t，使所有平均值大于t的UPW进入公共侧档；</p>
      <p>[0066] (4.4)公共侧档的安全策略有两种：</p>
      <p>[0067] (4.4.1)对公共侧档媒体资料的描述符中，限制每位用户就特定关键字的投票次数，并对用户投票设有时间限定，超过时间限定后，用户针对同一关键短语和多媒体对象更改其投票；</p>
      <p>[0068] (4.4.2)、每位用户都存储有公共侧档的副本，原先公共侧档的升级转变为对副本的“在线(On-line)”更新，从而形成新版公共侧档，本地仅记录变化部分；在特定的时间段，系统会自动运行一个“脱机(Ofi-line)”处理进程将所有本地公共侧档融合到一个中心公共侧档，每个本地版本都会被手动或自动程序检查后决定是否放入公共侧档，生成一个新的公共侧档，并且所有的本地版本都可与它保持一致。</p>
      <p>[0069] 更进一步，所述的步骤(5)中，基于多模态信息融合分析的跨媒体检索，包含以下步骤：</p>
      <p>[0070] (5.1)提取出各种模态媒体对象的底层特征，计算同种模态媒体所有对象间两两的距离，并将所有距离进行高斯归一化；</p>
      <p>[0071] (5.2)通过非线性方法对不同多媒体文档内的声音、视频、图像、文本等对象所携带的信息进行融合分析，求得步骤(5.1)中得到的各个距离的最大值max&#160;dis和最小值min&#160;dis，定义多媒体文档之间的距离Dis如下：</p>
      <p>[0072] Dis＝λ×min&#160;dis+(α+ln(β×(max&#160;dis-min&#160;dis)+1))+A&#160;&#160;&#160;&#160;(12)</p>
      <p>[0073] 其中α、β、λ和A是根据数据库大小和数据分布情况可调节的常数；</p>
      <p>[0074] (5.3)建立多媒体文档关联图，每个多媒体文档是该图上的一个顶点，任意两点间有一条边，边的权重即为步骤(5.2)中计算的距离，表示两个多媒体文档的相似关系；</p>
      <p>[0075] (5.4)重构多媒体文档关联图，首先设置一个阈值，将权重大于阈值的边的权全部设为无穷大；然后对所有的边，用两点间最短路径作为该边的新权重；</p>
      <p>[0076] (5.5)采用多向度量法将多媒体文档关联图投影到多媒体语义空间，所有多媒体文档都在该空间有唯一的坐标，所有多媒体文档内的媒体对象也都被该坐标所指向；</p>
      <p>[0077] (5.6)用户检索时，首先找到该媒体对象在多媒体语义空间的坐标，再计算与其他所有媒体对象的距离，并返回距离最近的目标模态的媒体对象。</p>
      <p>[0078] 本发明的有益效果主要表现在：1、实现多媒体对象语义的自动标注；2、引入用户侧档及相关反馈机制，使得系统能够准确理解用户的真实意图，对检索结果进行排序和优化，实现了检索的个性化，提高了检索的准确性；3、建立多层公共侧档，层次间有继承和共享机制，求同存异，支持海量存储，根据成员情况协同化的更新，更准确描述成员的共同喜好；4、实现了跨模态的多媒体对象检索。</p>
    </disclosure>
    <mode-for-invention>
      <p>[0079] 具体实施方式</p>
      <p>[0080] 下面对本发明作进一步描述。</p>
      <p>[0081] 一种个性化及协同化融合的网上多媒体检索与查询方法，该方法包括以下步骤：</p>
      <p>[0082] (1)、对多媒体信息进行语义的自动标注：利用多媒体信息数据库的各种已有的高层语义，所述各种已有的高层语义包括文本语义标注、多媒体信息间的超链接说明、主题词、图像的主体名及其视觉特征描述词、Web页面内的多媒体信息间的关联描述，通过统计学习模型从中自动选取最能表达多媒体内容的若干关键字作为媒体信息的语义，并结合多媒体信息的底层特征相似度检索，进行关键字传播和多媒体语义的自动信息标注。</p>
      <p>[0083] (2)、建立用户侧档，其中包含用户的信息及个人喜好，实现网络多媒体搜索的个性化，能够按照用户的喜好程度，对检索结果进行优化排序，剔除用户不感兴趣的内容。</p>
      <p>[0084] 用户侧档的基本结构可以定义如下：</p>
      <p>[0085] UP＝&lt;UInfo，P，UPL&gt;</p>
      <p>[0086] UInfo＝&lt;UID，UN，UD&gt;</p>
      <p>[0087] 其中UPL表示用户感兴趣的关键短语的相关信息，P是指向用户所属群组的公共侧档的指针；UInfo表示用户信息，UID表示用户唯一标识符，UN表示用户名，UD表示用户其它描述信息。</p>
      <p>[0088] 用户侧档的建立是在用户使用过程中，按照用户搜索的结果进行聚类分析，确定用户最感兴趣的若干关键短语。</p>
      <p>[0089] (3)建立的用户侧档并不是一成不变的，在每次检索结束后，用户都可以对系统当前查询结果的满意程度进行反馈，系统接收用户的相关反馈意见，然后根据用户的反馈意见进行查询调整。这就要求系统能够进行自动学习，动态调整用户侧档中各关键短语的权重，在下一次检索时能够按照新的关键短语的优先度对检索结果进行排序。</p>
      <p>[0090] (4)、本发明还建立了多层侧档模式，例如用户侧档→群组侧档→社区侧档的三层侧档模式。这样用户就可以根据自身的实际情况，选择属于某一群组，系统为该群组建立公共侧档来描述群组的共同行为和群组成员的普遍爱好。当一个用户新加入一个群组，他就能够从这个群组的公共侧档中继承了一些属性。同样，群组侧档又能够从范围更大的社区侧档中继承一些属性。</p>
      <p>[0091] 公共侧档的基本结构定义为：</p>
      <p>[0092] CP＝&lt;CInfo，WL，Suc&gt;</p>
      <p>[0093] CInfo＝&lt;GID，NAME，DE&gt;</p>
      <p>[0094] 其中WL表示该公共侧档中用户的共同偏好，Suc表示该公共侧档的继承关系；CInfo表示该公共侧档的信息，GID表示本公共侧档唯一标识符，NAME表示公共侧档的名称，DE表示该公共侧档其它的描述信息。</p>
      <p>[0095] 公共侧档的建立可以是在系统建立时，根据已有的经验知识，为不同的群组事先指定一些共同偏好，以缩小检索的范围，提高多媒体检索的速度。同时，公共侧档提供了与用户侧档一样的学习功能，并提供用户协同检索功能，能够根据内部各成员的检索偏好及相关反馈的情况，动态调整预先制定的共同爱好，使得公共侧档能够更加准确地描述成员的共同喜好。在公共侧档进行更新时，通过限制每个用户对特定关键字的投票次数，并结合公共侧档的用户副本在线更新模式，保证公共侧档的安全性。</p>
      <p>[0096] 由于网络用户数量庞大，因此无论是用户侧档还是公共侧档，容量都不宜太大，服务器必须支持海量存储，并运用合理的数据结构来组织这些海量的侧档信息；同时采用有效的机制来减少侧档中所含的关键短语的数量，提高搜索引擎的响应速度。</p>
      <p>[0097] (5)、基于多模态信息融合分析的跨媒体检索，系统对多模态信息融合分析进行多媒体语义的理解，建立不同模态媒体对象之间的语义链，使得用户可以实现跨模态的多媒体信息查询，即用户可以提交任意模态的检索例子去检索任意模态的媒体对象或者多媒体文档。</p>
      <p>[0098] 由于网络媒体信息的丰富性以及用户需求的多样性，因此在网络检索中实现个性化，准确把握用户的真实意图是非常有意义的一项工作。不同用户在进行检索时，即使使用的是同一个关键字，但是他所要检索的内容却未必是一样的。例如当用户在搜索框中键入一个查询关键字“dog”或“狗”，则相关的检索结果可能包括下列这些图片：(a)狗的照片；(b)玩具狗；(c)卡通狗；(d)油画中的狗。尽管检索结果都存在与关键词对应的“狗”，但它们无论是在视觉上还是在语义上都有很大的不同。从用户层面上来说，不同检索者也很可能会喜爱不同的狗，比如儿童可能喜欢玩具狗或者卡通狗，而艺术家者很可能最喜欢油画中的狗。再比如检索“Apple”或“苹果”，结果中可能出现真正的水果类苹果，也可能出现苹果品牌的电脑，对于一个农民用户来讲，可能他真正想找的是苹果而不是电脑，而对于电脑科技工作者来说，他检索的目标可能就是苹果电脑。因此个性化是因人而异的，搜索引擎每次可能检索到大量不同的结果，而其中只有很小一部分才会真正满足用户喜好。理解用户的准确意图，尽可能得满足用户的喜好，是网络个性化检索的重要目标之一。</p>
      <p>[0099] 要实现个性化的多媒体检索，每个用户就必须通过一定的机制来说明自己的喜好和检索意图。为了有效表达用户的真实意图，实现检索的个性化，本发明提出了多层侧档模型，来实现求同存异，具体分为用户侧档→群组侧档→社区侧档的三层，其中群组侧档和社区侧档我们统称为公共侧档。通过各层侧档描述用户意图的步骤如下：</p>
      <p>[0100] Step1.当一个新用户加入要进行多媒体检索时，为了实现个性化的检索，系统要求用户进行注册并填写部分相关的信息。用户注册时需要应有唯一的用户名、简要的个人信息以及个人兴趣等；</p>
      <p>[0101] Step2.用户完成注册后，可以按照个人的实际情况，加入一个或若干个群组中，比如作为IT行业人员加入相关的群组中。这样一来，用户就不是一个单独的用户了，他属于一个群组，同时也继承了群组的属性，即群组已有的共同爱好此时也加入到个人用户的兴趣信息中；</p>
      <p>[0102] Step3.在三层侧档模式中，我们定义“社区”是一个覆盖范围更大的概念，例如一个用户，他主修的专业是计算机软件，此时他就可以选择加入“计算机软件”这一个群组中，并继承其中的属性；与此同时，“计算机软件”这一个群组又从属于“IT”这个更大的社区，并从社区侧档中继承了属性(群组和社区的初始公共爱好等信息是设计者按照已有知识事先设定的)。因此，对于该用户来说，他能够继承“计算机软件”群组以及“IT”社区两个公共侧档中的部分公共和默认缺省属性。</p>
      <p>[0103] 建立三层侧档模式之后，各层侧档中的信息并非一成不变的，而是随着用户的检索操作动态调整的。为了实现这一功能，我们引入了用户相关反馈机制。当用户输入一个关键字进行检索后，他可以对检索的结果按照是否符合自己的意图来进行相关性的评判，检索系统就是按照用户的反馈来动态调整用户侧档中的信息记录。具体步骤如下：</p>
      <p>[0104] Step1.当用户选择了所属的群组之后，他继承了群组及社区的部分属性作为初始默认的个人爱好。为了控制侧档的大小，我们可以对个人喜好信息的数量进行一定的限制，只取出现频率最高的若干个关键短语作为用户的喜好信息。用户的喜好信息被描述为“关键短语+权重”的模式，若一个关键短语的权重越大，则说明用户对这方面的内容兴趣越大，且检索结果中也是按照权重大小来进行排序；</p>
      <p>[0105] Step2.当用户检索完成后，对于检索结果进行相关性反馈。每一项检索结果都提供给用户“正相关”(符合)或者“负相关”(不符合)两个反馈选项，用户可以根据自己的实际情况通过来选择。对于正相关的检索结果，其相应的关键短语的权重会相应增加，而负相关的则对应权重减小。这样就实现了用户侧档中用户喜好信息的动态调整，也使得用户每次检索的结果都会改变，并越来越接近他的真实意愿；</p>
      <p>[0106] Step3.仅仅动态调整用户信息是不够的，还必须能够协同调整公共侧档中的相应信息，而公共侧档中信息的更新则是完全随着该群组(社区)内成员用户侧档的改变而改变的。基本思路是综合内部所有成员的侧档信息，选取其中平均权重最大的若干个关键短语作为该公共侧档的公共喜好信息。由于公共侧档的初始化信息是设计者个人设定的，因此并不能十分准确地表达成员的共同喜好，只有按这样的模式久而久之地进行调整，公共侧档才能够尽可能准确地表达成员的共同意愿。</p>
      <p>[0107] 对于检索系统而言，由于各层侧档中的信息都是高层语义描述的关键短语，因此首先必须为所有的媒体信息标注上准确的语义信息。传统的基于内容的检索中，人们经常产用的是基于底层特征相似度比较的语义标注方法。然而基于相似度比较在很多场合是没有意义的。例如，当用户检索的真实意图是他所喜爱的“烤鸡肉”，但是传统的方法下，烤鸡、烤鸭、烤鹅等等的图片从底层特征方面来看，相似度是非常高的，是不足以区分不同的多媒体对象。</p>
      <p>[0108] 由于绝大多数的多媒体对象都是在网页或其它多媒体文档内，而不会是单独的，因此对于一个等待标注语义信息的多媒体对象，我们的方法是充分利用已有的语义信息以及上下文的联系。以网络上常见的网页为例，一个网页中存在的图片本身或许并没有任何的语义描述。但是由于它在一个信息丰富的网页中，因此我们完全可以从网页的地址、链接以及文本描述中取得很多的语义描述。举一个简单的例子来描述提取语义信息的思路：假如在浏览一个电影网站页面时，我们不能确认一张图片的详细内容，此时可以利用网页中存在的大量文本信息进行分析，从其中选取部分关键字进行统计，最终将出现频率最高的若干关键字，如“Tom&#160;Hanks”(汤姆&#183;汉克斯)、“movie&#160;star”、“Hollywood”、“Oscar”等作为图片的语义信息。同样，我们还能从该演员的信息中得到他主演影片“You’ve&#160;Got&#160;Mail”(《电子情书》)的信息，通过相关链接，我们自然而然能够得到女主角“Meg&#160;Ryan”(梅格&#183;瑞恩)的相关信息，并可以由此引出了“Tom&#160;Hanks”与“Meg&#160;Ryan”合作的许多“Movie”的信息。这个简单的例子说明，现有的多媒体文档中存在的上下文信息为我们的多媒体对象语义标注提供了丰富的源泉。</p>
      <p>[0109] 对于部分单独存在的多媒体对象，由于不存在上下文等文本信息可以提取，因此我们通过底层特征相似性比较，采取关键字传播手段，从现有的媒体库中找到与它最为相似的若干个文件，并在它们的语义描述中取出出现概率最高的若干项作为这个多媒体对象的语义描述。</p>
      <p>[0110] 在目前传统的网络检索中，用户通常的方法是在搜索引擎中输入关键字进行检索，例如，我们可以以“浙江师范大学”作为关键字进行检索多媒体信息，能查询到的信息包含浙江师范大学文本简介、图片、相关新闻报道、视频剪影、校歌歌曲、广播等多种媒体信息。而本发明所要实现的跨模态的多媒体检索则要跳出单纯用关键字查询的局限，同样在上一例子中，我们可以通过一张新闻图片或者一段视频来检索浙江师范大学的相关内容。其检索过程如下：</p>
      <p>[0111] Step1.当用户提交了校歌歌曲音频作为检索例子时，系统首先找到该音频文件所属的多媒体文档，并且定位出该文档在整个多媒体语义空间中的坐标；</p>
      <p>[0112] Step2.根据数据库内已有的所有多媒体文档到该音频所属多媒体文档的空间距离(权值)从小到大进行排序；</p>
      <p>[0113] Step3.按照距离由近及远查找每个多媒体文档中是否存在所需要的“浙江师范大学”的图像资料，若有，则返回给用户，如果没有，则继续向下一个文档进行查找，直到检索到的图像结果数量达到用户的要求。</p>
      <p>[0114] 本发明实现了多媒体对象语义的自动标注，引入了用户侧档→群组侧档→社区侧档的多层侧档模式及相关反馈机制，求同存异，提出了跨模态的多媒体对象检索方法，使得系统能够准确理解用户的真实意图，对检索结果进行排序和优化，实现了个性化、协同化、跨模态的多媒体对象信息检索，有效提高了检索的准确性。</p>
    </mode-for-invention>
  </div>
  </div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title"> 被以下专利引用</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">引用专利</th><th class="patent-data-table-th"> 申请日期</th><th class="patent-data-table-th">公开日</th><th class="patent-data-table-th"> 申请人</th><th class="patent-data-table-th">专利名</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101853299A?cl=zh">CN101853299A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年5月31日</td><td class="patent-data-table-td patent-date-value">2010年10月6日</td><td class="patent-data-table-td ">杭州淘淘搜科技有限公司</td><td class="patent-data-table-td ">一种基于感性认知的图像检索结果排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101908061A?cl=zh">CN101908061A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年7月2日</td><td class="patent-data-table-td patent-date-value">2010年12月8日</td><td class="patent-data-table-td ">互动在线（北京）科技有限公司</td><td class="patent-data-table-td ">词条同步方法及词条同步装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102012900B?cl=zh">CN102012900B</a></td><td class="patent-data-table-td patent-date-value">2009年9月4日</td><td class="patent-data-table-td patent-date-value">2013年1月30日</td><td class="patent-data-table-td ">阿里巴巴集团控股有限公司</td><td class="patent-data-table-td ">信息检索方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102063469A?cl=zh">CN102063469A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2010年12月3日</td><td class="patent-data-table-td patent-date-value">2011年5月18日</td><td class="patent-data-table-td ">百度在线网络技术（北京）有限公司</td><td class="patent-data-table-td ">一种用于获取相关关键词信息的方法、装置和计算机设备</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102063469B?cl=zh">CN102063469B</a></td><td class="patent-data-table-td patent-date-value">2010年12月3日</td><td class="patent-data-table-td patent-date-value">2013年4月24日</td><td class="patent-data-table-td ">百度在线网络技术（北京）有限公司</td><td class="patent-data-table-td ">一种用于获取相关关键词信息的方法、装置和计算机设备</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102129477A?cl=zh">CN102129477A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年4月23日</td><td class="patent-data-table-td patent-date-value">2011年7月20日</td><td class="patent-data-table-td ">山东大学</td><td class="patent-data-table-td ">一种多模态联合的图像重排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102129477B?cl=zh">CN102129477B</a></td><td class="patent-data-table-td patent-date-value">2011年4月23日</td><td class="patent-data-table-td patent-date-value">2013年1月9日</td><td class="patent-data-table-td ">山东大学</td><td class="patent-data-table-td ">一种多模态联合的图像重排序方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102262659A?cl=zh">CN102262659A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年7月15日</td><td class="patent-data-table-td patent-date-value">2011年11月30日</td><td class="patent-data-table-td ">北京航空航天大学</td><td class="patent-data-table-td ">一种基于内容计算的音频标签传播方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102262659B?cl=zh">CN102262659B</a></td><td class="patent-data-table-td patent-date-value">2011年7月15日</td><td class="patent-data-table-td patent-date-value">2013年8月21日</td><td class="patent-data-table-td ">北京航空航天大学</td><td class="patent-data-table-td ">一种基于内容计算的音频标签传播方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102289430A?cl=zh">CN102289430A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年6月29日</td><td class="patent-data-table-td patent-date-value">2011年12月21日</td><td class="patent-data-table-td ">北京交通大学</td><td class="patent-data-table-td ">多模态数据的融合概率潜在语义分析方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102289430B?cl=zh">CN102289430B</a></td><td class="patent-data-table-td patent-date-value">2011年6月29日</td><td class="patent-data-table-td patent-date-value">2013年11月13日</td><td class="patent-data-table-td ">北京交通大学</td><td class="patent-data-table-td ">多模态数据的融合概率潜在语义分析方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102292722B?cl=zh">CN102292722B</a></td><td class="patent-data-table-td patent-date-value">2009年1月21日</td><td class="patent-data-table-td patent-date-value">2014年9月3日</td><td class="patent-data-table-td ">瑞典爱立信有限公司</td><td class="patent-data-table-td ">基于多模元数据和结构化语义描述符来产生注释标签</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102419779A?cl=zh">CN102419779A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年1月13日</td><td class="patent-data-table-td patent-date-value">2012年4月18日</td><td class="patent-data-table-td ">青岛理工大学</td><td class="patent-data-table-td ">一种基于属性排序的商品个性化搜索方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102419779B?cl=zh">CN102419779B</a></td><td class="patent-data-table-td patent-date-value">2012年1月13日</td><td class="patent-data-table-td patent-date-value">2014年6月11日</td><td class="patent-data-table-td ">青岛理工大学</td><td class="patent-data-table-td ">一种基于属性排序的商品个性化搜索方法及装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521321A?cl=zh">CN102521321A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年12月2日</td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td ">华中科技大学</td><td class="patent-data-table-td ">基于检索词歧义性和用户偏好的视频搜索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521321B?cl=zh">CN102521321B</a></td><td class="patent-data-table-td patent-date-value">2011年12月2日</td><td class="patent-data-table-td patent-date-value">2013年7月31日</td><td class="patent-data-table-td ">华中科技大学</td><td class="patent-data-table-td ">基于检索词歧义性和用户偏好的视频搜索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521337A?cl=zh">CN102521337A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年12月8日</td><td class="patent-data-table-td patent-date-value">2012年6月27日</td><td class="patent-data-table-td ">华中科技大学</td><td class="patent-data-table-td ">一种基于海量知识网络的学术社区系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102521337B?cl=zh">CN102521337B</a></td><td class="patent-data-table-td patent-date-value">2011年12月8日</td><td class="patent-data-table-td patent-date-value">2014年5月7日</td><td class="patent-data-table-td ">华中科技大学</td><td class="patent-data-table-td ">一种基于海量知识网络的学术社区系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102592039A?cl=zh">CN102592039A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年1月18日</td><td class="patent-data-table-td patent-date-value">2012年7月18日</td><td class="patent-data-table-td ">四川火狐无线科技有限公司</td><td class="patent-data-table-td ">一种餐饮娱乐休闲数据处理交互方法、装置和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102662953A?cl=zh">CN102662953A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年3月1日</td><td class="patent-data-table-td patent-date-value">2012年9月12日</td><td class="patent-data-table-td ">倪&#26107;</td><td class="patent-data-table-td ">与输入法集成的语义标注系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102662953B?cl=zh">CN102662953B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年3月1日</td><td class="patent-data-table-td patent-date-value">2016年4月6日</td><td class="patent-data-table-td ">倪&#26107;</td><td class="patent-data-table-td ">与输入法集成的语义标注系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102663447A?cl=zh">CN102663447A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年4月28日</td><td class="patent-data-table-td patent-date-value">2012年9月12日</td><td class="patent-data-table-td ">中国科学院自动化研究所</td><td class="patent-data-table-td ">基于判别相关分析的跨媒体检索方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102682079A?cl=zh">CN102682079A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年3月30日</td><td class="patent-data-table-td patent-date-value">2012年9月19日</td><td class="patent-data-table-td ">梁宗强</td><td class="patent-data-table-td ">为搜索非药品性医疗项目名分配权重方法和模块</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102693321A?cl=zh">CN102693321A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年6月4日</td><td class="patent-data-table-td patent-date-value">2012年9月26日</td><td class="patent-data-table-td ">常州南京大学高新技术研究院</td><td class="patent-data-table-td ">一种跨媒体间信息分析与检索的方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102737050A?cl=zh">CN102737050A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年4月11日</td><td class="patent-data-table-td patent-date-value">2012年10月17日</td><td class="patent-data-table-td ">阿里巴巴集团控股有限公司</td><td class="patent-data-table-td ">应用在搜索引擎优化中的关键词动态调整方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102737050B?cl=zh">CN102737050B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年4月11日</td><td class="patent-data-table-td patent-date-value">2015年4月22日</td><td class="patent-data-table-td ">阿里巴巴集团控股有限公司</td><td class="patent-data-table-td ">应用在搜索引擎优化中的关键词动态调整方法和系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102799593A?cl=zh">CN102799593A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年5月24日</td><td class="patent-data-table-td patent-date-value">2012年11月28日</td><td class="patent-data-table-td ">一零四资讯科技股份有限公司</td><td class="patent-data-table-td ">个人化搜寻排序方法以及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102799593B?cl=zh">CN102799593B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年5月24日</td><td class="patent-data-table-td patent-date-value">2015年9月9日</td><td class="patent-data-table-td ">一零四资讯科技股份有限公司</td><td class="patent-data-table-td ">个人化搜寻排序方法以及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102968416A?cl=zh">CN102968416A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年9月1日</td><td class="patent-data-table-td patent-date-value">2013年3月13日</td><td class="patent-data-table-td ">佳能株式会社</td><td class="patent-data-table-td ">基于用户意图识别执行推荐的设备和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102999513A?cl=zh">CN102999513A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年9月14日</td><td class="patent-data-table-td patent-date-value">2013年3月27日</td><td class="patent-data-table-td ">腾讯科技（深圳）有限公司</td><td class="patent-data-table-td ">基于地理位置服务搜索的信息展示方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN102999513B?cl=zh">CN102999513B</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年9月14日</td><td class="patent-data-table-td patent-date-value">2016年3月16日</td><td class="patent-data-table-td ">腾讯科技（深圳）有限公司</td><td class="patent-data-table-td ">基于地理位置服务搜索的信息展示方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103064903A?cl=zh">CN103064903A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2012年12月18日</td><td class="patent-data-table-td patent-date-value">2013年4月24日</td><td class="patent-data-table-td ">厦门市美亚柏科信息股份有限公司</td><td class="patent-data-table-td ">图片检索方法和装置</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103108252A?cl=zh">CN103108252A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月15日</td><td class="patent-data-table-td patent-date-value">2013年5月15日</td><td class="patent-data-table-td ">安徽广行通信科技股份有限公司</td><td class="patent-data-table-td ">一种互联网电视播出的方法及系统</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103116623A?cl=zh">CN103116623A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年1月29日</td><td class="patent-data-table-td patent-date-value">2013年5月22日</td><td class="patent-data-table-td ">江苏大学</td><td class="patent-data-table-td ">一种信息检索自适应数据融合方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN103150685A?cl=zh">CN103150685A</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年2月4日</td><td class="patent-data-table-td patent-date-value">2013年6月12日</td><td class="patent-data-table-td ">中国电力科学研究院</td><td class="patent-data-table-td ">一种智能检修计划优化编制系统和方法</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2013075310A1?cl=zh">WO2013075310A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2011年11月24日</td><td class="patent-data-table-td patent-date-value">2013年5月30日</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Reranking using confident image samples</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2013131430A1?cl=zh">WO2013131430A1</a><span class='patent-tooltip-anchor' data-tooltip-text="由审查员引用"> *</span></td><td class="patent-data-table-td patent-date-value">2013年2月21日</td><td class="patent-data-table-td patent-date-value">2013年9月12日</td><td class="patent-data-table-td ">Tencent Technology (Shenzhen) Company Limited</td><td class="patent-data-table-td ">一种搜索结果显示方法、装置、系统及计算机存储介质</td></tr></table><div class="patent-section-footer">* 由审查员引用</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">分类</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">国际分类号</td><td class="patent-data-table-td "><span class="nested-value"><a href="https://www.google.com/url?id=g51mBwABERAJ&amp;q=http://web2.wipo.int/ipcpub/&amp;usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&amp;notion=scheme&amp;version=20130101&amp;symbol=G06F0017300000">G06F17/30</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">法律事件</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> 日期</th><th class="patent-data-table-th">代码</th><th class="patent-data-table-th">事件</th><th class="patent-data-table-th">说明</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">2008年12月31日</td><td class="patent-data-table-td ">C06</td><td class="patent-data-table-td ">Publication</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2009年2月25日</td><td class="patent-data-table-td ">C10</td><td class="patent-data-table-td ">Request of examination as to substance</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">2011年1月12日</td><td class="patent-data-table-td ">C14</td><td class="patent-data-table-td ">Granted</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">旋转</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">原始图片</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " 未提供图片。\x3ca href\x3d//docs.google.com/viewer?url\x3dpatentimages.storage.googleapis.com/pdfs/e023ce8a5b502cd380e2/CN101334796A.pdf\x3e查看 PDF\x3c/a\x3e"});</script></div></div></div></div></div><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_6e802a6b2b28d51711baddc2f3bec198.js", Host:"https://www.google.com/", IsBooksRentalEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsImageModeNotesEnabled:1, IsOfflineBubbleEnabled:1, IsFutureOnSaleVolumesEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsMobileRequest:0, IsZipitFolderCollectionEnabled:1, IsAdsDisabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:1, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsDisabledRandomBookshelves:0});_OC_Run({"enable_p13n":false,"is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=https://www.google.com/patents%3Fcl%3Dzh%26hl%3Dzh-CN\u0026hl=zh-CN"}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"https://www.google.com/patents/download/%E4%B8%80%E7%A7%8D%E4%B8%AA%E6%80%A7%E5%8C%96%E5%8F%8A%E5%8D%8F%E5%90%8C%E5%8C%96%E8%9E%8D%E5%90%88%E7%9A%84%E7%BD%91.pdf?id=g51mBwABERAJ\u0026hl=zh-CN\u0026output=pdf\u0026sig=ACfU3U2mjUivdffzU_IJvBFQ3VVAQOcKAg"},"sample_url":"https://www.google.com/patents/reader?id=g51mBwABERAJ\u0026hl=zh-CN\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href="https://www.google.com/search?hl=zh-CN"><nobr>Google&nbsp;首页</nobr></a> - <a href="//www.google.com/patents/sitemap/"><nobr>站点地图</nobr></a> - <a href="http://www.google.com/googlebooks/uspto.html"><nobr>美国专利商标局 (USPTO) 专利信息批量下载</nobr></a> - <a href="/intl/zh-CN/privacy/"><nobr>隐私权政策</nobr></a> - <a href="/intl/zh-CN/policies/terms/"><nobr>服务条款</nobr></a> - <a href="https://support.google.com/faqs/answer/2539193?hl=zh-CN"><nobr> 关于 Google 专利</nobr></a> - <a href="//www.google.com/tools/feedback/intl/zh-CN/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'zh-CN'});return false;}catch(e){}"><nobr>发送反馈</nobr></a></div></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>